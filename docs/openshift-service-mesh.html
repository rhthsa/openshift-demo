
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>OpenShift Service Mesh Â· OpenShift Demo</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="Red Hat Thailand SA">
        
        
    
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-plus/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    


    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="application-metrics.html" />
    
    
    <link rel="prev" href="kustomize.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    Infrastructure
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="infrastructure-authentication-providers.html">
            
                <a href="infrastructure-authentication-providers.html">
            
                    
                    OpenShift Authentication Providers with AD
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="infrastructure-infra-nodes.html">
            
                <a href="infrastructure-infra-nodes.html">
            
                    
                    OpenShift MachineSet and Infrastructure Nodes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="infrastructure-monitoring-alerts.html">
            
                <a href="infrastructure-monitoring-alerts.html">
            
                    
                    OpenShift Platform Monitoring and Alert
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="infrastructure-cluster-logging.html">
            
                <a href="infrastructure-cluster-logging.html">
            
                    
                    OpenShift Cluster Logging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="infrastructure-networking.html">
            
                <a href="infrastructure-networking.html">
            
                    
                    OpenShift Networking
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="infrastructure-backup-etcd.html">
            
                <a href="infrastructure-backup-etcd.html">
            
                    
                    OpenShift state backup with etcd snapshot
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="infrastructure-taint-and-toleration.html">
            
                <a href="infrastructure-taint-and-toleration.html">
            
                    
                    Pod Taint and Toleration
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="assign-pod-to-node.html">
            
                <a href="assign-pod-to-node.html">
            
                    
                    Assign pod to node
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="custom-roles.html">
            
                <a href="custom-roles.html">
            
                    
                    Custom Roles and Service Account
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="custom-alert.html">
            
                <a href="custom-alert.html">
            
                    
                    Custom Alert
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="compliance-operator.html">
            
                <a href="compliance-operator.html">
            
                    
                    Compliance
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    Multi-cluster Management with Advanced Cluster Management (RHACM)
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="acm-application-management.html">
            
                <a href="acm-application-management.html">
            
                    
                    Application Manageement
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="acm-hibernate.html">
            
                <a href="acm-hibernate.html">
            
                    
                    Cost saving with hibernating OpenShift
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    Advanced Cluster Security for Kubernetes
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="acs.html">
            
                <a href="acs.html">
            
                    
                    ACS
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" >
            
                <span>
            
                    
                    Applications
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="build-with-dev-console.html">
            
                <a href="build-with-dev-console.html">
            
                    
                    Developer Console
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="build-with-oc.html">
            
                <a href="build-with-oc.html">
            
                    
                    Application Build & Deployment with oc
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="build-with-odo.html">
            
                <a href="build-with-odo.html">
            
                    
                    Application Build & Deployment with odo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="helm.html">
            
                <a href="helm.html">
            
                    
                    Application Deployment with Helm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="imagestreams.html">
            
                <a href="imagestreams.html">
            
                    
                    Image Streams
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="openshift-route.html">
            
                <a href="openshift-route.html">
            
                    
                    OpenShift Route
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.7" data-path="hpa.html">
            
                <a href="hpa.html">
            
                    
                    Horizontal Pod Autoscaler
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.8" data-path="health.html">
            
                <a href="health.html">
            
                    
                    Health Check
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.9" data-path="kustomize.html">
            
                <a href="kustomize.html">
            
                    
                    Kustomize
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.5.10" data-path="openshift-service-mesh.html">
            
                <a href="openshift-service-mesh.html">
            
                    
                    OpenShift Service Mesh
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.11" data-path="application-metrics.html">
            
                <a href="application-metrics.html">
            
                    
                    User Workload Monitoring
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.12" data-path="ci-cd-with-jenkins.html">
            
                <a href="ci-cd-with-jenkins.html">
            
                    
                    CI/CD with Jenkins
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.13" data-path="ci-cd.html">
            
                <a href="ci-cd.html">
            
                    
                    CI/CD with Azure DevOps
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.14" data-path="eap-on-ocp.html">
            
                <a href="eap-on-ocp.html">
            
                    
                    EAP on OpenShift
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.15" data-path="gitops.html">
            
                <a href="gitops.html">
            
                    
                    OpenShift GitOps
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >OpenShift Service Mesh</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div class="search-plus" id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="openshift-service-mesh">OpenShift Service Mesh</h1>
<!-- TOC -->
<ul>
<li><a href="#openshift-service-mesh">OpenShift Service Mesh</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#setup-control-plane-and-sidecar">Setup Control Plane and sidecar</a></li>
<li><a href="#traffic-management">Traffic Management</a><ul>
<li><a href="#destination-rule-virtual-service-and-gateway">Destination Rule, Virtual Service and Gateway</a><ul>
<li><a href="#kiali">Kiali</a></li>
<li><a href="#cliyaml">CLI/YAML</a></li>
</ul>
</li>
<li><a href="#test-istio-gateway">Test Istio Gateway</a></li>
<li><a href="#ab-deployment-with-weight-routing">A/B Deployment with Weight-Routing</a></li>
<li><a href="#conditional-routing-by-uri">Conditional Routing by URI</a></li>
<li><a href="#canary-deployment-using-http-headers">Canary Deployment using HTTP headers</a></li>
</ul>
</li>
<li><a href="#traffic-mirroring-dark-launch">Traffic Mirroring (Dark Launch)</a></li>
<li><a href="#observability">Observability</a><ul>
<li><a href="#traffic-analysis">Traffic Analysis</a></li>
<li><a href="#distributed-tracing">Distributed Tracing</a></li>
<li><a href="#jdbc-tracing-with-opentracing">JDBC Tracing with OpenTracing</a></li>
<li><a href="#envoy-access-log">Envoy Access Log</a></li>
</ul>
</li>
<li><a href="#service-resilience">Service Resilience</a><ul>
<li><a href="#circuit-breaker">Circuit Breaker</a></li>
</ul>
</li>
<li><a href="#secure-with-mtls">Secure with mTLS</a><ul>
<li><a href="#within-service-mesh">Within Service Mesh</a><ul>
<li><a href="#pod-liveness-and-readiness">Pod Liveness and Readiness</a></li>
</ul>
</li>
<li><a href="#istio-gateway-with-mtls">Istio Gateway with mTLS</a></li>
</ul>
</li>
<li><a href="#jwt-token">JWT Token</a><ul>
<li><a href="#red-hat-single-sign-on">Red Hat Single Sign-On</a></li>
<li><a href="#requestauthentication-and-authorization-policy">RequestAuthentication and Authorization Policy</a></li>
</ul>
</li>
<li><a href="#service-level-objective-slo">Service Level Objective (SLO)</a></li>
<li><a href="#control-plane-with-high-availability">Control Plane with High Availability</a><ul>
<li><a href="#openshift-service-mesh-1x">OpenShift Service Mesh 1.x</a></li>
<li><a href="#openshift-service-mesh-2x">OpenShift Service Mesh 2.x</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h2 id="overview">Overview</h2>
<p>Sample application</p>
<p><img src="images/service-mesh-sample-app.png" alt=""></p>
<h2 id="setup-control-plane-and-sidecar">Setup Control Plane and sidecar</h2>
<ul>
<li><p>Install following Operators from OperatorHub</p>
<ul>
<li>Red Hat OpenShift distributed tracing platform (Jaeger for OpenShift Service Mesh earlier than version 2.1)</li>
<li>Kiali</li>
<li>OpenShift Service Mesh</li>
<li><p>ElasticSearch (Optional)</p>
<p><img src="images/ossm-installed-operators.png" alt=""></p>
</li>
</ul>
<p>Install with CLI</p>
<pre><code class="lang-bash">oc apply -f manifests/ossm-sub.yaml
oc wait --for condition=established --timeout=180s \
crd/servicemeshcontrolplanes.maistra.io \
crd/kialis.kiali.io \
crd/jaegers.jaegertracing.io
oc get csv
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME                         DISPLAY                                          VERSION   REPLACES                     PHASE
jaeger-operator.v1.30.0      Red Hat OpenShift distributed tracing platform   1.30.0                                 Succeeded
kiali-operator.v1.36.7       Kiali Operator                                   1.36.7    kiali-operator.v1.36.6       Succeeded
servicemeshoperator.v2.1.1   Red Hat OpenShift Service Mesh                   2.1.1-0   servicemeshoperator.v2.1.0   Succeeded
</code></pre>
</li>
<li><p>Create control plane by create ServiceMeshControlPlane CRD</p>
<ul>
<li><p>CLI</p>
<pre><code class="lang-bash">oc new-project istio-system
oc create -f manifests/smcp.yaml -n istio-system
</code></pre>
</li>
<li><p>OpenShift Administrator Console</p>
<ul>
<li><p>Switch to Adminstration and navigate to: Operators -&gt; Installed Operators then select Red Hat OpenShift Service Mesh-&gt;Service Mesh Control Plane</p>
<p><img src="images/select-openshift-service-mesh.png" alt=""></p>
</li>
<li><p>Select Create ServiceMeshControlPlane</p>
<p><img src="images/create-control-plane.png" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>- Navigate to Proxy and input following values to enable access log at Envoy (sidecar)

  ![](images/smcp-envoy-access-log.png)

- Set outbound traffic policy

  ![](images/smcp-outbound-allow-any.png)

&lt;!-- - Set auto mTLS to false

  ![](images/smcp-mtls.png) --&gt;

- Verify YAML

  ![](images/dev-console-smcp-yaml.png)

- Create
</code></pre><ul>
<li><p>Check for control plane creating status</p>
<p><img src="images/admin-console-smcp-status.png" alt=""></p>
<p>or Bash shell <a href="bin/get-smcp-status.sh">get-smcp-status.sh</a></p>
<pre><code class="lang-bash">bin/get-smcp-status.sh basic istio-system
</code></pre>
<p>or just CLI</p>
<pre><code class="lang-bash">oc get smcp/basic -n istio-system
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME    READY   STATUS            PROFILES      VERSION   AGE
basic   10/10   ComponentsReady   [&quot;default&quot;]   2.1.1     66s
</code></pre>
</li>
<li><p>Join project1 into control plane</p>
<ul>
<li><p>Create data plane project</p>
<pre><code class="lang-bash">oc new-project project1
</code></pre>
</li>
<li><p>Review <a href="manifests/smcp.yaml">ServiceMeshMemberRoll CRD</a></p>
<pre><code class="lang-yaml">apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - project1
</code></pre>
</li>
<li><p>Apply ServiceMeshMemberRoll</p>
<pre><code class="lang-bash">oc create -f manifests/smmr.yaml -n istio-system
</code></pre>
</li>
<li><p>Check for ServiceMeshMemberRoll status</p>
<pre><code class="lang-bash">oc describe smmr/default -n istio-system | grep -A2 Spec:
</code></pre>
</li>
</ul>
</li>
<li><p>Deploy sidecar to frontend app in project1</p>
<pre><code class="lang-bash">oc apply -f manifests/frontend.yaml -n project1
oc patch deployment/frontend-v1 -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos; -n project1
oc patch deployment/frontend-v2 -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos; -n project1
</code></pre>
<p><img src="images/dev-console-frontend.png" alt=""></p>
</li>
</ul>
<ul>
<li><p>Check for sidecar in frontend-v1 and frontend-v2 pods</p>
<pre><code class="lang-bash">oc get pods -n project1
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">NAME                           READY   STATUS        RESTARTS   AGE
frontend-v1-577b98f48c-6j5zg   2/2     Running       0          15s
frontend-v1-c5d4648f9-7jfk2    1/1     Terminating   0          13m
frontend-v2-5cd968bc59-cwsd8   2/2     Running       0          14s
frontend-v2-5d4dbdbc9-k6787    1/1     Terminating   0          13m
</code></pre>
<p>Check developer console</p>
<p><img src="images/pod-with-sidecar.png" alt=""></p>
</li>
</ul>
<h2 id="traffic-management">Traffic Management</h2>
<h3 id="destination-rule-virtual-service-and-gateway">Destination Rule, Virtual Service and Gateway</h3>
<h4 id="kiali">Kiali</h4>
<ul>
<li><p>Open Kiali Console</p>
<p><img src="images/dev-console-kiali.png" alt=""></p>
</li>
<li><p>Navigate to Services and select frontend, Actions-&gt; Request Routing</p>
<p><img src="images/kiali-request-routing-menu.png" alt=""></p>
</li>
<li><p>Click Route To, Set weight of frontend-v1 to 100% then Click Add Rule</p>
<p><img src="images/kiali-reqeust-routing-weight.png" alt=""></p>
</li>
<li><p>Click Show Advanced Options</p>
<ul>
<li><p>Check cluster domain</p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
echo $DOMAIN
</code></pre>
</li>
<li><p>Virtual Service Hosts frontend.apps.$DOMAIN</p>
<p><img src="images/kiali-reqeust-routing-virtual-host.png" alt=""></p>
</li>
<li><p>Gateway</p>
<p><img src="images/kiali-reqeust-routing-gateway.png" alt=""></p>
</li>
<li><p>Traffic Policy</p>
<p><img src="images/kiali-reqeust-routing-traffic-policy.png" alt=""></p>
</li>
<li><p>Create</p>
</li>
</ul>
</li>
</ul>
<h4 id="cliyaml">CLI/YAML</h4>
<ul>
<li><p>Create Destination Rule for frontend v1 and frontend v2</p>
<ul>
<li><p>Review <a href="manifests/frontend-destination-rule.yaml">Destination Rule CRD</a></p>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
    name: frontend
spec:
    host: frontend
    subsets:
    - name: v1
    labels:
        app: frontend
        version: v1
    trafficPolicy:
        loadBalancer:
        simple: ROUND_ROBIN
    - name: v2
    labels:
        app: frontend
        version: v2
    trafficPolicy:
        loadBalancer:
        simple: ROUND_ROBIN
</code></pre>
</li>
<li><p>Create destination rule</p>
<pre><code class="lang-bash">oc apply -f manifests/frontend-destination-rule.yaml -n project1
</code></pre>
</li>
</ul>
</li>
<li><p>Create Virtual Service for frontend app</p>
<ul>
<li><p>Review <a href="manifests/frontend-virtual-service.yaml">Virtual Service CRD</a>, Replace DOMAIN with cluster&apos;s sub-domain.</p>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
    name: frontend
spec:
    hosts:
    - frontend.apps.DOMAIN
    gateways:
    - project1/frontend-gateway
    http:
    - route:
    - destination:
        port:
            number: 8080
        host: frontend.project1.svc.cluster.local
</code></pre>
</li>
<li><p>Replace DOMAIN with cluster DOMAIN and create <a href="manifests/frontend-virtual-service.yaml">virtual service</a> or run following command</p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
</code></pre>
</li>
</ul>
</li>
<li><p>Create Gateway for frontend app</p>
<ul>
<li><p>Check for cluster&apos;s sub-domain</p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
echo $DOMAIN
</code></pre>
</li>
<li><p>Review <a href="manifests/frontend-gateway.yaml">Gateway CRD</a>, Replaced DOMAIN with cluster&apos;s sub-domain</p>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: frontend-gateway
spec:
selector:
    istio: ingressgateway # use istio default controller
servers:
- port:
    number: 80
    name: http
    protocol: HTTP
    hosts:
    - &apos;frontend.apps.DOMAIN&apos;
</code></pre>
</li>
<li><p>Replace DOMAIN with your clsuter sub-domain and Create <a href="manifests/frontend-gateway.yaml">gateway</a></p>
<pre><code class="lang-bash">oc apply -f manifests/frontend-gateway.yaml -n project1
</code></pre>
<p>or use following bash command </p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-gateway.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
</code></pre>
</li>
<li><p>Check that route is automatically created</p>
<pre><code class="lang-bash">oc get route -n istio-system | grep frontend-gateway
</code></pre>
<p>Sample outout</p>
<pre><code class="lang-bash">project1-frontend-gateway-5f5077c573bd9294   frontend.apps.cluster-27bb.27bb.sandbox664.opentlc.com                                   istio-ingressgateway   http                        None
</code></pre>
<p>&lt;!-- - Create Route (configured with Istio Gateway) for frontend app</p>
</li>
<li><p>Review <a href="manifests/frontend-route-istio.yaml">Route</a>, Replace DOMAIN with cluster&apos;s DOMAIN</p>
<pre><code class="lang-yaml">apiVersion: v1
kind: Route
metadata:
    name: frontend
spec:
    host: frontend.apps.DOMAIN
    port:
    targetPort: http2
    to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
    wildcardPolicy: None
</code></pre>
</li>
<li><p>Replace DOMAIN with cluster DOMAIN then create Route</p>
<pre><code class="lang-bash">oc apply -f manifests/frontend-route-istio.yaml -n istio-system
</code></pre>
<p>or use following bash command </p>
<p><code>bash
DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-route-istio.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -</code> --&gt;</p>
</li>
</ul>
</li>
</ul>
<h3 id="test-istio-gateway">Test Istio Gateway</h3>
<ul>
<li><p>Test with cURL  </p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl http://$FRONTEND_ISTIO_ROUTE
</code></pre>
</li>
</ul>
<h3 id="ab-deployment-with-weight-routing">A/B Deployment with Weight-Routing</h3>
<ul>
<li><p>Set weight routing between 2 services with virtual service
Remark: if you use above Kiali steps then you already set 100% traffic to frontend-v1</p>
<ul>
<li><p>Check for <a href="manifests/frontend-virtual-service-with-weight-routing.yaml">virtual service with weight routing</a>, Replace DOMAIN with cluster&apos;s DOMAIN</p>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: frontend
spec:
  hosts:
  - frontend.apps.DOMAIN
  gateways:
  - project1/frontend-gateway
  http:
  - route:
    - destination:
        port:
          number: 8080
        host: frontend.project1.svc.cluster.local
        subset: v1
      weight: 100
    - destination:
        port:
          number: 8080
        host: frontend.project1.svc.cluster.local
        subset: v2
      weight: 0
</code></pre>
<ul>
<li>Apply <a href="manifests/frontend-virtual-service-with-weight-routing.yaml">virtual service</a> for Blue/Green deployment with route all traffic to v1</li>
</ul>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service-with-weight-routing.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
</code></pre>
<h4 id="cli">CLI</h4>
<ul>
<li>Test with cURL to verify that all requests are routed to v1</li>
<li><p>Blue/Green deployment by route all requests to v2</p>
<pre><code class="lang-bash">oc patch virtualservice frontend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:0},{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:100}]}}]&apos; -n project1
</code></pre>
</li>
<li><p>Test with cURL to verify that all requests are routed to v2</p>
</li>
<li><p>Adjust traffic 30% to v2</p>
<pre><code class="lang-bash">oc patch virtualservice frontend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:70},{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:30}]}}]&apos; -n project1
</code></pre>
</li>
</ul>
<h4 id="kiali">Kiali</h4>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
<li><p>Test A/B deployment</p>
<ul>
<li><p>Run 100 requests</p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
COUNT=0
rm -f result.txt
while [ $COUNT -lt 100 ];
do
    OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE/version)
    printf &quot;%s\n&quot; $OUTPUT &gt;&gt; result.txt
    printf &quot;%s\n&quot; $OUTPUT
    sleep .2
    COUNT=$(expr $COUNT + 1)
done
</code></pre>
</li>
<li><p>Check result for comparing percentage of requests to v1 and v2</p>
<pre><code class="lang-bash">printf &quot;Version 1: %s\n&quot; $(cat result.txt | grep &quot;v1&quot; | wc -l)
printf &quot;Version 2: %s\n&quot; $(cat result.txt | grep &quot;v2&quot; | wc -l)
rm -f result.txt
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="conditional-routing-by-uri">Conditional Routing by URI</h3>
<ul>
<li><p>Set conditional routing between 2 services with virtual service</p>
<ul>
<li><p>Check for <a href="manifests/frontend-virtual-service-with-uri.yaml">virtual service by URI</a>, Replace DOMAIN with cluster&apos;s DOMAIN. Condition with regular expression</p>
<ul>
<li><p>Route to v1 if request URI start with &quot;/ver&quot; and end with &quot;1&quot;</p>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: frontend
spec:
  hosts:
  - frontend.apps.SUBDOMAIN
  gateways:
  - project1/frontend-gateway
  http:
  - match:
    - uri:
        regex: /ver(.*)1
    # Rewrite URI back to / because frontend app not have /ver(*)1
    rewrite:
      uri: &quot;/&quot;
    route:
    - destination:
        host: frontend
        port:
          number: 8080
        subset: v1
  - route:
    - destination:
        host: frontend
        port:
          number: 8080
        subset: v2
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Apply virtual service</p>
<pre><code class="lang-bash">oc apply -f manifests/frontend-virtual-service-with-uri.yaml -n project1
</code></pre>
<p>or use following bash command </p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service-with-uri.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
</code></pre>
</li>
<li><p>Test with URI /version1 and /ver1</p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl -w&quot;\n\n&quot; $FRONTEND_ISTIO_ROUTE/version1
curl -w&quot;\n\n&quot; $FRONTEND_ISTIO_ROUTE/vers1
curl -w&quot;\n\n&quot; $FRONTEND_ISTIO_ROUTE/ver1
</code></pre>
</li>
<li><p>Test with URI /</p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl -w&quot;\n\n&quot; $FRONTEND_ISTIO_ROUTE/
</code></pre>
</li>
</ul>
<h3 id="canary-deployment-using-http-headers">Canary Deployment using HTTP headers</h3>
<ul>
<li><p>Canary Deployment by checking User-Agent header with <a href="manifests/frontend-virtual-service-with-header.yaml">Virtual Service</a>, Replace DOMAIN with cluster&apos;s sub-domain.</p>
<ul>
<li>If HTTP header User-Agent contains text Firewall, request will be routed to frontend v2</li>
</ul>
<pre><code class="lang-yaml">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: frontend
spec:
  hosts:
  - frontend.apps.DOMAIN
  gateways:
  - project1/frontend-gateway
  http:
  - match:
    - headers:
        user-agent:
          regex: (.*)Firefox(.*)
    route:
    - destination:
        host: frontend
        port:
          number: 8080
        subset: v2
  - route:
    - destination:
        host: frontend
        port:
          number: 8080
        subset: v1
</code></pre>
</li>
<li><p>Apply <a href="manifests/frontend-virtual-service-with-header.yaml">Virtual Service</a></p>
<pre><code class="lang-bash">oc apply -f manifests/frontend-virtual-service-with-header.yaml -n project1
</code></pre>
<p>or use following bash command </p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service-with-header.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
</code></pre>
</li>
<li><p>Test with cURL with HTTP header User-Agent contains Firefox</p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl -H &quot;User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0&quot; $FRONTEND_ISTIO_ROUTE
</code></pre>
</li>
</ul>
<h2 id="traffic-mirroring-dark-launch">Traffic Mirroring (Dark Launch)</h2>
<ul>
<li><p>Deploy backend application</p>
<pre><code class="lang-bash">oc apply -f manifests/backend.yaml -n project1
oc apply -f manifests/backend-destination-rule.yaml -n project1
oc apply -f manifests/backend-virtual-service-v1-v2-50-50.yaml -n project1
oc get pods -n project1
</code></pre>
</li>
<li><p>Configure frontend to request to backend</p>
<pre><code class="lang-bash">oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1
oc set env deployment/frontend-v2 BACKEND_URL=http://backend:8080/ -n project1
</code></pre>
</li>
</ul>
<ul>
<li><p><strong>Optional</strong>: Draw connetion from frontend to backend in Developer Console</p>
<pre><code class="lang-bash">oc annotate deployment frontend-v1 &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v1&quot;},{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v2&quot;}]&apos; -n project1
oc annotate deployment frontend-v2 &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v1&quot;},{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v2&quot;}]&apos; -n project1
</code></pre>
<p><img src="images/dev-console-topology-frontend-v1-v2-backend-v1-v2.png" alt=""></p>
</li>
<li><p>Deploy audit app and mirror every requests that frontend call backend to audit app</p>
<pre><code class="lang-bash">oc apply -f manifests/audit-app.yaml -n project1
oc get pods -n project1
</code></pre>
</li>
<li><p>Update <a href="manifests/backend-virtual-service-mirror.yaml">backend virtual service</a> to mirror requests to audit app.</p>
<pre><code class="lang-bash">oc apply -f manifests/backend-virtual-service-mirror.yaml -n project1
</code></pre>
</li>
<li><p>Use cURL to call frontend and check audit&apos;s pod log by CLI (with another terminal) or Web Console</p>
<ul>
<li>cURL frontend</li>
</ul>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl $FRONTEND_ISTIO_ROUTE
</code></pre>
<ul>
<li>View audit log </li>
</ul>
<pre><code class="lang-bash">oc logs -f $(oc get pods --no-headers | grep audit|head -n 1|awk &apos;{print $1}&apos;) -c backend -n project1
</code></pre>
<p><img src="images/mirror-log.png" alt=""></p>
</li>
</ul>
<p>  Kiali Graph</p>
<p>  <img src="images/kiali-frontend-backend-audit.png" alt=""></p>
<h2 id="observability">Observability</h2>
<h3 id="traffic-analysis">Traffic Analysis</h3>
<ul>
<li>Check Kiali Console</li>
<li><p>login to OpenShift Developer Console, select project istio-system and open Kiali console </p>
<p><img src="images/istio-system-project.png" alt=""></p>
</li>
<li><p>Login to Kiali Console and select Graph</p>
<ul>
<li>Namespace: select checkbox &quot;project1&quot;</li>
<li>Display: select checkbox &quot;Request distribution&quot; and &quot;Traffic animation&quot;</li>
</ul>
</li>
<li><p>Run following command</p>
<pre><code class="lang-bash">DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service-with-weight-routing.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
oc patch virtualservice frontend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:70},{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:30}]}}]&apos; -n project1
FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
while [ 1 ];
do
        OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE)
        printf &quot;%s\n&quot; $OUTPUT
        sleep .2
done
</code></pre>
</li>
<li><p>Check Kiali Console</p>
<p><img src="images/kiali-graph.png" alt=""></p>
</li>
<li><p>Traffic analysis for frontend app. Select Application-&gt;frontend-&gt;inbound traffic and outbound traffic</p>
<p><img src="images/kiali-frontend-inboud-traffic.png" alt=""></p>
</li>
</ul>
<h3 id="distributed-tracing">Distributed Tracing</h3>
<ul>
<li><p>Distributed tracing with Jaeger. Select tab Tracing</p>
<ul>
<li><p>Overall tracing for frontend app</p>
<p><img src="images/frontend-app-tracing.png" alt=""></p>
</li>
<li><p>Login to Jaeger by select &quot;View in Tracing&quot;</p>
<p><img src="images/jaeger-main.png" alt=""></p>
</li>
<li><p>Drill down to tracing information</p>
<p><img src="images/jaeger-transaction.png" alt=""> </p>
</li>
<li><p>Simulate error on backend app </p>
<ul>
<li><p>set weight to 50/50</p>
<pre><code class="lang-bash">  oc patch virtualservice frontend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:50},{&quot;destination&quot;:{&quot;host&quot;:&quot;frontend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:50}]}}]&apos; -n project1
</code></pre>
</li>
<li><p>set backend pod to return 504</p>
<pre><code class="lang-bash">oc exec -c backend -n project1 $(oc get pods -n project1 -l app=backend --no-headers -o=custom-columns=&quot;NAME:.metadata.name&quot;|head -n 1) -- curl -w &quot;\n\n&quot; -s http://localhost:8080/stop
</code></pre>
</li>
<li><p>Request to frontend app</p>
<pre><code class="lang-bash">curl -s -w &quot;\n&quot; $FRONTEND_ISTIO_ROUTE
curl -s -w &quot;\n&quot; $FRONTEND_ISTIO_ROUTE
</code></pre>
</li>
<li><p>Query Jaeger with tag http.status_code=504. Check detail trace to verify that envoy retry request to backend service</p>
<p><img src="images/jaeger-with-http-504.png" alt=""></p>
<p>Drill down into detail of transaction</p>
<p><img src="images/jaeger-with-http-504-drill-down.png" alt=""></p>
</li>
<li><p>set backend pod to return 200</p>
<pre><code class="lang-bash">oc exec -c backend -n project1 $(oc get pods -n project1 -l app=backend --no-headers -o=custom-columns=&quot;NAME:.metadata.name&quot;|head -n 1) -- curl -w &quot;\n\n&quot; -s http://localhost:8080/start
</code></pre>
<h3 id="jdbc-tracing-with-opentracing">JDBC Tracing with OpenTracing</h3>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Deploy todo application with Kustomize</p>
<pre><code class="lang-bash"> oc create -k manifests/todo-kustomize/overlays/dev
 watch oc get pods -n todo
</code></pre>
</li>
<li><p>Todo Application</p>
<p><img src="images/todo-app.png" alt=""></p>
</li>
<li><p>Add namespace todo to ServiceMeshMemberRoll</p>
<pre><code class="lang-bash">oc patch smmr default -n istio-system --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/members/-&quot;,&quot;value&quot;:&quot;todo&quot;}]&apos;
</code></pre>
</li>
<li><p>Add istio sidecar to deployment todo</p>
<pre><code class="lang-bash"> oc patch deployment todo -n todo -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos;
</code></pre>
</li>
<li><p>Change todo container image to <em>quay.io/voravitl/todo:trace</em> and set JDBC URL to use tracing driver</p>
<pre><code class="lang-bash">oc patch deployment todo -n todo --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/template/spec/containers/0/image&quot;,&quot;value&quot;:&quot;quay.io/voravitl/todo:trace&quot;}]&apos;
oc set env deployment todo -n todo  quarkus.datasource.jdbc.url=jdbc:tracing:postgresql://todo-db/todo
oc set env deployment todo -n todo  quarkus.jaeger.endpoint=http://jaeger-collector.istio-system:14268/api/traces
</code></pre>
</li>
<li><p>Create, update and delete tasks in todo application then Check Kiali console.</p>
<ul>
<li><p>Select by query type</p>
<p><img src="images/todo-trace-jaeger.png" alt=""></p>
</li>
<li><p>Transaction with create todo item</p>
<p><img src="images/todo-trace-create.png" alt=""></p>
</li>
<li><p>Transaction with query todo items</p>
<p><img src="images/todo-trace-select.png" alt=""></p>
</li>
<li><p>Transaction with deleting todo item</p>
<p><img src="images/todo-trace-delete.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Check for traceid in todo pod&apos;s log</p>
<pre><code class="lang-bash">oc logs -f \
$(oc get pods -l app=todo -n todo --no-headers -o=custom-columns=&quot;NAME:.metadata.name&quot;|head -n 1) \
 -n todo -c todo
</code></pre>
<p>output</p>
<pre><code class="lang-log">09:54:27 INFO  x-b3-traceid=510dc04de55eb770167efec4a8cd622a, , parentId=167efec4a8cd622a, x-b3-spanid=5ad367c2c286947e, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) update id:9
09:54:27 INFO  x-b3-traceid=447bbafa3ed323909180faf410d181f5, , parentId=9180faf410d181f5, x-b3-spanid=a7a9d4b287917f5f, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) update id:10
09:54:32 INFO  x-b3-traceid=32c779ecb20641fa3823e3b6d448c41e, , parentId=3823e3b6d448c41e, x-b3-spanid=bf61443edfc58082, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) getAll
</code></pre>
</li>
</ul>
<h3 id="envoy-access-log">Envoy Access Log</h3>
<ul>
<li><p>Envoy access log already enabled with <a href="manifests/smcp.yaml">ServiceMeshControlPlane CRD</a></p>
<pre><code class="lang-yaml">  proxy:
  accessLogging:
    envoyService:
      enabled: false
    file:
      encoding: TEXT
      name: /dev/stdout
</code></pre>
</li>
<li><p>Check access log</p>
<pre><code class="lang-bash">oc logs -f \
$(oc get pods -n project1 --no-headers -o=custom-columns=&quot;NAME:.metadata.name&quot; -l app=frontend|head -n 1) \
-c istio-proxy -n project1
</code></pre>
<p>Sample output</p>
<pre><code class="lang-log">03:17:46 INFO  x-b3-traceid=7256eae02a1f11166d5add572165bfa0, , parentId=6d5add572165bfa0, x-b3-spanid=41e2c3434fbb022a, sampled=true [io.qu.sa.TodoResource] (executor-thread-2) getAll
</code></pre>
<p>Search by x-b3-traceid </p>
<p><img src="images/jaeger-by-trace-id.png" alt=""></p>
<p>View trace</p>
<p><img src="images/jaeger-by-trace-id-tx.png" alt=""></p>
</li>
</ul>
<h2 id="service-resilience">Service Resilience</h2>
<ul>
<li><p>Configure our application to contains only frontend-v1 and backend-v1 and scale backend to 3 pods.</p>
<pre><code class="lang-bash">oc delete all --all -n project1
oc delete gateway --all -n project1
oc delete dr,vs --all -n project1
oc apply -f manifests/frontend.yaml -n project1
oc patch deployment/frontend-v1 -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos; -n project1
oc apply -f manifests/backend.yaml -n project1
oc delete deployment/frontend-v2 -n project1
oc delete deployment/backend-v2 -n project1
oc delete route frontend -n project1
oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1
oc annotate deployment frontend-v1 &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v1&quot;},{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v2&quot;}]&apos; -n project1
oc scale deployment backend-v1 --replicas=3 -n project1
oc apply -f manifests/backend-destination-rule-v1-only.yaml -n project1
oc apply -f manifests/backend-virtual-service.yaml -n project1
oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1
DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
cat manifests/frontend-gateway.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
watch oc get pods -n project1
</code></pre>
<p><img src="images/dev-console-app-for-cb.png" alt=""></p>
</li>
</ul>
<ul>
<li><p>Test with cURL</p>
<pre><code class="lang-bash">FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
curl http://$FRONTEND_ISTIO_ROUTE
</code></pre>
<p>Sample output - Check for field Host that is backend pod that processed for this request</p>
<pre><code class="lang-bash">Frontend version: 1.0.0 =&gt; [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Hello, Quarkus]
</code></pre>
</li>
<li><p>Loop 6 times. Result from backend will be round robin.</p>
<ul>
<li><p>Create bash function</p>
<pre><code class="lang-bash">function loop_frontend(){
  FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
  COUNT=0
  MAX=$1
  while [ $COUNT -lt $MAX ];
  do
    curl -s http://$FRONTEND_ISTIO_ROUTE | awk -F&apos;,&apos; &apos;{print $5 &quot;=&gt;&quot; $2}&apos;
    COUNT=$(expr $COUNT + 1 )
  done
}
</code></pre>
</li>
<li><p>Run function with input paramter 6</p>
</li>
</ul>
<pre><code class="lang-bash">loop_frontend 6
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
</code></pre>
</li>
<li><p>Check that pods run on which node</p>
<pre><code class="lang-bash">oc get pods -o=custom-columns=&quot;NAME:.metadata.name,AZ:.metadata.labels[&apos;topology\.kubernetes\.io/zone&apos;]&quot; -n project1
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">NAME                          AZ
backend-v1-7779cb476b-6wbsp   us-east-2a
backend-v1-7779cb476b-bgk22   us-east-2a
backend-v1-7779cb476b-q2hz9   us-east-2b
frontend-v1-d6dc6768-vbzcc    us-east-2a
</code></pre>
<p>Envoy has <strong>Localcity Load Balancing</strong> feature and this feature is enabled by default. A locality defines geographic location by region, zone and subzone. Envoy will try to send request to pod within defined geographic if avilable In this case is within same AZ</p>
<p><em>Notice that response came frome 2 pods in AZ us-east-2a same AZ with frontend</em></p>
</li>
<li><p>By default, Envoy will automatically retry if it get response with code 503</p>
</li>
<li><p>Force one backend pod to return 503 </p>
<ul>
<li><p>by command line.</p>
<!-- $(oc get pod -n project1 | grep -m1 backend | cut -d " " -f1) -->
<pre><code class="lang-bash">oc exec -n project1 -c backend &lt;backend pod same zone with frontend&gt; -- curl -s http://localhost:8080/not_ready
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">Backend version:v1, Response:200, Host:backend-v1-7779cb476b-bgk22, Status:200, Message: Readiness: false
</code></pre>
</li>
<li><p>by Web Console</p>
<p>  <img src="images/dev-console-terminal-set-pod-not-ready.png" alt=""></p>
</li>
</ul>
</li>
<li><p>Verify response from that pod.</p>
<pre><code class="lang-bash">  oc exec -n project1 -c backend   &lt;backend pod same zone with frontend&gt; -- curl -sv http://localhost:8080/
</code></pre>
<p>Sample Output</p>
<pre><code class="lang-bash">  &lt; HTTP/1.1 503 Service Unavailable
  &lt; Content-Encoding: text/plain
  &lt; Expires: Wed, 08 Sep 2021 12:46:28 GMT
  &lt; Content-Length: 126
  &lt; Content-Type: text/plain;charset=UTF-8
  &lt;
  { [126 bytes data]
  * Connection #0 to host localhost left intact
  Backend version:v1, Response:503, Host:backend-v1-7779cb476b-bgk22, Status:503, Message: Application readiness is set to false
</code></pre>
</li>
<li><p>Test with cURL again. You will get only status 200</p>
<pre><code class="lang-bash">  loop_frontend 10
</code></pre>
<p>  Sample Output</p>
<pre><code class="lang-bash">  Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
  Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
  Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
  Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
  Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
  Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
  Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
  Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
  Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
  Host:backend-v1-7779cb476b-q2hz9=&gt; Status:200
</code></pre>
</li>
<li><p>Check tracing in Jaeger by query with http.response_code=503</p>
<p><img src="images/jaeger-with-http-503.png" alt=""></p>
<p>Drill down to check that envoy retry request to backend after it got 503 response.</p>
<p> <img src="images/jaeger-with-http-503-drill-down.png" alt=""></p>
</li>
<li><p>Set backend pod to return 200</p>
<pre><code class="lang-bash">  oc exec -n project1 -c backend  &lt;backend pod same zone with frontend&gt; -- curl -s http://localhost:8080/ready
</code></pre>
</li>
</ul>
<h3 id="circuit-breaker">Circuit Breaker</h3>
<ul>
<li><p>Update destination rule with circuit breaker</p>
<pre><code class="lang-bash">oc apply -f manifests/backend-destination-rule-circuit-breaker.yaml -n project1
</code></pre>
</li>
<li><p>Review Circuit Breaker configuration in <a href="manifests/backend-destination-rule-circuit-breaker.yaml">deatination rule</a></p>
<ul>
<li>If found error 1 times (consecutiveErrors)</li>
<li>then eject that pod from pool for 15 mintues (baseEjectionTime)</li>
<li>Maximum number of pod that can be ejected is 100% (maxEjectionPercent)</li>
<li>Check this every 15 min (interval)</li>
</ul>
<pre><code class="lang-yaml">outlierDetection:
      baseEjectionTime: 15m
      consecutiveErrors: 1
      interval: 15m
      maxEjectionPercent: 100
</code></pre>
</li>
<li><p>Set one backend pod to return 504 and verify that pod return 504</p>
<pre><code class="lang-bash">oc exec -n project1 -c backend &lt;backend pod same zone with frontend&gt; -- curl -s http://localhost:8080/stop
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">Backend version:v1, Response:200, Host:backend-v1-7779cb476b-bgk22, Status:200, Message: Liveness: false
</code></pre>
</li>
<li><p>Verify that backend pod return 504</p>
<pre><code class="lang-bash">oc exec -n project1 -c backend &lt;backend pod same zone with frontend&gt; -- curl -s http://localhost:8080/
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">Backend version:v1, Response:504, Host:backend-v1-7779cb476b-bgk22, Status:504, Message: Application liveness is set to fasle
</code></pre>
</li>
<li><p>Test again with cURL. You will get 504 just one times.</p>
<pre><code class="lang-bash">loop_frontend 15
</code></pre>
<p>Sample output</p>
<pre><code class="lang-bash">Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-q2hz9=&gt; Status:504
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
Host:backend-v1-7779cb476b-6wbsp=&gt; Status:200
</code></pre>
</li>
<li><p>Check Kiali Console. Remark that there is lightning icon at backend service. This is represent for circuit breaker.</p>
<p><img src="images/kiali-graph-cb.gif" alt=""></p>
</li>
<li><p>Set backend pod to normal status</p>
<pre><code class="lang-bash">oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d &quot; &quot; -f1) -- curl -sv http://localhost:8080/start
</code></pre>
</li>
</ul>
<h2 id="secure-with-mtls">Secure with mTLS</h2>
<h3 id="within-service-mesh">Within Service Mesh</h3>
<ul>
<li><p>Enable data plane mTLS by edit ServiceMeshControlPlane with following configuration</p>
<p><img src="images/smcp-data-plane-mtls.png" alt=""></p>
</li>
<li><p>Deploy another pod without sidecar and try to connect to anther services that is part of mesh</p>
<p><img src="images/pod-without-sidecar.png" alt=""></p>
</li>
</ul>
<h4 id="pod-liveness-and-readiness">Pod Liveness and Readiness</h4>
<ul>
<li><p>Enable Liveness nad Readiness on backend-v1</p>
<pre><code class="lang-bash">oc set probe deployment backend-v1 \
 --readiness --get-url=http://:8080/q/health/ready \
 --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=5 -n project1
  oc set probe deployment backend-v1 \
 --liveness --get-url=http://:8080/q/health/live \
 --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=5 -n project1
</code></pre>
</li>
<li><p>Check for pod status</p>
<pre><code class="lang-bash">watch oc get pods -l app=backend,version=v1 -n project1
</code></pre>
<p>Example of output</p>
<pre><code class="lang-bash">NAME                          READY   STATUS            RESTARTS   AGE
backend-v1-5846f59c84-p6tn5   1/2     CrashLoopBackOff   4          68s
</code></pre>
<p>Remark: Liveness and Readiness probe fail because kubelet cannot connect to port 8080 anymore.</p>
</li>
<li><p>Rewrite HTTP probe by annotation to deployment </p>
<pre><code class="lang-bash">oc patch deployment/backend-v1 \
-n project1 \
-p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/rewriteAppHTTPProbers&quot;:&quot;true&quot;}}}}}&apos;
</code></pre>
</li>
<li><p>Remove Liveness and Readiness probe</p>
<pre><code class="lang-bash">oc set probe deployment backend-v1 --remove --readiness --liveness -n project1
</code></pre>
</li>
</ul>
<h3 id="istio-gateway-with-mtls">Istio Gateway with mTLS</h3>
<ul>
<li><p>Create certificates and private key</p>
<pre><code class="lang-bash">mkdir -p certs
DOMAIN=$(oc whoami --show-console  | awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
CN=frontend.apps.$DOMAIN
echo &quot;Create Root CA and Private Key&quot;
openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj &apos;/O=example Inc./CN=example.com&apos; \
-keyout certs/example.com.key -out certs/example.com.crt
echo &quot;Create Certificate and Private Key for $CN&quot;
openssl req -out certs/frontend.csr -newkey rsa:2048 -nodes -keyout certs/frontend.key -subj &quot;/CN=${CN}/O=Great Department&quot;
openssl x509 -req -days 365 -CA certs/example.com.crt -CAkey certs/example.com.key -set_serial 0 -in certs/frontend.csr -out certs/frontend.crt
</code></pre>
</li>
<li><p>Create secret to store private key and certificate</p>
<pre><code class="lang-bash">oc create secret generic frontend-credential \
--from-file=tls.key=certs/frontend.key \
--from-file=tls.crt=certs/frontend.crt \
-n istio-system
</code></pre>
</li>
<li><p>Update <a href="manifests/gateway-tls.yaml">Gateway</a></p>
<pre><code class="lang-bash">cat manifests/gateway-tls.yaml|sed s/DOMAIN/$DOMAIN/g|oc apply -n project1 -f -
</code></pre>
</li>
<li><p>Verify updated gateway configuration</p>
<pre><code class="lang-bash">oc get gateway frontend-gateway -n project1 -o yaml
</code></pre>
<p>Example of output</p>
<pre><code class="lang-bash">spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - frontend.apps.cluster-27bb.27bb.sandbox664.opentlc.com
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      credentialName: frontend-credential
      mode: SIMPLE
</code></pre>
<ul>
<li>port is changed to 443 with protocol HTTPS</li>
<li>TLS mode is SIMPLE and use private key and certificate from secret name <em>frontend-credential</em> in control plane namespace</li>
<li>SIMPLE mode is for TLS. For mutual TLS use MUTUAL</li>
</ul>
</li>
<li><p>Check that route created by Istio Gateway is updated to passthrough mode</p>
<pre><code class="lang-bash">oc get route \
$(oc get route -n istio-system --no-headers -o=custom-columns=&quot;NAME:.metadata.name&quot; | grep frontend) \
-n istio-system -o jsonpath=&apos;{.spec.tls.termination}&apos;
</code></pre>
<p>Example of output</p>
<pre><code class="lang-bash">passthrough
</code></pre>
</li>
<li><p>Test with cURL</p>
<pre><code class="lang-bash">export GATEWAY_URL=$(oc get route $(oc get route -n istio-system | grep frontend | awk &apos;{print $1}&apos;) -n istio-system -o yaml  -o jsonpath=&apos;{.spec.host}&apos;)
curl -kv https://$GATEWAY_URL
</code></pre>
<p>Example of output</p>
<pre><code class="lang-bash">* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=frontend-istio-user1.apps.; O=Great Department
*  start date: Sep  1 12:10:22 2021 GMT
*  expire date: Sep  1 12:10:22 2022 GMT
*  issuer: O=example Inc.; CN=example.com
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
</code></pre>
</li>
<li><p>Create client certificate</p>
<pre><code class="lang-bash">CN=great-partner.apps.acme.com
echo &quot;Create Root CA and Private Key&quot;
openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj &apos;/O=Acme Inc./CN=acme.com&apos; \
-keyout certs/acme.com.key -out certs/acme.com.crt
echo &quot;Create Certificate and Private Key for $CN&quot;
openssl req -out certs/great-partner.csr -newkey rsa:2048 -nodes -keyout certs/great-partner.key -subj &quot;/CN=${CN}/O=Great Department&quot;
openssl x509 -req -days 365 -CA certs/acme.com.crt -CAkey certs/acme.com.key -set_serial 0 -in certs/great-partner.csr -out certs/great-partner.crt
</code></pre>
</li>
<li><p>Update frontend-credential secret</p>
<pre><code class="lang-bash">oc create secret generic frontend-credential \
--from-file=tls.key=certs/frontend.key \
--from-file=tls.crt=certs/frontend.crt \
--from-file=ca.crt=certs/acme.com.crt \
-n istio-system --dry-run=client -o yaml \
| oc replace -n istio-system  secret frontend-credential -f -
</code></pre>
</li>
<li><p>Update frontend gateway TLS mode to MUTUAL</p>
<pre><code class="lang-bash">oc patch gateway frontend-gateway -n project1 \
--type=&apos;json&apos; \
-p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/servers/0/tls/mode&quot;,&quot;value&quot;:&quot;MUTUAL&quot;}]&apos;
</code></pre>
</li>
<li><p>Test</p>
<ul>
<li><p>cURL without client certificate</p>
<pre><code class="lang-bash">curl -k https://$GATEWAY_URL
</code></pre>
<p>You will get following error</p>
<pre><code class="lang-bash">curl: (35) error:1401E410:SSL routines:CONNECT_CR_FINISHED:sslv3 alert handshake failure
</code></pre>
</li>
<li><p>cURL with Acme Inc certificate</p>
<pre><code class="lang-bash">curl -kv --cacert certs/acme.com.crt \
--cert certs/great-partner.crt \
--key certs/great-partner.key \
https://$GATEWAY_URL
</code></pre>
<p>Example of output</p>
<pre><code class="lang-bash">* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: certs/acme.com.crt
  CApath: none
...
* Server certificate:
*  subject: CN=frontend-istio-user1.apps.; O=Great Department
*  start date: Sep  1 12:10:22 2021 GMT
*  expire date: Sep  1 12:10:22 2022 GMT
*  issuer: O=example Inc.; CN=example.com
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway
...
Frontend version: 1.0.0 =&gt; [Backend: http://backend:8080, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-xp65r, Status:200, Message: Hello, Quarkus]
</code></pre>
</li>
<li><p>Generate another certificate and private key (Pirate Inc) that frontend gateway not trust</p>
<pre><code class="lang-bash">CN=bad-partner.apps.pirate.com
echo &quot;Create Root CA and Private Key&quot;
openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj &apos;/O=Pirate Inc./CN=pirate.com&apos; \
-keyout certs/pirate.com.key -out certs/pirate.com.crt
echo &quot;Create Certificate and Private Key for $CN&quot;
openssl req -out certs/bad-partner.csr -newkey rsa:2048 -nodes -keyout certs/bad-partner.key -subj &quot;/CN=${CN}/O=Bad Department&quot;
openssl x509 -req -days 365 -CA certs/pirate.com.crt -CAkey certs/pirate.com.key -set_serial 0 -in certs/bad-partner.csr -out certs/bad-partner.crt
</code></pre>
</li>
<li><p>cURL with Pirate Inc certificate</p>
<pre><code class="lang-bash">curl -k --cacert certs/pirate.com.crt \
--cert certs/bad-partner.crt \
--key certs/bad-partner.key \
https://$GATEWAY_URL
</code></pre>
<p>You will get error alert unknown ca</p>
<pre><code class="lang-bash">curl: (35) error:1401E418:SSL routines:CONNECT_CR_FINISHED:tlsv1 alert unknown ca
</code></pre>
</li>
<li><p>Update frontend gateway to trust Pirate Inc by update frontend-credential secret</p>
<pre><code class="lang-bash">cat certs/acme.com.crt &gt; certs/trusted.crt
cat certs/pirate.com.crt &gt;&gt; certs/trusted.crt
oc create secret generic frontend-credential \
  --from-file=tls.key=certs/frontend.key \
  --from-file=tls.crt=certs/frontend.crt \
  --from-file=ca.crt=certs/trusted.crt \
  -n istio-system --dry-run=client -o yaml \
  | oc replace -n istio-system secret frontend-credential -f -
</code></pre>
</li>
<li><p>Test with Pirate Inc certificate</p>
<pre><code class="lang-bash">curl -k --cacert certs/pirate.com.crt \
--cert certs/bad-partner.crt \
--key certs/bad-partner.key \
https://$GATEWAY_URL
</code></pre>
</li>
<li><p>Recheck that Acme Inc can acess frontend app</p>
<pre><code class="lang-bash">curl -kv --cacert certs/acme.com.crt \
--cert certs/great-partner.crt \
--key certs/great-partner.key \
https://$GATEWAY_URL
</code></pre>
</li>
</ul>
</li>
<li><p>Update frontend gateway TLS mode to SIMPLE</p>
<pre><code class="lang-bash">oc patch gateway frontend-gateway -n project1 \
--type=&apos;json&apos; \
-p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/servers/0/tls/mode&quot;,&quot;value&quot;:&quot;SIMPLE&quot;}]&apos;
</code></pre>
</li>
</ul>
<h2 id="jwt-token">JWT Token</h2>
<h3 id="red-hat-single-sign-on">Red Hat Single Sign-On</h3>
<ul>
<li><p>Setup Red Hat Single Sign-On (Keycloak)</p>
<ul>
<li><p>Create namespace</p>
<pre><code class="lang-bash">oc new-project sso --description=&quot;Red Hat Single Sign-On&quot; --display-name=&quot;Red Hat Single Sign-On&quot;
</code></pre>
</li>
<li><p>Install Red Hat Single Sign-On Operator</p>
<p><img src="images/rhsso-operator-hub.png" alt=""></p>
<p>Install to namespace sso</p>
<p><img src="images/rhsso-operator.png" alt=""></p>
</li>
<li><p>Create Keycloak instance</p>
<pre><code class="lang-bash">oc create -f manifests/keycloak.yaml -n sso
watch oc get pods -n sso
</code></pre>
<p>Sample outout</p>
<pre><code class="lang-bash">NAME                                   READY   STATUS              RESTARTS   AGE
keycloak-0                             0/1     PodInitializing     0          19s
keycloak-postgresql-76c57d6f5b-jgnx6   0/1     ContainerCreating   0          19s
rhsso-operator-748cdf5c96-6nwtq        1/1     Running             0          82s
</code></pre>
</li>
<li><p>Cosmetic topology view </p>
<pre><code class="lang-bash">oc label deployment/keycloak-postgresql app.kubernetes.io/name=postgresql -n sso
oc annotate statefulset/keycloak &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;keycloak-postgresql&quot;}]&apos; -n sso
</code></pre>
<p><img src="images/dev-console-rhsso-topology.png" alt=""></p>
</li>
<li><p>Extract admin password</p>
<pre><code class="lang-bash">KEYCLOAK_ADMIN_PASSWORD=$(oc extract secret/credential-demo -n sso --to=- --keys=ADMIN_PASSWORD  2&gt;/dev/null)
</code></pre>
</li>
<li><p>Create Realm and Keycloak Client with Client Credential</p>
<pre><code class="lang-bash">oc create -f manifests/keycloak-realm.yaml -n sso
oc create -f manifests/keycloak-client.yaml -n sso
</code></pre>
</li>
<li><p>Client secret</p>
<p><img src="images/rhsso-client-secret.png" alt=""></p>
</li>
<li><p>Test login with client secret</p>
<pre><code class="lang-bash">KEYCLOAK=$(oc get route/keycloak -n sso -o jsonpath=&apos;{.spec.host}&apos;)
CLIENT_SECRET=e31fe61b-2cc1-41da-9644-d72bdb8339d5
TOKEN=$(curl -s  --location --request  \
POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \
--header &apos;Content-Type: application/x-www-form-urlencoded&apos; \
--data-urlencode client_id=frontend-app \
--data-urlencode client_secret=$CLIENT_SECRET \
--data-urlencode scope=email \
--data-urlencode grant_type=client_credentials  | jq .access_token | sed s/\&quot;//g)
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="requestauthentication-and-authorization-policy">RequestAuthentication and Authorization Policy</h3>
<ul>
<li><p>Create <a href="manifests/frontend-jwt.yaml">RequestAuthentication and AuthorizationPolicy</a></p>
<pre><code class="lang-bash">oc apply -f manitests/frontend-jwt -n project1
</code></pre>
</li>
<li><p>Test without JWT token. You will get HTTP resonse code 403</p>
<pre><code class="lang-bash">curl -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com
</code></pre>
</li>
<li>Test with invalid JWT token. You will get HTTP resonse code 401</li>
<li><p>Test with valid JWT token</p>
<pre><code class="lang-bash">TOKEN=$(curl -s  --location --request  \
  POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \
  --header &apos;Content-Type: application/x-www-form-urlencoded&apos; \
  --data-urlencode client_id=frontend-app \
  --data-urlencode client_secret=$CLIENT_SECRET \
  --data-urlencode scope=email \
  --data-urlencode grant_type=client_credentials  | jq .access_token | sed s/\&quot;//g)
  curl --header &apos;Authorization: Bearer &apos;$TOKEN -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com
</code></pre>
</li>
<li><p>Test with expired JWT token (token life is 5 minutes). You will get HTTP response code 401 with following message</p>
<pre><code class="lang-bash">Jwt is expired* Closing connection 0
</code></pre>
</li>
</ul>
<h2 id="service-level-objective-slo">Service Level Objective (SLO)</h2>
<p>We can use Service Level Indicator (SLI) and Service Level Objective (SLO) to determine and measure availability of services. For RESTful Web Service we can use HTTP response code to measure for SLI</p>
<ul>
<li><p>Deploy applicaition</p>
<pre><code class="lang-bash">oc delete all --all -n project1
oc delete gateway --all -n project1
oc delete dr,vs --all -n project1
oc apply -f manifests/frontend.yaml -n project1
oc patch deployment/frontend-v1 -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos; -n project1
oc apply -f manifests/backend.yaml -n project1
oc scale deployment/frontend-v1 --replicas=5 -n project1
oc scale deployment/backend-v1 --replicas=10 -n project1
oc scale deployment/backend-v2 --replicas=10 -n project1
oc delete deployment/frontend-v2 -n project1
oc delete route frontend -n project1
oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1
oc annotate deployment frontend-v1 &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v1&quot;},{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v2&quot;}]&apos; -n project1
oc apply -f manifests/backend-destination-rule.yaml -n project1
oc apply -f manifests/backend-virtual-service-v1-v2-50-50.yaml -n project1
oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1
DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
cat manifests/frontend-gateway.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
oc patch virtualservice backend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;backend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:100},{&quot;destination&quot;:{&quot;host&quot;:&quot;backend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:0}]}}]&apos; -n project1
watch oc get pods -n project1
</code></pre>
</li>
<li><p>Generate load</p>
<ul>
<li><p>Create namespace</p>
<pre><code class="lang-bash">oc new-project load-test
</code></pre>
</li>
<li><p>Run K6 with 15 threads for 10 minutes to simulate workload</p>
<pre><code class="lang-bash">  FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
  oc run load-test -n load-test -i --rm \
  --image=loadimpact/k6 --rm=true --restart=Never \
  --  run -  &lt; manifests/load-test-k6.js \
  -e URL=http://$FRONTEND_ISTIO_ROUTE -e THREADS=15 -e DURATION=10m -e RAMPUP=1s -e RAMPDOWN=0s
</code></pre>
</li>
</ul>
</li>
<li><p>Prometheus in Service Mesh&apos;s control plane contains information about HTTP responses then we can use following PromQL to check for the sucessfull request and total request of backend service</p>
<p>Use OpenShift Developer Console, select project istio-system and open Prometheus console</p>
<ul>
<li><p>Success Rate</p>
<ul>
<li><p>Successful request for last 5 minutes</p>
<pre><code>sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;,response_code!~&quot;5*&quot;}[5m]))
</code></pre><p><img src="images/prometheus-backend-service-total-request.png" alt=""></p>
</li>
<li><p>Total requests for last 5 minutes</p>
<pre><code>sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;}[5m]))
</code></pre><!-- Sample data provided by Prometheus

```
istio_requests_total{connection_security_policy="unknown",destination_app="backend",destination_canonical_revision="v1",destination_canonical_service="backend",destination_principal="spiffe://cluster.local/ns/user1/sa/default",destination_service="backend.user1.svc.cluster.local",destination_service_name="backend",destination_service_namespace="user1",destination_version="v1",destination_workload="backend-v1",destination_workload_namespace="user1",instance="10.128.2.42:15090",job="envoy-stats",namespace="user1",pod_name="frontend-v1-66fbd89459-8ksr8",reporter="source",request_protocol="http",response_code="503",response_flags="URX",source_app="frontend",source_canonical_revision="v1",source_canonical_service="frontend",source_principal="spiffe://cluster.local/ns/user1/sa/default",source_version="v1",source_workload="frontend-v1",source_workload_namespace="user1"}
``` -->
</li>
</ul>
</li>
<li><p>Latency</p>
<ul>
<li><p>99th Percentile of response time in sec of frontend service</p>
<pre><code>histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=&quot;frontend&quot;,response_code!~&quot;5*&quot;}[5m])) by (le))/1000
</code></pre></li>
</ul>
</li>
</ul>
</li>
<li><p>SLO for success rate can be calculated by following PromQL and compare this to your desired service level e.g. 99.9%</p>
<pre><code>sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;,response_code!~&quot;5*&quot;}[5m])) / sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;}[5m]))*100
</code></pre></li>
<li><p>Login to Grafana Dashbaord in control plane and import <a href="manifests/grafana-slo-dashboard.json">SLO Dashbaord</a></p>
<ul>
<li><p>Backend Application service %availability</p>
<pre><code>sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;,response_code!~&quot;5.*&quot;}[5m])) / sum(increase(istio_requests_total{destination_service_name=&quot;backend&quot;}[5m])) *100
</code></pre></li>
<li><p>Frontend 99th percentile response time in second</p>
<pre><code>histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=&quot;frontend&quot;,response_code!~&quot;5*&quot;}[5m])) by (le))/1000
</code></pre></li>
<li><p>Backend 99th percentile response time in second</p>
<pre><code>histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=&quot;backend&quot;,response_code!~&quot;5*&quot;}[5m])) by (le))/1000
</code></pre><p><img src="images/grafana-dashboard-slo.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Run following bash script to force 5 backend-v1 pod to return 504 then set those pods to return to 200 OK.</p>
<pre><code class="lang-bash">for pod in $(oc get pods -l app=backend,version=v1 --no-headers  -o=custom-columns=&quot;NAME:.metadata.name&quot; -n project1|head -n 5|sort)
do
  oc exec -n project1 -c backend $pod -- curl -s http://localhost:8080/stop -w &quot;\n&quot;
  sleep 1
done
sleep 10
for pod in $(oc get pods -l app=backend,version=v1 --no-headers  -o=custom-columns=&quot;NAME:.metadata.name&quot; -n project1|head -n 5|sort)
do
  oc exec -n project1 -c backend $pod -- curl -s http://localhost:8080/start -w &quot;\n&quot;
done
</code></pre>
<p><img src="images/slo-success-rate-decrease.png" alt=""></p>
</li>
<li><p>Run following bash script to set traffic to backend-v2 and check both frontend and backend response time increasing. </p>
<pre><code class="lang-bash">  oc patch virtualservice backend --type=&apos;json&apos; -p=&apos;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/http/0&quot;,&quot;value&quot;:{&quot;route&quot;:[{&quot;destination&quot;:{&quot;host&quot;:&quot;backend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v1&quot;},&quot;weight&quot;:30},{&quot;destination&quot;:{&quot;host&quot;:&quot;backend.project1.svc.cluster.local&quot;,&quot;port&quot;:{&quot;number&quot;:8080},&quot;subset&quot;:&quot;v2&quot;},&quot;weight&quot;:70}]}}]&apos; -n project1
</code></pre>
<p><img src="images/slo-response-time-increase.png" alt=""></p>
</li>
</ul>
<h2 id="control-plane-with-high-availability">Control Plane with High Availability</h2>
<h3 id="openshift-service-mesh-1x">OpenShift Service Mesh 1.x</h3>
<p><a href="manifests/smcp-v1-ha.yaml">ServiceMeshControlPlane</a> with high availability configuration</p>
<ul>
<li><p>Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway</p>
<ul>
<li>Set request and limit</li>
<li>Set autoscaling to true</li>
<li><p>Set number of min and max replicas with target CPU utilization to trigger HPA</p>
<pre><code class="lang-yaml">ingress:
    enabled: true
    ingress: false
    runtime:
      container:
        resources:
          requests:
            cpu: 10m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 2048Mi
</code></pre>
</li>
</ul>
</li>
<li><p>For others components </p>
<ul>
<li><p>Set number of replicas to 2</p>
<pre><code class="lang-yaml">    deployment:
      autoScaling:
        enabled: false
      replicas: 2
</code></pre>
</li>
<li><p>Set pod anti-affinity to prevent scheduler to place pods to the same node</p>
<p><em>Remark: namespaces in podAntiAffinity is needed to support multiples control planes in the same OpenShift cluster. Change this to match name of control plane&apos;s namespace</em> </p>
<pre><code class="lang-yaml">    pod:
      tolerations:
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 60
      affinity:
        podAntiAffinity:
          requiredDuringScheduling:
          - key: istio
            topologyKey: kubernetes.io/hostname
            operator: In
            values:
            - galley
            namespaces: istio-system
</code></pre>
</li>
</ul>
</li>
<li><p>Check that pods of each deployment run on different nodes</p>
<pre><code class="lang-bash">oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME                                    NODE                                        PHASE
grafana-7bdb4fb848-847c8                ip-10-0-160-48.us-east-2.compute.internal   Running
istio-citadel-6668b5b947-njgbb          ip-10-0-160-48.us-east-2.compute.internal   Running
istio-citadel-6668b5b947-nk9dz          ip-10-0-137-21.us-east-2.compute.internal   Running
istio-galley-6dc7f9c496-hkm57           ip-10-0-137-21.us-east-2.compute.internal   Running
istio-galley-6dc7f9c496-qcw9q           ip-10-0-160-48.us-east-2.compute.internal   Running
istio-ingressgateway-6bcd484457-25tq7   ip-10-0-137-21.us-east-2.compute.internal   Running
istio-ingressgateway-6bcd484457-nvfb9   ip-10-0-160-48.us-east-2.compute.internal   Running
istio-pilot-74d5db759c-m9jxm            ip-10-0-137-21.us-east-2.compute.internal   Running
istio-pilot-74d5db759c-rcdxj            ip-10-0-160-48.us-east-2.compute.internal   Running
istio-policy-58ff56d7dc-26wsq           ip-10-0-137-21.us-east-2.compute.internal   Running
istio-policy-58ff56d7dc-62gwl           ip-10-0-160-48.us-east-2.compute.internal   Running
istio-sidecar-injector-ffc58c87-4t5gc   ip-10-0-137-21.us-east-2.compute.internal   Running
istio-sidecar-injector-ffc58c87-rjz7l   ip-10-0-160-48.us-east-2.compute.internal   Running
istio-telemetry-646d7cf56c-fz72g        ip-10-0-137-21.us-east-2.compute.internal   Running
istio-telemetry-646d7cf56c-lctxg        ip-10-0-160-48.us-east-2.compute.internal   Running
jaeger-7b866d475f-nhrp5                 ip-10-0-160-48.us-east-2.compute.internal   Running
kiali-75dc58b5f6-bwk7q                  ip-10-0-137-21.us-east-2.compute.internal   Running
prometheus-85db9d786b-vzskf             ip-10-0-160-48.us-east-2.compute.internal   Running
prometheus-85db9d786b-wgrwz             ip-10-0-137-21.us-east-2.compute.internal   Running
</code></pre>
</li>
<li><p>Verify HPA for ingress gateway</p>
<pre><code class="lang-bash">oc get hpa -n istio-system
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME                   REFERENCE                         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
istio-ingressgateway   Deployment/istio-ingressgateway   0%/85%    2         4         2          10m
</code></pre>
</li>
</ul>
<h3 id="openshift-service-mesh-2x">OpenShift Service Mesh 2.x</h3>
<p><a href="manifests/smcp-ha.yaml">ServiceMeshControlPlane</a> with high availability configuration</p>
<ul>
<li><p>Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway</p>
<ul>
<li>Set request and limit</li>
<li>Set autoscaling to true</li>
<li><p>Set number of min and max replicas with target CPU utilization to trigger HPA</p>
<pre><code class="lang-yaml">ingress:
  enabled: true
  runtime:
    container:
      resources:
        requests:
          cpu: 500m
          memory: 300Mi
        limits:
          cpu: 2
          memory: 1Gi
    deployment:
      autoScaling:
        enabled: true
        maxReplicas: 4
        minReplicas: 2
        targetCPUUtilizationPercentage: 85
</code></pre>
</li>
</ul>
</li>
<li><p>For others components </p>
<ul>
<li><p>Set number of replicas to 2</p>
<pre><code class="lang-yaml">pilot:
  deployment:
    replicas: 2
</code></pre>
</li>
</ul>
<!-- - Set Pod Disruption Budget (PDB) with minAvailable to 1

  ```yaml
  defaults:
    deployment:
      podDisruption:
        enabled: true
        minAvailable: 1
  ``` -->
</li>
<li><p>Check that pods of each deployment run on different nodes</p>
<pre><code class="lang-bash">oc get pods -n istio-system
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">grafana-78f656547-gkm92                 2/2     Running   0          54s
istio-ingressgateway-667749f4bd-pfl2l   1/1     Running   0          54s
istio-ingressgateway-667749f4bd-sfwx4   1/1     Running   0          39s
istiod-basic-install-6994d86579-4n8jf   1/1     Running   0          77s
istiod-basic-install-6994d86579-b5bgv   1/1     Running   0          77s
jaeger-85d4744d8b-krqfl                 2/2     Running   0          54s
kiali-784df775f8-xccsw                  1/1     Running   0          28s
prometheus-79ff59d59f-6j99k             3/3     Running   0          65s
prometheus-79ff59d59f-msrpb             3/3     Running   0          65s
</code></pre>
</li>
<li><p>Verify HPA for ingress gateway</p>
<pre><code class="lang-bash">oc get hpa -n istio-system
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME                   REFERENCE                         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
istio-ingressgateway   Deployment/istio-ingressgateway   0%/85%    2         4         2          10m
</code></pre>
</li>
<li><p>Check that pods of each deployment run on different nodes</p>
<pre><code class="lang-bash">oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase
</code></pre>
<p>Output</p>
<pre><code class="lang-bash">NAME                                    NODE                                         PHASE
grafana-99f7c499f-jnd9k                 ip-10-0-166-202.us-east-2.compute.internal   Running
istio-ingressgateway-5fc94885b5-hjhqw   ip-10-0-166-202.us-east-2.compute.internal   Running
istio-ingressgateway-5fc94885b5-hxn9r   ip-10-0-151-28.us-east-2.compute.internal    Running
istiod-basic-install-58c9bc5bf8-4wbhq   ip-10-0-151-28.us-east-2.compute.internal    Running
jaeger-596448d54d-gwn97                 ip-10-0-166-202.us-east-2.compute.internal   Running
kiali-85c677967c-k7767                  ip-10-0-166-202.us-east-2.compute.internal   Running
prometheus-565c997f45-plqqb             ip-10-0-151-28.us-east-2.compute.internal    Running
prometheus-565c997f45-s7q2t             ip-10-0-166-202.us-east-2.compute.internal   Running
</code></pre>
</li>
</ul>
<!-- - Verify PDB for istiod

  ```bash
  oc describe pdb/istiod-basic-install
  ```

  Output

  ```bash
  Name:           istiod-basic-install
  Namespace:      istio-system
  Min available:  1
  Selector:       app=istiod,istio.io/rev=basic-install
  Status:
      Allowed disruptions:  1
      Current:              2
      Desired:              1
      Total:                2
  Events:
    Type    Reason  Age                    From               Message
    ----    ------  ----                   ----               -------
    Normal  NoPods  2m17s (x2 over 2m17s)  controllermanager  No matching pods found
  ``` -->
<h2 id="rate-limit">Rate Limit</h2>
<h3 id="openshift-service-mesh-20x-or-istio-16x">OpenShift Service Mesh 2.0.x or Istio 1.6.x</h3>
<p>Support Rate Limiting feature in OSSM 2.1.x, OSSM 2.0.x was built on upstream istio 1.6 and we tested Rating Limiting case by</p>
<ol>
<li><p>Enable SMCP Policy Mixer Plugins: (change smcp/basic to another your control plane)</p>
<pre><code class="lang-bash">oc patch -n istio-system smcp/basic --type merge -p &apos;{&quot;spec&quot;:{&quot;policy&quot;:{&quot;type&quot;: &quot;Mixer&quot;, &quot;mixer&quot;:{&quot;enableChecks&quot;:true}}}}&apos;
</code></pre>
<p>wait until operator create istio-policy pod complete.</p>
<p>  <img src="images/smcp-policy.jpg" alt=""></p>
</li>
<li><p>Create Sample Microservice for Test this feature</p>
<pre><code class="lang-bash">oc delete all --all -n project1
oc delete gateway --all -n project1
oc delete dr,vs --all -n project1
oc apply -f manifests/frontend.yaml -n project1
oc patch deployment/frontend-v1 -p &apos;{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;sidecar.istio.io/inject&quot;:&quot;true&quot;}}}}}&apos; -n project1
oc apply -f manifests/backend.yaml -n project1
oc delete deployment/frontend-v2 -n project1
oc delete deployment/backend-v2 -n project1
oc delete route frontend -n project1
oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1
oc annotate deployment frontend-v1 &apos;app.openshift.io/connects-to=[{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v1&quot;},{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;name&quot;:&quot;backend-v2&quot;}]&apos; -n project1
oc scale deployment backend-v1 --replicas=3 -n project1
oc apply -f manifests/backend-destination-rule-v1-only.yaml -n project1
oc apply -f manifests/backend-virtual-service.yaml -n project1
oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1
DOMAIN=$(oc whoami --show-console|awk -F&apos;apps.&apos; &apos;{print $2}&apos;)
cat manifests/frontend-virtual-service.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
cat manifests/frontend-gateway.yaml | sed &apos;s/DOMAIN/&apos;$DOMAIN&apos;/&apos;|oc apply -n project1 -f -
watch oc get pods -n project1
</code></pre>
<p>after finish</p>
<p><img src="images/dev-console-app-for-cb.png" alt=""></p>
</li>
</ol>
<p>   Test with cURL</p>
<pre><code class="lang-bash">   FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk &apos;{print $2}&apos;)
   curl http://$FRONTEND_ISTIO_ROUTE
</code></pre>
<p>   Sample output - Check for field Host that is backend pod that processed for this request</p>
<pre><code class="lang-bash">   Frontend version: 1.0.0 =&gt; [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Hello, Quarkus]
</code></pre>
<ol>
<li><p>Follow upstream istio 1.6 Rate Limiting documentation for test rate limit : <a href="https://istio.io/v1.6/docs/tasks/policy-enforcement/rate-limiting/" target="_blank">https://istio.io/v1.6/docs/tasks/policy-enforcement/rate-limiting/</a> or use this example (memquota example)</p>
<ul>
<li><p>create rate limit configuration (QuotaSpecBinding, QuotaSpec, instance, rule, handler), see detail in <a href="manifests/mesh/rate-limit-memquota.yaml">rate-limit-memquota.yaml</a></p>
<pre><code class="lang-bash">oc apply -f manifests/mesh/rate-limit-memquota.yaml
</code></pre>
</li>
<li><p>set handler for manage receive only 1 req/min</p>
<pre><code class="lang-yaml">apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: quotahandler
  namespace: istio-system
spec:
  compiledAdapter: memquota
  params:
    quotas:
    - name: requestcountquota.instance.istio-system
      maxAmount: 1
      validDuration: 60s
</code></pre>
</li>
<li><p>set service for rate limit</p>
<pre><code class="lang-yaml">apiVersion: config.istio.io/v1alpha2
kind: QuotaSpecBinding
metadata:
  name: request-count
  namespace: istio-system
spec:
  quotaSpecs:
  - name: request-count
    namespace: istio-system
  services:
  - name: frontend
    namespace: project1 # default
  #- service: &apos;*&apos;  # Uncomment this to bind *all* services to request-count
</code></pre>
</li>
</ul>
</li>
<li><p>Test with cURL</p>
<ul>
<li><p>call first time</p>
<pre><code class="lang-bash">curl -v http://$FRONTEND_ISTIO_ROUTE
</code></pre>
<p>example result </p>
<pre><code class="lang-bash">*   Trying 3.131.170.108...
* TCP_NODELAY set
* Connected to frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com (3.131.170.108) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com
&gt; User-Agent: curl/7.64.1
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; x-correlation-id: 79e46c33-987c-49a4-8c4b-5844d1ab1095
&lt; x-powered-by: Express
&lt; content-type: text/html; charset=utf-8
&lt; content-length: 181
&lt; etag: W/&quot;b5-44EZPxVbVPm/rx6FVPtx2E/v4RQ&quot;
&lt; date: Wed, 06 Oct 2021 08:35:18 GMT
&lt; x-envoy-upstream-service-time: 73
&lt; server: istio-envoy
&lt; set-cookie: 56146e318a5c046b870cb6cd1fd45ebf=9e6c14811364e963dccffa42a2e0f2f2; path=/; HttpOnly
&lt; cache-control: private
&lt;
* Connection #0 to host frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com left intact
Frontend version: v1 =&gt; [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-79668c5b99-tkqjn, Status:200, Message: Hello, Quarkus]* Closing connection 0
</code></pre>
</li>
<li><p>call it again for check rate limit, openshift service mesh will return HTTP Status 429 Too Many Requests</p>
<pre><code class="lang-bash">curl -v http://$FRONTEND_ISTIO_ROUTE
</code></pre>
<p>example result </p>
<pre><code class="lang-bash">*   Trying 3.131.170.108...
* TCP_NODELAY set
* Connected to frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com (3.131.170.108) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com
&gt; User-Agent: curl/7.64.1
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 429 Too Many Requests
&lt; content-length: 60
&lt; content-type: text/plain
&lt; date: Wed, 06 Oct 2021 08:36:42 GMT
&lt; server: istio-envoy
&lt; set-cookie: 56146e318a5c046b870cb6cd1fd45ebf=9e6c14811364e963dccffa42a2e0f2f2; path=/; HttpOnly
&lt;
* Connection #0 to host frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com left intact
RESOURCE_EXHAUSTED:Quota is exhausted for: requestcountquota* Closing connection 0
</code></pre>
</li>
<li><p>retest again after 1 minute, service will back response with HTTP Status 200</p>
</li>
</ul>
</li>
</ol>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="kustomize.html" class="navigation navigation-prev " aria-label="Previous page: Kustomize">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="application-metrics.html" class="navigation navigation-next " aria-label="Next page: User Workload Monitoring">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"OpenShift Service Mesh","level":"1.5.10","depth":2,"next":{"title":"User Workload Monitoring","level":"1.5.11","depth":2,"path":"application-metrics.md","ref":"application-metrics.md","articles":[]},"previous":{"title":"Kustomize","level":"1.5.9","depth":2,"path":"kustomize.md","ref":"kustomize.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-lunr","-search","-highlight","-livereload","search-plus","copy-code-button","mermaid-gb3"],"root":"./","styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"mermaid-newface":{"theme":"neutral"},"search-plus":{},"copy-code-button":{},"mermaid-gb3":{},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Red Hat Thailand SA","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"OpenShift Demo","gitbook":"3.2.3"},"file":{"path":"openshift-service-mesh.md","mtime":"2022-06-10T08:04:29.091Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2022-06-10T08:05:15.007Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-plus/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    <script src="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>

    </body>
</html>

