{"./":{"url":"./","title":"Introduction","keywords":"","body":"OpenShift Demo Gitbook Chick here for Gitbook Table of Contents Infrastructure OpenShift Authentication Providers with AD OpenShift External Authentication Provider LDAP group sync Group Policy Access and Projects collaboration OpenShift MachineSet and Infrastructure Nodes MachineSet on VMware Infrastructure Node and Moving Cluster Services to Infra Nodes OpenShift Platform Monitoring and Alert Monitoring Stack AlertRules and Alert Receiver OpenShift Cluster Logging Loki OpenShift Networking Network Policy OpenShift state backup with etcd snapshot Pod Taint and Toleration Assign pod to node Custom Roles and Service Account Custom Alert Compliance Network ObservabilityMulti-cluster Management with Advanced Cluster Management (RHACM) Application Manageement Cost saving with hibernating OpenShiftContainer Applications Application Build & Deployment Developer Console Command Line with oc Command Line with odo Helm Image Streams OpenShift Route Blue/Green Deployment Canary Deployment Configure TLS version Horizontal Pod Autoscaler HPA by CPU HPA by Memory HPA by Custom Metrics Health Check Readiness Probe Liveness Probe Startup Probe Kustomize User Workload Monitoring Setup User Workload Monitoring Monitor Custom Metrics Custom Grafana Dashboard Custom Alert OpenTelemetry with Tempo Build Container with OC command Build Container with OpenShift DO (odo) CI/CD with Jenkins Build Quarkus App Pull artifacts from Nexus Unit Test Code Quality Push container image to Nexus or internal registry Blue/Green Deployment CI/CD with Azure DevOps Azure DevOps Deploy Back App Deploy Front App Prepare Harbor On Kubernetes/OpenShift Prepare Azure DevOps Service Connection Azure pipelines EAP on OpenShift gRPC or HTTP/2 Ingress Connectivity in OpenShift Advanced Cluster Security for Kubernetes ACS Additional Solutions OpenShift GitOps OpenShift Service Mesh Install and configure control plane Sidecar injection Blue/Green Deployment Canary Deployment A/B Testing Deployment Routing by URI with regular expression Traffic Analysis Traffic Mirroring Tracing Circuit Breaker Secure with mTLS JWT Token (with RHSSO) Service Level Objective (SLO) Control Plane with High Availability Rate Limit (OSSM 2.0.x or ISTIO 1.6) "},"infrastructure-authentication-providers.html":{"url":"infrastructure-authentication-providers.html","title":"OpenShift Authentication Providers with AD","keywords":"","body":"Authentication Providers with AD Authentication Providers with AD Prerequisites OpenShift RBAC with AD Background: LDAP Structure Examine the OAuth configuration Syncing LDAP Groups to OpenShift Groups Change Group Policy Examine cluster-admin policy Examine cluster-reader policy Create Projects for Collaboration Map Groups to Projects Examine Group Access Prometheus Prerequisites Microsoft AD (with LDAP protocol) Users and Groups Assigned Roles OpenShift RBAC with AD Configuring External Authentication Providers OpenShift supports a number of different authentication providers, and you can find the complete list in the understanding identity provider configuration. One of the most commonly used authentication providers is LDAP, whether provided by Microsoft Active Directory or by other sources. OpenShift can perform user authentication against an LDAP server, and can also configure group membership and certain RBAC attributes based on LDAP group membership. Background: LDAP Structure In this environment we are providing LDAP with the following user groups: ocp-user: Users with OpenShift access Any users who should be able to log-in to OpenShift must be members of this group All of the below mentioned users are in this group ocp-normal-dev: Normal OpenShift users Regular users of OpenShift without special permissions Contains: normaluser1, teamuser1, teamuser2 ocp-fancy-dev: Fancy OpenShift users Users of OpenShift that are granted some special privileges Contains: fancyuser1, fancyuser2 ocp-teamed-app: Teamed app users A group of users that will have access to the same OpenShift Project Contains: teamuser1, teamuser2 Examine the OAuth configuration Since this is a pure, vanilla OpenShift 4 installation, it has the default OAuth resource. You can examine that OAuth configuration with the following: oc get oauth cluster -o yaml You will see something like: apiVersion: config.openshift.io/v1 kind: OAuth metadata: annotations: release.openshift.io/create-only: \"true\" creationTimestamp: \"2020-03-17T18:12:52Z\" generation: 1 name: cluster resourceVersion: \"1563\" selfLink: /apis/config.openshift.io/v1/oauths/cluster uid: ebb0582d-b0e4-4c40-a33f-12459593f8e2 spec: {} There are a few things to note here. Firstly, there's basically nothing here! How does the kubeadmin user work, then? The OpenShift OAuth system knows to look for a kubeadmin Secret in the kube-system Namespace. You can examine it with the following: oc get secret -n kube-system kubeadmin -o yaml You will see something like: apiVersion: v1 data: kubeadmin: JDJhJDEwJDdQNHZtbXMxdmpDa3FsNlJMLjJBcC5BSWdBazB6d09IWUdXZEdrRXBERGRwWXNmVVcxanpX kind: Secret metadata: creationTimestamp: \"2019-04-29T17:30:51Z\" name: kubeadmin namespace: kube-system resourceVersion: \"2065\" selfLink: /api/v1/namespaces/kube-system/secrets/kubeadmin uid: 892945dc-6aa4-11e9-9959-02774c6d6b2e type: Opaque That Secret contains the encoded hash of the kubeadmin password. This account will continue to work even after we configure a new OAuth. If you want to disable it, you would need to delete the secret. In a real-world environment, you will likely want to integrate with your existing identity management solution. For this lab we are configuring LDAP as our identityProvider. Here's an example of the OAuth configuration. Look for the element in identityProviders with type: LDAP like the following: apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldap challenge: false login: true mappingMethod: claim type: LDAP ldap: attributes: id: - distinguishedName email: - userPrincipalName name: - givenName preferredUsername: - sAMAccountName bindDN: \"cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com\" bindPassword: name: ldapuser-secret insecure: true url: \"ldap://ad1.dcloud.demo.com:389/cn=Users,dc=dcloud,dc=demo,dc=com?sAMAccountName?sub?(memberOf=cn=ocp-user,cn=Users,dc=dcloud,dc=demo,dc=com)\" tokenConfig: accessTokenMaxAgeSeconds: 86400 Some notable fields under identityProviders:: name: The unique ID of the identity provider. It is possible to have multiple authentication providers in an OpenShift environment, and OpenShift is able to distinguish between them. mappingMethod: claim: This section has to do with how usernames are assigned within an OpenShift cluster when multiple providers are configured. See the Identity provider parameters section for more information. attributes: This section defines the LDAP fields to iterate over and assign to the fields in the OpenShift user's \"account\". If any attributes are not found / not populated when searching through the list, the entire authentication fails. In this case we are creating an identity that is associated with the AD distinguishedName, an email address from the LDAP userPrincipalName, a name from the LDAP givenName, and a username from the AD sAMAccountName. bindDN: When searching LDAP, bind to the server as this user. bindPassword: Reference to the Secret that has the password to use when binding for searching. url: Identifies the LDAP server and the search to perform. For more information on the specific details of LDAP authentication in OpenShift you can refer to the Configuring an LDAP identity provider documentation. To setup the LDAP identity provider we must: Create a Secret with the bind password. Update the cluster OAuth object with the LDAP identity provider. As the kubeadmin user apply the OAuth configuration with oc. oc create secret generic ldapuser-secret --from-literal=bindPassword=b1ndP^ssword -n openshift-config cat Syncing LDAP Groups to OpenShift Groups In OpenShift, groups can be used to manage users and control permissions for multiple users at once. There is a section in the documentation on how to sync groups with LDAP. Syncing groups involves running a program called groupsync when logged into OpenShift as a user with cluster-admin privileges, and using a configuration file that tells OpenShift what to do with the users it finds in the various groups. We have provided a groupsync configuration file for you: View configuration file kind: LDAPSyncConfig apiVersion: v1 url: ldap://ad1.dcloud.demo.com:389 insecure: true bindDN: cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com bindPassword: b1ndP^ssword rfc2307: groupsQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (cn=ocp-*) scope: sub pageSize: 0 groupUIDAttribute: distinguishedName groupNameAttributes: - cn groupMembershipAttributes: - member usersQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (objectclass=user) scope: sub pageSize: 0 userUIDAttribute: distinguishedName userNameAttributes: - sAMAccountName Without going into too much detail (you can look at the documentation), the groupsync config file does the following: searches LDAP using the specified bind user and password queries for any LDAP groups whocp name begins with ocp- creates OpenShift groups with a name from the cn of the LDAP group finds the members of the LDAP group and then puts them into the created OpenShift group uses the dn and uid as the UID and name attributes, respectively, in OpenShift Execute the groupsync: cat groupsync.yaml kind: LDAPSyncConfig apiVersion: v1 url: ldap://ad1.dcloud.demo.com:389 insecure: true bindDN: cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com bindPassword: b1ndP^ssword rfc2307: groupsQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (cn=ocp-*) scope: sub pageSize: 0 groupUIDAttribute: distinguishedName groupNameAttributes: - cn groupMembershipAttributes: - member usersQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (objectclass=user) scope: sub pageSize: 0 userUIDAttribute: distinguishedName userNameAttributes: - sAMAccountName EOF oc adm groups sync --sync-config=./groupsync.yaml --confirm You will see output like the following: group/ocp-fancy-dev group/ocp-user group/ocp-normal-dev group/ocp-teamed-app What you are seeing is the Group objects that have been created by the groupsync command. If you are curious about the --confirm flag, check the output of the help with oc adm groups sync -h. If you want to see the Groups that were created, execute the following: oc get groups You will see output like the following: NAME USERS ocp-admin ldapuser ocp-fancy-dev fancyuser1, fancyuser2 ocp-normal-dev normaluser1, teamuser1, teamuser2 ocp-teamed-app teamuser1, teamuser2 ocp-user fancyuser1, fancyuser2, normaluser1, teamuser1, teamuser2 Take a look at a specific group in YAML: oc get group ocp-fancy-dev -o yaml The YAML looks like: apiVersion: user.openshift.io/v1 kind: Group metadata: annotations: openshift.io/ldap.sync-time: 2020-03-11T10:57:03-0400 openshift.io/ldap.uid: cn=ocp-fancy-dev,ou=Users,o=5e615ba46b812e7da02e93b5,dc=jumpcloud,dc=com openshift.io/ldap.url: ldap.jumpcloud.com:636 creationTimestamp: \"2020-03-11T14:57:03Z\" labels: openshift.io/ldap.host: ldap.jumpcloud.com name: ocp-fancy-dev resourceVersion: \"48481\" selfLink: /apis/user.openshift.io/v1/groups/ocp-fancy-dev uid: 630a9d2b-b577-46bd-8294-6b26e7f9a6e1 users: - fancyuser1 - fancyuser2 OpenShift has automatically associated some LDAP metadata with the Group, and has listed the users who are in the group. What happens if you list the Users? oc get user You will get: No resources found. Why would there be no Users found? They are clearly listed in the Group definition. Users are not actually created until the first time they try to log in. What you are seeing in the Group definition is simply a placeholder telling OpenShift that, if it encounters a User with that specific ID, that it should be associated with the Group. Change Group Policy We will grant a cluster role cluster-admin to ldap group ocp-admin Change the policy for the ocp-admin Group: oc adm policy add-cluster-role-to-group cluster-admin ocp-admin In your environment, there is a special group of super developers called ocp-fancy-dev who should have special cluster-reader privileges. This is a role that allows a user to view administrative-level information about the cluster. For example, they can see the list of all Projects in the cluster. Change the policy for the ocp-fancy-dev Group: oc adm policy add-cluster-role-to-group cluster-reader ocp-fancy-dev Note: If you are interested in the different roles that come with OpenShift, you can learn more about them in the role-based access control (RBAC) documentation. Examine cluster-admin policy login as a ldapuser oc login -u ldapuser -p b1ndP^ssword Then, try to list Projects: oc get projects You will see a full list of projects. Examine cluster-reader policy Go ahead and login as a regular user: oc login -u normaluser1 -p openshift Then, try to list Projects: oc get projects You will see: No resources found. Now, login as a member of ocp-fancy-dev: oc login -u fancyuser1 -p openshift And then perform the same oc get projects and you will now see the list of all of the projects in the cluster: NAME DISPLAY NAME STATUS app-management * default kube-public kube-system labguide openshift openshift-apiserver ... You should now be starting to understand how RBAC in OpenShift Container Platform can work. Create Projects for Collaboration Make sure you login as the cluster administrator: oc login -u ldapuser Then, create several Projects for people to collaborate: oc adm new-project app-dev --display-name=\"Application Development\" oc adm new-project app-test --display-name=\"Application Testing\" oc adm new-project app-prod --display-name=\"Application Production\" You have now created several Projects that represent a typical Software Development Lifecycle setup. Next, you will configure Groups to grant collaborative access to these projects. Note: Creating projects with oc adm new-project does not use the project request process or the project request template. These projects will not have quotas or limitranges applied by default. A cluster administrator can \"impersonate\" other users, so there are several options if you wanted these projects to get quotas/limit ranges: . use --as to specify impersonating a regular user with oc new-project . use oc process and provide values for the project request template, piping into create (eg: oc process ... | oc create -f -). This will create all of the objects in the project request template, which would include the quota and limit range. . manually create/define the quota and limit ranges after creating the projects. For these exercises it is not important to have quotas or limit ranges on these projects. Map Groups to Projects As you saw earlier, there are several roles within OpenShift that are preconfigured. When it comes to Projects, you similarly can grant view, edit, or administrative access. Let's give our ocp-teamed-app users access to edit the development and testing projects: oc adm policy add-role-to-group edit ocp-teamed-app -n app-dev oc adm policy add-role-to-group edit ocp-teamed-app -n app-test And then give them access to view production: oc adm policy add-role-to-group view ocp-teamed-app -n app-prod Now, give the ocp-fancy-dev group edit access to the production project: oc adm policy add-role-to-group edit ocp-fancy-dev -n app-prod Examine Group Access Log in as normaluser1 and see what Projects you can see: oc login -u normaluser1 -p openshift oc get projects You should get: No resources found. Then, try teamuser1 from the ocp-teamed-app group: oc login -u teamuser1 -p openshift oc get projects You should get: NAME DISPLAY NAME STATUS app-dev Application Development Active app-prod Application Production Active app-test Application Testing Active You did not grant the team users edit access to the production project. Go ahead and try to create something in the production project as teamuser1: oc project app-prod oc new-app docker.io/siamaksade/mapit You will see that it will not work: error: can't lookup images: imagestreamimports.image.openshift.io is forbidden: User \"teamuser1\" cannot create resource \"imagestreamimports\" in API group \"image.openshift.io\" in the namespace \"app-prod\" error: local file access failed with: stat docker.io/siamaksade/mapit: no such file or directory error: unable to locate any images in image streams, templates loaded in accessible projects, template files, local docker images with name \"docker.io/siamaksade/mapit\" Argument 'docker.io/siamaksade/mapit' was classified as an image, image~source, or loaded template reference. The 'oc new-app' command will match arguments to the following types: 1. Images tagged into image streams in the current project or the 'openshift' project - if you don't specify a tag, we'll add ':latest' 2. Images in the Docker Hub, on remote registries, or on the local Docker engine 3. Templates in the current project or the 'openshift' project 4. Git repository URLs or local paths that point to Git repositories --allow-missing-images can be used to point to an image that does not exist yet. See 'oc new-app -h' for examples. This failure is exactly what we wanted to see. Prometheus Now that you have a user with cluster-reader privileges (one that can see many administrative aspects of the cluster), we can revisit Prometheus and attempt to log-in to it. Login as a the user with cluster-reader privileges: oc login -u fancyuser1 -p openshift Find the prometheus Route with the following command: oc get route prometheus-k8s -n openshift-monitoring You will see something like the following: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD prometheus-k8s prometheus-k8s-openshift-monitoring.{{ ROUTE_SUBDOMAIN }} prometheus-k8s web reencrypt/Redirect None Warning: Before continuing, make sure to go to the OpenShift web console and log out by using the dropdown menu at the upper right where it says kube:admin. Otherwise Prometheus will try to use your kubeadmin user to pass through authentication. While it will work, it doesn't demonstrate the cluster-reader role. The installer configured a Route for Prometheus by default. Go ahead and control+click the Prometheus link to open it in your browser. You'll be greeted with a login screen. Click the Log in with OpenShift button, then select the ldap auth mechanism, and use the fancyuser1 user that you gave cluster-reader privileges to earlier. More specifically, the ocp-fancy-dev group has cluster-reader permissions, and fancyuser1 is a member. Remember that the password for all of these users is openshift. You will probably get a certificate error because of the self-signed certificate. Make sure to accept it. After logging in, the first time you will be presented with an auth proxy permissions acknowledgement. There is actually an OAuth proxy that sits in the flow between you and the Prometheus container. This proxy is used to validate your AuthenticatioN (AuthN) as well as authorize (AuthZ) what is allowed to happen. Here you are explicitly authorizing the permissions from your fancyuser1 account to be used as part of accessing Prometheus. Hit Allow selected permissions. At this point you are viewing Prometheus. There are no alerts configured. If you look at Status and then Targets you can see some interesting information about the current state of the cluster. "},"infrastructure-infra-nodes.html":{"url":"infrastructure-infra-nodes.html","title":"OpenShift MachineSet and Infrastructure Nodes","keywords":"","body":"OpenShift Infrastructure Nodes OpenShift Infrastructure Nodes Prerequisites OpenShift Infrastructure Nodes OpenShift MachineSet Defining a Custom MachineSet Logging Machine Config MachineConfig overview Checking Machine Config Pool status Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI Resources enough to provision 3 infra nodes e.g. 4vCPU, 16GB RAM per node OpenShift Infrastructure Nodes The OpenShift subscription model allows customers to run various core infrastructure components at no additional charge. In other words, a node that is only running core OpenShift infrastructure components is not counted in terms of the total number of subscriptions required to cover the environment. OpenShift components that fall into the infrastructure categorization include: kubernetes and OpenShift control plane services (\"masters\") router container image registry cluster metrics collection (\"monitoring\") cluster aggregated logging service brokers Any node running a container/pod/component not described above is considered a worker and must be covered by a subscription. OpenShift MachineSet In the case of an infrastructure node, we want to create additional Machines that have specific Kubernetes labels. Then, we can configure the various infrastructure components to run specifically on nodes with those labels. To accomplish this, you will create additional MachineSets. In order to understand how MachineSets work, run the following. This will allow you to follow along with some of the following discussion. oc get machineset -n openshift-machine-api -o yaml $(oc get machineset -n openshift-machine-api | grep worker | cut -d' ' -f 1) Sample Output apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: creationTimestamp: \"2020-12-28T05:02:31Z\" generation: 3 labels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 name: ocp01-7k4c4-worker namespace: openshift-machine-api resourceVersion: \"799241\" selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/ocp01-7k4c4-worker uid: 52ad683d-99b6-423b-b045-4279f241640e spec: replicas: 1 selector: matchLabels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 machine.openshift.io/cluster-api-machineset: ocp01-7k4c4-worker template: metadata: labels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker machine.openshift.io/cluster-api-machineset: ocp01-7k4c4-worker spec: metadata: {} providerSpec: value: apiVersion: vsphereprovider.openshift.io/v1beta1 credentialsSecret: name: vsphere-cloud-credentials diskGiB: 120 kind: VSphereMachineProviderSpec memoryMiB: 8192 metadata: creationTimestamp: null network: devices: - networkName: VM Network numCPUs: 2 numCoresPerSocket: 2 snapshot: \"\" template: ocp01-7k4c4-rhcos userDataSecret: name: worker-user-data workspace: datacenter: dCloud-DC datastore: NFS_Datastore folder: /dCloud-DC/vm/ocp01-7k4c4 resourcePool: /dCloud-DC/host/dCloud-Cluster/Resources server: vc1.dcloud.cisco.com status: availableReplicas: 1 fullyLabeledReplicas: 1 observedGeneration: 3 readyReplicas: 1 replicas: 1 Important information in MachineSet Metadata The metadata on the MachineSet itself includes information like the name of the MachineSet and various labels. Selector The MachineSet defines how to create Machines, and the Selector tells the operator which machines are associated with the set Template Metadata The template is the part of the MachineSet that templates out the Machine. The template itself can have metadata associated, and we need to make sure that things match here when we make changes: Template Spec The template needs to specify how the Machine/Node should be created. You will notice that the spec and, more specifically, the providerSpec contains all of the important AWS data to help get the Machine created correctly and bootstrapped. In our case, we want to ensure that the resulting node inherits one or more specific labels. As you’ve seen in the examples above, labels go in metadata sections: Defining a Custom MachineSet Now that you’ve analyzed an existing MachineSet it’s time to go over the rules for creating one, at least for a simple change like we’re making: Don’t change anything in the providerSpec Don’t change any instances of machine.openshift.io/cluster-api-cluster: Give your MachineSet a unique name Make sure any instances of machine.openshift.io/cluster-api-machineset match the name Add labels you want on the nodes to .spec.template.spec.metadata.labels Even though you’re changing MachineSet name references, be sure not to change the subnet. This sounds complicated, but we have a template that will do the hard work for you: export CLUSTERID=$(oc get machineset -n openshift-machine-api | grep worker | cut -d' ' -f 1 | sed 's/-worker//g') cat oc get machineset -n openshift-machine-api You should see the new infra set listed with a name similar to the following: ocp01-7k4c4-infra 1 1 13s We don’t yet have any ready or available machines in the set because the instances are still coming up and bootstrapping. You can check oc get machine -n openshift-machine-api to see when the instance finally starts running. Then, you can use oc get node to see when the actual node is joined and ready. Note: It can take several minutes for a Machine to be prepared and added as a Node. oc get nodes NAME STATUS ROLES AGE VERSION ocp01-7k4c4-infra-tz8w4 Ready infra,worker 18m v1.19.0+7070803 ocp01-7k4c4-master-0 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-master-1 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-master-2 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-worker-wbq9b Ready worker 56m v1.19.0+7070803 ocp01-7k4c4-worker-zw9w8 Ready worker 2d v1.19.0+7070803 If you’re having trouble figuring out which node is the new one, take a look at the AGE column. It will be the youngest! Also, in the ROLES column you will notice that the new node has both a worker and an infra role. For the HA, we will need 3 infra nodes. export $INFRAMS=$(oc get machineset -n openshift-machine-api | grep infra | cut -d' ' -f 1) oc scale machineset $INFRAMS -n openshift-machine-api --replicas=3 Binding infrastructure node workloads using taints and tolerations If you have an infra node that has the infra and worker roles assigned, you must configure the node so that user workloads are not assigned to it. Use the following command to add a taint to the infra node to prevent scheduling user workloads on it: for node in $(oc get nodes | grep infra | cut -d' ' -f 1 ); do oc adm taint nodes $node node-role.kubernetes.io/infra:NoSchedule done Check the Labels We can ask what its labels are by using command: oc get nodes --show-labels -l node-role.kubernetes.io/infra= Output NAME STATUS ROLES AGE VERSION LABELS ocp01-7k4c4-infra-tz8w4 Ready infra,worker 10m v1.19.0+7070803 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ocp01-7k4c4-infra-tz8w4,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos Moving Infrastructure Components Now that you have infra nodes, it’s time to move various infrastructure components onto them. Router The OpenShift router is managed by an Operator called openshift-ingress-operator. Its Pod lives in the openshift-ingress-operator project: Registry The registry uses a similar CRD mechanism to configure how the operator deploys the actual registry pods. That CRD is configs.imageregistry.operator.openshift.io. You will edit the cluster CR object in order to add the nodeSelector oc patch configs.imageregistry.operator.openshift.io/cluster -p '{\"spec\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\": \"NoSchedule\",\"key\": \"node-role.kubernetes.io/infra\",\"operator\": \"Exists\"}]}}' --type=merge Monitoring The Cluster Monitoring operator is responsible for deploying and managing the state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is installed by default during the initial cluster installation. Its operator uses a ConfigMap in the openshift-monitoring project to set various tunables and settings for the behavior of the monitoring stack. The following ConfigMap definition will configure the monitoring solution to be redeployed onto infrastructure nodes. apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \"\" grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" Logging OpenShift’s log aggregation solution is not installed by default. There is a dedicated lab exercise that goes through the configuration and deployment of logging. Machine Config There are times when you need to make changes to the operating systems running on OpenShift Container Platform nodes. This can include changing settings for network time service, adding kernel arguments, or configuring journaling in a specific way. MachineConfig overview The Machine Config Operator (MCO) manages updates to systemd, CRI-O and Kubelet, the kernel, Network Manager and other system features. It also offers a MachineConfig CRD that can write configuration files onto the host. Understanding what MCO does and how it interacts with other components is critical to making advanced, system-level changes to an OpenShift Container Platform cluster. Here are some things you should know about MCO, MachineConfigs, and how they are used: A MachineConfig can make a specific change to a file or service on the operating system of each system representing a pool of OpenShift Container Platform nodes. MCO is only supported for writing to files in /etc and /var directories, although there are symbolic links to some directories that can be writeable by being symbolically linked to one of those areas. The /opt directory is an example. Ignition is the configuration format used in MachineConfigs. See the Ignition Configuration Specification v3.1.0 for details. Checking Machine Config Pool status To see the status of the Machine Config Operator, its sub-components, and the resources it manages, use the following oc commands: Procedure To see the number of MCO-managed nodes available on your cluster for each pool, type: oc get machineconfigpool result NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE master rendered-master-dd… True False False 3 3 3 0 4h42m worker rendered-worker-fde… True False False 3 3 3 0 4h42m In the previous output, there are three master and three worker nodes. All machines are updated and none are currently updating. Because all nodes are Updated and Ready and none are Degraded, you can ell that there are no issues. To see each existing machineconfig, type: oc get machineconfigs result NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 00-worker 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 01-master-container-runtime 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 01-master-kubelet 2c9371fbb673b97a6fe8b1c52… 3.1.0 5h18m ... rendered-master-dde... 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m rendered-worker-fde... 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m Note that the machineconfigs listed as rendered are not meant to be changed or deleted. Expect them to be hidden at some point in the future. Check the status of worker (or change to master) to see the status of that pool of nodes: `` oc describe mcp worker ... Degraded Machine Count: 0 Machine Count: 3 Observed Generation: 2 Ready Machine Count: 3 Unavailable Machine Count: 0 Updated Machine Count: 3 Events: You can view the contents of a particular machineconfig (in this case, 01-master-kubelet). The trimmed output from the following oc describe command shows that this machineconfig contains both configuration files (cloud.conf and kubelet.conf) and a systemd service (Kubernetes Kubelet): oc describe machineconfigs 01-master-kubelet result Name: 01-master-kubelet ... Spec: Config: Ignition: Version: 3.1.0 Storage: Files: Contents: Source: data:, Mode: 420 Overwrite: true Path: /etc/kubernetes/cloud.conf Contents: Source: data:,kind%3A%20KubeletConfiguration%0AapiVersion%3A%20kubelet.config.k8s.io%2Fv1beta1%0Aauthentication%3A%0A%20%20x509%3A%0A%20%20%20%20clientCAFile%3A%20%2Fetc%2Fkubernetes%2Fkubelet-ca.crt%0A%20%20anonymous... Mode: 420 Overwrite: true Path: /etc/kubernetes/kubelet.conf Systemd: Units: Contents: [Unit] Description=Kubernetes Kubelet Wants=rpc-statd.service network-online.target crio.service After=network-online.target crio.service ExecStart=/usr/bin/hyperkube \\ kubelet \\ --config=/etc/kubernetes/kubelet.conf \\ ...\\ Using MachineConfigs to configure nodes Tasks in this section let you create MachineConfig objects to modify files, systemd unit files, and other operating system features running on OpenShift Container Platform nodes. For more ideas on working with MachineConfigs, see content related to changing MTU network settings, adding or updating SSH authorized keys, , replacing DNS nameservers, verifying image signatures, enabling SCTP, and configuring iSCSI initiatornames for OpenShift Container Platform. MachineConfigs OpenShift Container Platform version 4.6 supports Ignition specification version 3.1. All new MachineConfigs you create going forward should be based on Ignition specification version 3.1. If you are upgrading your OpenShift Container Platform cluster, any existing Ignition specification version 2.x MachineConfigs will be translated automatically to specification version 3.1. Configuring chrony time service You can set the time server and related settings used by the chrony time service (chronyd) by modifying the contents of the chrony.conf file and passing those contents to your nodes as a machine config. Procedure Create the contents of the chrony.conf file and encode it as base64. Specify any valid, reachable time source. For example: cat chrony.conf pool 198.18.128.1 iburst driftfile /var/lib/chrony/drift rtcsync makestep 1.0 3 logdir /var/log/chrony EOF Create the MachineConfig object file, replacing the base64 string with the one you just created yourself. This example adds the file to master nodes. You can change it to worker or make an additional MachineConfig object for the worker role: cat ./99_masters-chrony-configuration.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: master name: 99-master-etc-chrony-conf spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 3.1.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat chrony.conf | base64 -w 0) group: name: root mode: 420 overwrite: true path: /etc/chrony.conf user: name: root osImageURL: \"\" EOF cat ./99_workers-chrony-configuration.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: worker name: 99-worker-etc-chrony-conf spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 3.1.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat chrony.conf | base64 -w 0) group: name: root mode: 420 overwrite: true path: /etc/chrony.conf user: name: root osImageURL: \"\" EOF Make a backup copy of the configuration file. Apply the configuration in one of two ways: If the cluster is not up yet, generate manifest files, add this file to the openshift directory, and then continue to create the cluster. Example: generate the cluster manifests: ./openshift-install create manifests --dir install-config/ --log-level debug And then copy the 2 files into install_dir/manifests: cp 99_masters-chrony-configuration.yaml install-config/manifests/. cp 99_workers-chrony-configuration.yaml install-config/manifests/. Then, deploy the cluster: ./openshift-install create cluster --dir install-config/ --log-level debug If the cluster is already running, apply the file as follows: oc apply -f ./99_masters-chrony-configuration.yaml oc apply -f ./99_workers-chrony-configuration.yaml "},"infrastructure-monitoring-alerts.html":{"url":"infrastructure-monitoring-alerts.html","title":"OpenShift Platform Monitoring and Alert","keywords":"","body":"Cluster Monitoring and Alerts Cluster Monitoring and Alerts Prerequisites OpenShift Mornitering and Alert Configuring the monitoring stack Managing alerts Sending notifications to external systems Tesing Alerts Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI OpenShift installer Node subnet with DHCP pool DNS NTP OpenShift Mornitering and Alert OpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. OpenShift Container Platform delivers monitoring best practices out of the box. A set of alerts are included by default that immediately notify cluster administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. The OpenShift Container Platform monitoring stack is based on the Prometheus open source project and its wider ecosystem. The monitoring stack includes the following: Default platform monitoring components. A set of platform monitoring components are installed in the openshift-monitoring project by default during an OpenShift Container Platform installation. This provides monitoring for core OpenShift Container Platform components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters. These components are illustrated in the Installed by default section in the following diagram. Components for monitoring user-defined projects. After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the openshift-user-workload-monitoring project. This provides monitoring for user-defined projects. Default monitoring targets In addition to the components of the stack itself, the default monitoring stack monitors: CoreDNS Elasticsearch (if Logging is installed) etcd Fluentd (if Logging is installed) HAProxy Image registry Kubelets Kubernetes apiserver Kubernetes controller manager Kubernetes scheduler Metering (if Metering is installed) OpenShift apiserver OpenShift controller manager Operator Lifecycle Manager (OLM) Configuring the monitoring stack The OpenShift Container Platform 4 installation program provides only a low number of configuration options before installation. Configuring most OpenShift Container Platform framework components, including the cluster monitoring stack, happens post-installation. You can configure the monitoring stack by creating and updating monitoring config maps. Procedure Check whether the cluster-monitoring-config ConfigMap object exists: oc -n openshift-monitoring get configmap cluster-monitoring-config If the ConfigMap object does not exist: Create and apply the following YAML manifest. In this example the file is called cluster-monitoring-config.yaml: cat Managing alerts In OpenShift Container Platform 4.6, the Alerting UI enables you to manage alerts, silences, and alerting rules. Alerting rules. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed. Alerts. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an OpenShift Container Platform cluster. Silences. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue. Understanding alert filters In the Administrator perspective, the Alerts page in the Alerting UI provides details about alerts relating to default OpenShift Container Platform and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown. You can filter by alert state, severity, and source. By default, only Platform alerts that are Firing are displayed. The following describes each alert filtering option: Alert State filters: Firing. The alert is firing because the alert condition is true and the optional for duration has passed. The alert will continue to fire as long as the condition remains true. Pending. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires. Silenced. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions. Severity filters: Critical. The condition that triggered the alert could have a critical impact. The alert requires immediate attention when fired and is typically paged to an individual or to a critical response team. Warning. The alert provides a warning notification about something that might require attention in order to prevent a problem from occurring. Warnings are typically routed to a ticketing system for non-immediate review. Info. The alert is provided for informational purposes only. None. The alert has no defined severity. You can also create custom severity definitions for alerts relating to user-defined projects. Source filters: Platform. Platform-level alerts relate only to default OpenShift Container Platform projects. These projects provide core OpenShift Container Platform functionality. User. User alerts relate to user-defined projects. These alerts are user-created and are customizable. User-defined workload monitoring can be enabled post-installation to provide observability into your own workloads. Sending notifications to external systems In OpenShift Container Platform 4.6, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure OpenShift Container Platform to send alerts to the following receiver types: PagerDuty Webhook Email Slack Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review. **Set up Test Mail Server and Slack Channel Before we configure the AlertManager to send email and slack alert, we will need to run a webmail with smtp and slack channel for testing Deploy the maildev in default namespace and expose webmail oc project default oc new-app --docker-image=maildev/maildev --name='maildev' oc expose service/maildev --port=80 From this deployment you can reach smtp via maildev.default.svc.cluster.local:25 and webmail from route host using command oc get route Create Slack Channel webhooks token, or use prepared token you have Webhook notification with line-notify-gateway You will need to setup Line Notification service to allow Line Notifications in your chat group. After you setup you will get your token to use in AlertManager webhook to line-notify-gateway container, which is provided as an example for receive AlertManager webhook json and send to Line Notification Service. oc -n default new-app --docker-image=nontster/line-notify-gateway oc -n default get svc Configuring alert receivers Procedure In the Administrator perspective, navigate to Administration → Cluster Settings → Global Configuration → Alertmanager. You can apply Alert Receivers configuration example that includes receivers for smtp to webmail, slack and line-notification-gateway. cat - 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 type: Opaque EOF Configuration content in plain-text global: resolve_timeout: 5m smtp_from: ocp@example.com smtp_smarthost: 'maildev.default.svc.cluster.local:25' smtp_hello: localhost smtp_require_tls: false slack_api_url: >- https://hooks.slack.com/services/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 inhibit_rules: - equal: - namespace - alertname source_match: severity: critical target_match_re: severity: warning|info - equal: - namespace - alertname source_match: severity: warning target_match_re: severity: info receivers: - name: Critical email_configs: - to: admin@example.com - name: Default email_configs: - to: admin@example.com - name: slack slack_configs: - channel: '#alert' - name: slack-warning slack_configs: - channel: '#alert' - name: slack-watchdog slack_configs: - channel: '#alert' - name: Warning email_configs: - to: warning@example.com - name: Watchdog email_configs: - to: watchdog@example.com route: group_by: - namespace group_interval: 5m group_wait: 30s receiver: Default repeat_interval: 1h routes: - receiver: Watchdog match: alertname: Watchdog - receiver: Critical match: severity: critical - receiver: Warning match: severity: warning - receiver: slack match: severity: info - receiver: slack-watchdog match: alertname: Watchdog - receiver: slack-warning match: severity: warning - receiver: line-warning match: severity: warning - receiver: line-watchdog match: alertname: Watchdog Tesing Alerts We will demontrate Alerts from Prometheus and AlertManager by using Platform and User Workload application with metrics from cpu, memory, persistent volume resource. Procedures Deploy a sample httpd pod with persistent volume 1G size mount to /httpd-pvc path oc new-project user1 cat rsh to httpd pod to check PV size and use fallocate command to make PV high utilization (>95%) oc rsh $(oc -n user1 get pod | grep httpd | cut -d' ' -f1) check volume utilization df -h result, /httpd-pvc, Use 1% Filesystem Size Used Avail Use% Mounted on overlay 120G 19G 102G 16% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 47M 3.9G 2% /etc/passwd /dev/sdd 976M 2.6M 958M 1% /httpd-pvc /dev/mapper/coreos-luks-root-nocrypt 120G 19G 102G 16% /etc/hosts tmpfs 3.9G 28K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware use fallocate to create a file with 950M size cd /httpd-pvc/ fallocate -l 950M file.tmp re-check /httpd-pvc again, now it is Use 100% df -h result Filesystem Size Used Avail Use% Mounted on overlay 120G 19G 102G 16% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 47M 3.9G 2% /etc/passwd /dev/sdd 976M 953M 7.4M 100% /httpd-pvc /dev/mapper/coreos-luks-root-nocrypt 120G 19G 102G 16% /etc/hosts tmpfs 3.9G 28K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware For the User Workload Monitoring, we will need to add rolebinding to enabled permission to edit/view PrometheusRule, ServiceMonitor and PodMonitor. Let's add monitoring-edit role to user ldapuser oc policy add-role-to-user monitoring-edit ldapuser -n user1 Create Prometheus Rule to alert when PV utilization is >80% in namespace user1 cat Now we can go back to OpenShift Console Developer View, go to user1 project > Monitoring > Alerts We will see our user defined alert rule and alert that trigger You can click on Alert and see the details When alert is Firing, firing alerts will also route to external notifications defined by AlertManager "},"infrastructure-cluster-logging.html":{"url":"infrastructure-cluster-logging.html","title":"OpenShift Cluster Logging","keywords":"","body":"OpenShift Infrastructure Nodes OpenShift Infrastructure Nodes Prerequisites OpenShift Cluster Logging Deploying OpenShift Logging Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI done the OpenShift Infra Nodes provisioning OpenShift Cluster Logging In this lab you will explore the logging aggregation capabilities of OpenShift. An extremely important function of OpenShift is collecting and aggregating logs from the environments and the application pods it is running. OpenShift ships with an elastic log aggregation solution: EFK. (ElasticSearch, Fluentd and Kibana) The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). The collector, Fluentd, is deployed to each node in the OpenShift cluster. It collects all node and container logs and writes them to Elasticsearch (ES). Kibana is the centralized, web UI where users and administrators can create rich visualizations and dashboards with the aggregated data. Administrators can see and search through all logs. Application owners and developers can allow access to logs that belong to their projects. The EFK stack runs on top of OpenShift. Warning This lab requires that you have completed the infra-nodes lab. The logging stack will be installed on the infra nodes that were created in that lab. Deploying OpenShift Logging OpenShift Container Platform cluster logging is designed to be used with the default configuration, which is tuned for small to medium sized OpenShift Container Platform clusters. The installation instructions that follow include a sample Cluster Logging Custom Resource (CR), which you can use to create a cluster logging instance and configure your cluster logging deployment. If you want to use the default cluster logging install, you can use the sample CR directly. If you want to customize your deployment, make changes to the sample CR as needed. The following describes the configurations you can make when installing your cluster logging instance or modify after installtion. See the Configuring sections for more information on working with each component, including modifications you can make outside of the Cluster Logging Custom Resource. Create the openshift-logging namespace OpenShift Logging will be run from within its own namespace openshift-logging. This namespace does not exist by default, and needs to be created before logging may be installed. The namespace is represented in yaml format as: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-logging: \"true\" openshift.io/cluster-monitoring: \"true\" To create the namespace, run the following command: cat Install the Elasticsearch and Cluster Logging Operators in the cluster In order to install and configure the EFK stack into the cluster, additional operators need to be installed. These can be installed from the Operator Hub from within the cluster via the GUI. When using operators in OpenShift, it is important to understand the basics of some of the underlying principles that make up the Operators. CustomResourceDefinion (CRD) and CustomResource (CR) are two Kubernetes objects that we will briefly describe.CRDs are generic pre-defined structures of data. The operator understands how to apply the data that is defined by the CRD. In terms of programming, CRDs can be thought as being similar to a class. CustomResource (CR) is an actual implementations of the CRD, where the structured data has actual values. These values are what the operator will use when configuring it’s service. Again, in programming terms, CRs would be similar to an instantiated object of the class. The general pattern for using Operators is first, install the Operator, which will create the necessary CRDs. After the CRDs have been created, we can create the CR which will tell the operator how to act, what to install, and/or what to configure. For installing openshift-logging, we will follow this pattern. To begin, log-in to the OpenShift Cluster’s GUI. Then follow the following steps: Install the Elasticsearch Operator: In the OpenShift console, click Operators → OperatorHub. Choose Elasticsearch Operator from the list of available Operators, and click Install. On the Create Operator Subscription page, select Update Channel 4.6, leave all other defaults and then click Subscribe. This makes the Operator available to all users and projects that use this OpenShift Container Platform cluster. Install the Cluster Logging Operator: Note The Cluster Logging operator needs to be installed in the openshift-logging namespace. Please ensure that the openshift-logging namespace was created from the previous steps In the OpenShift console, click Operators → OperatorHub. Choose Cluster Logging from the list of available Operators, and click Install. On the Create Operator Subscription page, Under ensure Installation Mode that A specific namespace on the cluster is selected, and choose openshift-logging. In addition, select Update Channel 4.6 and leave all other defaults and then click Subscribe. Verify the operator installations: Switch to the Operators → Installed Operators page. Make sure the openshift-logging project is selected. In the Status column you should see green checks with either InstallSucceeded or Copied and the text Up to date. Note During installation an operator might display a Failed status. If the operator then installs with an InstallSucceeded message, you can safely ignore the Failed message. Create the Loggging CustomResource (CR) instance Now that we have the operators installed, along with the CRDs, we can now kick off the logging install by creating a Logging CR. This will define how we want to install and configure logging. In the OpenShift Console, switch to the the Administration → Custom Resource Definitions page. On the Custom Resource Definitions page, click ClusterLogging. On the Custom Resource Definition Overview page, select View Instances from the Actions menu Note If you see a 404 error, don’t panic. While the operator installation succeeded, the operator itself has not finished installing and the CustomResourceDefinition may not have been created yet. Wait a few moments and then refresh the page. On the Cluster Loggings page, click Create Cluster Logging. In the YAML editor, replace the code with the following: Note: you need to change storageclass that available in your environment apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" elasticsearch: nodeCount: 3 storage: storageClassName: thin size: 100Gi redundancyPolicy: \"SingleRedundancy\" nodeSelector: node-role.kubernetes.io/infra: \"\" resources: request: memory: 4G visualization: type: \"kibana\" kibana: replicas: 1 nodeSelector: node-role.kubernetes.io/infra: \"\" curation: type: \"curator\" curator: schedule: \"30 3 * * *\" nodeSelector: node-role.kubernetes.io/infra: \"\" collection: logs: type: \"fluentd\" fluentd: {} Then click Create. Verify the Loggging install Now that Logging has been created, let’s verify that things are working. Switch to the Workloads → Pods page. Select the openshift-logging project. You should see pods for cluster logging (the operator itself), Elasticsearch, and Fluentd, and Kibana. Alternatively, you can verify from the command line by using the following command: oc get pods -n openshift-logging You should eventually see something like: NAME READY STATUS RESTARTS AGE cluster-logging-operator-cb795f8dc-xkckc 1/1 Running 0 32m elasticsearch-cdm-b3nqzchd-1-5c6797-67kfz 2/2 Running 0 14m elasticsearch-cdm-b3nqzchd-2-6657f4-wtprv 2/2 Running 0 14m elasticsearch-cdm-b3nqzchd-3-588c65-clg7g 2/2 Running 0 14m fluentd-2c7dg 1/1 Running 0 14m fluentd-9z7kk 1/1 Running 0 14m fluentd-br7r2 1/1 Running 0 14m fluentd-fn2sb 1/1 Running 0 14m fluentd-pb2f8 1/1 Running 0 14m fluentd-zqgqx 1/1 Running 0 14m kibana-7fb4fd4cc9-bvt4p 2/2 Running 0 14m The Fluentd Pods are deployed as part of a DaemonSet, which is a mechanism to ensure that specific Pods run on specific Nodes in the cluster at all times: oc get daemonset -n openshift-logging You will see something like: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd 9 9 9 9 9 kubernetes.io/os=linux 94s You should expect 1 fluentd Pod for every Node in your cluster. Remember that Masters are still Nodes and fluentd will run there, too, to slurp the various logs. You will also see the storage for ElasticSearch has been automatically provisioned. If you query the PersistentVolumeClaim objects in this project you will see the new storage. oc get pvc -n openshift-logging You will see something like: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE elasticsearch-elasticsearch-cdm-ggzilasv-1 Bound pvc-f3239564-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m elasticsearch-elasticsearch-cdm-ggzilasv-2 Bound pvc-f324a252-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m elasticsearch-elasticsearch-cdm-ggzilasv-3 Bound pvc-f326aa7d-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m Note Much like with the Metrics solution, we defined the appropriate NodeSelector in the Logging configuration (CR) to ensure that the Logging components only landed on the infra nodes. That being said, the DaemonSet ensures FluentD runs on all nodes. Otherwise we would not capture all of the container logs. Accessing Kibana As mentioned before, Kibana is the front end and the way that users and admins may access the OpenShift Logging stack. To reach the Kibana user interface, first determine its public access URL by querying the Route that got set up to expose Kibana’s Service: To find and access the Kibana route: In the OpenShift console, click on the Networking → Routes page. Select the openshift-logging project. Click on the Kibana route. In the Location field, click on the URL presented. Click through and accept the SSL certificates Alternatively, this can be obtained from the command line: oc get route -n openshift-logging You will see something like: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD kibana kibana-openshift-logging.{{ ROUTE_SUBDOMAIN }} kibana reencrypt/Redirect None There is a special authentication proxy that is configured as part of the EFK installation that results in Kibana requiring OpenShift credentials for access. Because you’ve already authenticated to the OpenShift Console as a cluster-admin user, you will see an administrative view of what Kibana has to show you (which you authorized by clicking the button). Queries with Kibana Once the Kibana web interface is up, we are now able to do queries. Kibana offers a the user a powerful interface to query all logs that come from the cluster. By default, Kibana will show all logs that have been received within the the last 15 minutes. This time interval may be changed in the upper right hand corner. The log messages are shown in the middle of the page. All log messages that are received are indexed based on the log message content. Each message will have fields associated that are associated to that log message. To see the fields that make up an individual message, click on the arrow on the side of each message located in the center of the page. This will show the message fields that are contained. First, set the default index pattern to .all. On the left hand side towards the top, in the drop down menu select the .all index pattern. To select fields to show for messages, look on left hand side fore the Available Fields label. Below this are fields that can be selected and shown in the middle of the screen. Find the hostname field below the Available Fields and click add. Notice now, in the message pain, each message’s hostname is displayed. More fields may be added. Click the add button for kubernetes.pod_name and also for message. To create a query for logs, the Add a filter + link right below the search box may be used. This will allow us to build queries using the fields of the messages. For example, if we wanted to see all log messages from the openshift-logging namespace, we can do the following: Click on Add a filter +. In the Fields input box, start typing kubernetes.namespace_name. Notice all of the available fields that we can use to build the query Next, select is. In the Value field, type in openshift-logging Click the \"Save\" button Now, in the center of the screen you will see all of the logs from all the pods in the openshift-logging namespace. Of course, you may add more filters to refine the query. One other neat option that Kibana allows you to do is save queries to use for later. To save a query do the following: click on Save at the top of the screen. Type in the name you would like to save it as. In this case, let’s type in openshift-logging Namespace Once this has been saved, it can be used at a later time by hitting the Open button and selecting this query. Please take time to explore the Kibana page and get experience by adding and doing more queries. This will be helpful when using a production cluster, you will be able to get the exact logs that you are looking for in a single place. "},"loki.html":{"url":"loki.html","title":"Loki","keywords":"","body":"Logging with Loki Logging with Loki Install and Config Test with Sample Applications Support for multi-lines error log LogQL Alert Install and Config Install Logging Operator and Loki Operator oc create -f manifests/logging-operator.yaml oc create -f manifests/loki-operator.yaml sleep 60 oc wait --for condition=established --timeout=180s \\ crd/lokistacks.loki.grafana.com \\ crd/clusterloggings.logging.openshift.io oc get csv -n openshift-logging Output NAME DISPLAY VERSION REPLACES PHASE cluster-logging.v5.9.0 Red Hat OpenShift Logging 5.9.0 Succeeded loki-operator.v5.9.0 Loki Operator 5.9.0 Succeeded Create Logging Instance Prepare Object Storage configuration including S3 access Key ID, access Key Secret, Bucket Name, endpoint and Region In case of using ODF Create Bucket Admin Console Navigate to Storage -> Object Storage -> Object Bucket Claims Create ObjectBucketClaim Claim Name: loki StorageClass: openshift-storage.nooba.io BucketClass: nooba-default-bucket-class CLI with YAML oc create -f manifests/loki-odf-bucket.yaml Retrieve configuration into environment variables S3_BUCKET=$(oc get ObjectBucketClaim loki -n openshift-storage -o jsonpath='{.spec.bucketName}') REGION=\"''\" ACCESS_KEY_ID=$(oc get secret loki -n openshift-storage -o jsonpath='{.data.AWS_ACCESS_KEY_ID}'|base64 -d) SECRET_ACCESS_KEY=$(oc get secret loki -n openshift-storage -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}'|base64 -d) ENDPOINT=\"https://s3.openshift-storage.svc:443\" DEFAULT_STORAGE_CLASS=$(oc get sc -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}') If you want to test with existing S3 bucket used by OpenShift Image Registry S3_BUCKET=$(oc get configs.imageregistry.operator.openshift.io/cluster -o jsonpath='{.spec.storage.s3.bucket}' -n openshift-image-registry) REGION=$(oc get configs.imageregistry.operator.openshift.io/cluster -o jsonpath='{.spec.storage.s3.region}' -n openshift-image-registry) ACCESS_KEY_ID=$(oc get secret image-registry-private-configuration -o jsonpath='{.data.credentials}' -n openshift-image-registry|base64 -d|grep aws_access_key_id|awk -F'=' '{print $2}'|sed 's/^[ ]*//') SECRET_ACCESS_KEY=$(oc get secret image-registry-private-configuration -o jsonpath='{.data.credentials}' -n openshift-image-registry|base64 -d|grep aws_secret_access_key|awk -F'=' '{print $2}'|sed 's/^[ ]*//') ENDPOINT=$(echo \"https://s3.$REGION.amazonaws.com\") DEFAULT_STORAGE_CLASS=$(oc get sc -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}') Create Logging and Loki Instances cat manifests/logging-loki-instance.yaml \\ |sed 's/S3_BUCKET/'$S3_BUCKET'/' \\ |sed 's/REGION/'$REGION'/' \\ |sed 's|ACCESS_KEY_ID|'$ACCESS_KEY_ID'|' \\ |sed 's|SECRET_ACCESS_KEY|'$SECRET_ACCESS_KEY'|' \\ |sed 's|ENDPOINT|'$ENDPOINT'|'\\ |sed 's|DEFAULT_STORAGE_CLASS|'$DEFAULT_STORAGE_CLASS'|' \\ |oc apply -f - watch oc get po -n openshift-logging Output secret/logging-loki-s3 created lokistack.loki.grafana.com/logging-loki created clusterlogging.logging.openshift.io/instance created NAME READY STATUS RESTARTS AGE cluster-logging-operator-67d4f44f5c-6pn5l 1/1 Running 0 7m18s collector-8zfb7 1/1 Running 0 75s collector-gpn8c 1/1 Running 0 76s collector-j5gx7 1/1 Running 0 74s collector-ktssl 1/1 Running 0 75s collector-kw9wv 1/1 Running 0 74s collector-q4lsr 1/1 Running 0 73s logging-loki-compactor-0 1/1 Running 0 89s logging-loki-distributor-75d9b9fc8c-gn8d8 1/1 Running 0 89s logging-loki-distributor-75d9b9fc8c-qxttl 1/1 Running 0 89s logging-loki-gateway-946cf94d7-ttddj 2/2 Running 0 88s logging-loki-gateway-946cf94d7-vbx74 2/2 Running 0 88s logging-loki-index-gateway-0 1/1 Running 0 89s logging-loki-index-gateway-1 1/1 Running 0 61s logging-loki-ingester-0 1/1 Running 0 89s logging-loki-ingester-1 0/1 Pending 0 25s logging-loki-querier-5888b4fdf7-cvdst 1/1 Running 0 89s logging-loki-querier-5888b4fdf7-wx577 1/1 Running 0 89s logging-loki-query-frontend-66c7ffd5d4-4vpgm 1/1 Running 0 89s logging-loki-query-frontend-66c7ffd5d4-flbpv 1/1 Running 0 89s logging-loki-ruler-0 1/1 Running 0 88s logging-loki-ruler-1 1/1 Running 0 88s logging-view-plugin-65d59cb67b-hmb2b 1/1 Running 0 91s PVC storage (RWO) used by Loki NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE storage-logging-loki-compactor-0 Bound pvc-a405a61d-2361-44a6-9b9a-a79a5a3a4d48 10Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s storage-logging-loki-index-gateway-0 Bound pvc-688a34a3-562a-4e27-aa09-4d81baca5c5b 50Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s storage-logging-loki-index-gateway-1 Bound pvc-5ede4b00-ad62-4505-bd69-1931f79e52b2 50Gi RWO ocs-external-storagecluster-ceph-rbd 3m19s storage-logging-loki-ingester-0 Bound pvc-065cc5de-e506-45ca-925f-c1f3ba61803e 10Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s storage-logging-loki-ingester-1 Bound pvc-a21adf64-b93c-47fc-a454-20b0e9af220b 10Gi RWO ocs-external-storagecluster-ceph-rbd 2m48s storage-logging-loki-ruler-0 Bound pvc-fa925ae5-34dd-4497-90b5-37d5ec99b925 10Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s storage-logging-loki-ruler-1 Bound pvc-fb19290d-eaf2-4684-8721-3bc8aa4066f9 10Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s wal-logging-loki-ingester-0 Bound pvc-fed10b0d-19a5-49be-ae90-c1b2f5ed6108 150Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s wal-logging-loki-ingester-1 Bound pvc-89c5591b-8fb0-4229-84c3-9d0a28c721c5 150Gi RWO ocs-external-storagecluster-ceph-rbd 2m48s wal-logging-loki-ruler-0 Bound pvc-b431cae4-de01-4202-9ed3-97710e549722 150Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s wal-logging-loki-ruler-1 Bound pvc-50b62352-0e38-4109-98b5-db17df013030 150Gi RWO ocs-external-storagecluster-ceph-rbd 3m51s Enable Console Plugin Operator Navigate to Administrator->Operators->Installed Opertors->Red Hat OpenShift Logging then Enable Console Plugin on the right menu Or using CLI Remark: If you already enable other console plugins then run only the 2nd command oc patch console.operator cluster \\ --type json -p '[{\"op\": \"add\", \"path\": \"/spec/plugins\", \"value\": []}]' oc patch console.operator cluster \\ --type json -p '[{\"op\": \"add\", \"path\": \"/spec/plugins/-\", \"value\": \"logging-view-plugin\"}]' Verify that Logs menu is avaiable under Observe menu Test with Sample Applications Deploy sample applications oc new-project ui oc new-project api oc create -f manifests/frontend.yaml -n ui oc create -f manifests/backend-v1.yaml -n api oc expose deployment/backend-v1 -n api oc set env deployment/frontend-v1 BACKEND_URL=http://backend-v1.api.svc:8080 -n ui oc set env deployment/frontend-v2 BACKEND_URL=http://backend-v1.api.svc:8080 -n ui oc set env deployment/backend-v1 APP_BACKEND=https://httpbin.org/status/201 -n api oc scale deployment/frontend-v1 --replicas=3 -n ui oc scale deployment/frontend-v2 --replicas=3 -n ui oc scale deployment/backend-v1 --replicas=6 -n api Application Flow graph TD; Client--> Route Route-->|Project ui|Frontend; Frontend--> |Project api|Backend; Backend-->|External App|https://httpbin.org/status/201 Test sample app FRONTEND_URL=$(oc get route/frontend -o jsonpath='{.spec.host}' -n ui) curl -v https://$FRONTEND_URL Output Frontend version: v2 => [Backend: http://backend-v1.api.svc:8080, Response: 201, Body: Backend version:v1, Response:201, Host:backend-v1-b585d794d-pcw9k, Status:201, Message: Hello, World Check log Switch to Developer Console and choose project api Select menu Observe -> Logs Filter log by Severity Select Severity Output ![](images/loki-backend-log-info.png) Support for multi-lines error log Configure backend app to return 500 oc set env deployment/backend-v1 APP_BACKEND=https://httpbin.org/status/500 -n api Test app curl -v https://$FRONTEND_URL Output [Backend: http://backend-v1.api.svc:8080, Response: 500, Body: ] Check log Configure log forward with option detectMultilineErrors oc create -f manifests/ClusterLogForwarder-detectMultilineErrors.yaml Test app again and check log in Loki detail LogQL Open Developer Console then select Observe->Log Click Show Query and input following LogQL to query Application Log in namesapce api only worker node name ip-10-0-215-10.us-east-2.compute.internal and contain string Return Code Remark: replace your worker node hostname to ip-10-0-215-10.us-east-2.compute.internal { log_type=\"application\", kubernetes_namespace_name=\"api\" } | json | hostname=~\"ip-10-0-215-10.us-east-2.compute.internal\" |~ \"Return Code: .*\" Output Alert Label namespace api to match condition for LokiStack to monitor for alert oc label ns api openshift.io/cluster-monitoring=true [Optional] Add roles to user to manage alert (CRUD) oc adm policy add-role-to-user alertingrules.loki.grafana.com-v1-admin -n api user1 Create Alert Rule oc apply -f manifests/loki-backend-alert.yaml Configure backend app to return 500 oc set env deployment/backend-v1 APP_BACKEND=https://httpbin.org/status/500 -n api Call frontend Check for alert "},"infrastructure-networking.html":{"url":"infrastructure-networking.html","title":"OpenShift Networking","keywords":"","body":"Infrastructure Networking Infrastructure Networking Prerequisites OpenShift Network Policy Based SDN Switch Your Project Execute the Creation Script Examine the created infrastructure Test Connectivity should work Restricting Access Test Connectivity #2 should fail Allow Access Test Connectivity #3 should work again Test Connectivity #4 while chaning NetworkPolicy Egress IP address assignment for project egress traffic Configuring automatically assigned egress IP addresses for a namespace Network Logging Container Platform Network Segmentation Multi-Cluster Level Cluster Namespace separation level Additional Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 OpenShift Network Policy Based SDN OpenShift has a software defined network (SDN) inside the platform that is based on Open vSwitch. This SDN is used to provide connectivity between application components inside of the OpenShift environment. It comes with default network ranges pre-configured, although you can make changes to these should they conflict with your existing infrastructure, or for whatever other reason you may have. The OpenShift Network Policy SDN plug-in allows projects to truly isolate their network infrastructure inside OpenShift’s software defined network. While you have seen projects isolate resources through OpenShift’s RBAC, the network policy SDN plugin is able to isolate pods in projects using pod and namespace label selectors. The network policy SDN plugin was introduced in OpenShift 3.7, and more information about it and its configuration can be found in the link:https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html[networking documentation^]. Additionally, other vendors are working with the upstream Kubernetes community to implement their own SDN plugins, and several of these are supported by the vendors for use with OpenShift. These plugin implementations make use of appc/CNI, which is outside the scope of this lab. Switch Your Project Before continuing, make sure you are using a project that actually exists. If the last thing you did in the previous lab was delete a project, this will cause errors in the scripts in this lab. oc project default Execute the Creation Script Only users with project or cluster administration privileges can manipulate Project networks. Then, execute a script that we have prepared for you. It will create two Projects and then deploy a DeploymentConfig with a Pod for you: oc new-project netproj-a oc label namespace netproj-a app=iperf3-client oc new-project netproj-b oc label namespace netproj-b app=iperf3-server cat Examine the created infrastructure Two Projects were created for you, netproj-a and netproj-b. Execute the following command to see the created resources: oc get pods -n netproj-a After a while you will see something like the following: NAME READY STATUS RESTARTS AGE iperf3-clients-7c566cfdc-7dtn5 1/1 Running 0 14m Similarly: oc get pods -n netproj-b After a while you will see something like the following: NAME READY STATUS RESTARTS AGE iperf3-server-deployment-79c44f8b-6bkrn 1/1 Running 0 14m We will run commands inside the pod in the netproj-a Project that will connect to TCP port 5201 of the pod in the netproj-b Project. Test Connectivity (should work) Now that you have some projects and pods, let's test the connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project. To test connectivity between the two pods, run: export client=$(oc get pod -n netproj-a | grep iperf3-clients | cut -d' ' -f1) oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 58320 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 997 Mbits/sec 0 491 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 1 691 KBytes [ 5] 2.00-3.00 sec 119 MBytes 1.00 Gbits/sec 1 1.21 MBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 1 1.21 MBytes [ 5] 4.00-5.00 sec 119 MBytes 999 Mbits/sec 1 1.55 MBytes [ 5] 5.00-6.00 sec 119 MBytes 1.00 Gbits/sec 0 1.71 MBytes [ 5] 6.00-7.00 sec 119 MBytes 1.00 Gbits/sec 0 1.71 MBytes [ 5] 7.00-8.00 sec 119 MBytes 997 Mbits/sec 0 1.71 MBytes [ 5] 8.00-9.00 sec 119 MBytes 1.00 Gbits/sec 1 1.71 MBytes [ 5] 9.00-10.00 sec 119 MBytes 1.00 Gbits/sec 1 1.79 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec 6 sender [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec receiver iperf Done. Note that the last line says worked. This means that the pod in the netproj-a Project was able to connect to the pod in the netproj-b Project. This worked because, by default, with the network policy SDN, all pods in all projects can connect to each other. Restricting Access With the Network Policy based SDN, it's possible to restrict access in a project by creating a NetworkPolicy custom resource (CR). For example, the following restricts all access to all pods in a Project where this NetworkPolicy CR is applied. This is the equivalent of a DenyAll default rule on a firewall: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-default spec: podSelector: null ingress: - from: - podSelector: {} Note that the podSelector is empty, which means that this will apply to all pods in this Project. Also note that the ingress list is empty, which means that there are no allowed ingress rules defined by this NetworkPolicy CR. To restrict access to the pod in the netproj-b Project simply apply the above NetworkPolicy CR with: cat Test Connectivity #2 (should fail) Since the \"block all by default\" NetworkPolicy CR has been applied, connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project should now be blocked. Test by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: iperf3: error - unable to connect to server: Connection timed out command terminated with exit code 1 Note the last line that says FAILED!. This means that the pod in the netproj-a Project was unable to connect to the pod in the netproj-b Project (as expected). Allow Access With the Network Policy based SDN, it's possible to allow access to individual or groups of pods in a project by creating multiple NetworkPolicy CRs. The following allows access to port 5000 on TCP for all pods in the project with the label app: iperf3-server. The pod in the netproj-b project has this label. The ingress section specifically allows this access from all projects that have the label app: iperf3-client. # allow access to TCP port 5201 for pods with the label \"run: ose\" specifically # from projects with the label \"name: netproj-a\". cat Note that the podSelector is where the local project's pods are matched using a specific label selector. All NetworkPolicy CRs in a project are combined to create the allowed ingress access for the pods in the project. In this specific case the \"deny all\" policy is combined with the \"allow TCP 5201\" policy. Test Connectivity #3 (should work again) Since the \"allow access from netproj-a on port 5000\" NetworkPolicy has been applied, connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project should be allowed again. Test by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 34702 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 999 Mbits/sec 2 274 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 0 300 KBytes [ 5] 2.00-3.00 sec 119 MBytes 1.00 Gbits/sec 0 362 KBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 0 385 KBytes [ 5] 4.00-5.00 sec 119 MBytes 999 Mbits/sec 0 397 KBytes [ 5] 5.00-6.00 sec 119 MBytes 1.00 Gbits/sec 0 403 KBytes [ 5] 6.00-7.00 sec 119 MBytes 999 Mbits/sec 1 412 KBytes [ 5] 7.00-8.00 sec 119 MBytes 1.00 Gbits/sec 0 448 KBytes [ 5] 8.00-9.00 sec 119 MBytes 1.00 Gbits/sec 1 459 KBytes [ 5] 9.00-10.00 sec 119 MBytes 999 Mbits/sec 0 463 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec 4 sender [ 5] 0.00-10.03 sec 1.16 GBytes 997 Mbits/sec receiver iperf Done. Note the last line that says worked. This means that the pod in the netproj-a Project was able to connect to the pod in the netproj-b Project (as expected). Test Connectivity #4 while chaning NetworkPolicy To show NetworkPolicy is non-disruptive to the application connections while updating the network policies. We will run a test for 30 seconds and try to update network policies in between the test Verify that UDP 5201 is still closed by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -u -t 10 -b 1G' The UDP connection should be failed iperf3: error - unable to read from stream socket: Resource temporarily unavailable Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 command terminated with exit code 1 Run the test again as TCP 5201 for 30 seconds. oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 30 -b 1G' And update network policy while the test is still running eg. also add UDP port 5201 in OpenShift console spec: podSelector: matchLabels: app: iperf3-server ingress: - ports: - protocol: TCP port: 5201 - protocol: UDP port: 5201 from: - namespaceSelector: matchLabels: app: iperf3-client policyTypes: - Ingress TCP test result should be able to complete without connection reset or disconnect. Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 56720 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 1000 Mbits/sec 1 878 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 0 1.04 MBytes [ 5] 2.00-3.00 sec 119 MBytes 999 Mbits/sec 0 1.09 MBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 0 1.54 MBytes [ 5] 4.00-5.00 sec 119 MBytes 1.00 Gbits/sec 0 1.62 MBytes [ 5] 5.00-6.00 sec 119 MBytes 997 Mbits/sec 0 1.97 MBytes [ 5] 6.00-7.00 sec 118 MBytes 989 Mbits/sec 0 1.97 MBytes [ 5] 7.00-8.00 sec 121 MBytes 1.01 Gbits/sec 1 2.07 MBytes [ 5] 8.00-9.00 sec 119 MBytes 999 Mbits/sec 0 2.17 MBytes [ 5] 9.00-10.00 sec 119 MBytes 1.00 Gbits/sec 1 2.40 MBytes [ 5] 10.00-11.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 11.00-12.00 sec 119 MBytes 999 Mbits/sec 1 2.52 MBytes [ 5] 12.00-13.00 sec 119 MBytes 1000 Mbits/sec 1 2.52 MBytes [ 5] 13.00-14.00 sec 119 MBytes 1.00 Gbits/sec 1 2.52 MBytes [ 5] 14.00-15.00 sec 119 MBytes 999 Mbits/sec 0 2.52 MBytes [ 5] 15.00-16.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 16.00-17.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 17.00-18.00 sec 119 MBytes 999 Mbits/sec 0 2.64 MBytes [ 5] 18.00-19.00 sec 119 MBytes 1.00 Gbits/sec 0 2.64 MBytes [ 5] 19.00-20.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 20.00-21.00 sec 119 MBytes 999 Mbits/sec 0 2.77 MBytes [ 5] 21.00-22.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 22.00-23.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 23.00-24.00 sec 119 MBytes 997 Mbits/sec 0 2.77 MBytes [ 5] 24.00-25.01 sec 119 MBytes 988 Mbits/sec 0 2.77 MBytes [ 5] 25.01-26.00 sec 120 MBytes 1.02 Gbits/sec 0 2.77 MBytes [ 5] 26.00-27.00 sec 119 MBytes 999 Mbits/sec 1 2.77 MBytes [ 5] 27.00-28.00 sec 119 MBytes 1.00 Gbits/sec 1 2.77 MBytes [ 5] 28.00-29.01 sec 119 MBytes 985 Mbits/sec 0 2.77 MBytes [ 5] 29.01-30.00 sec 120 MBytes 1.01 Gbits/sec 1 2.77 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-30.00 sec 3.49 GBytes 1000 Mbits/sec 9 sender [ 5] 0.00-30.02 sec 3.49 GBytes 999 Mbits/sec receiver iperf Done. Re-run UDP test again to confirm UDP 5201 has been allowed. oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -u -t 10 -b 1G' iperf3 UDP test is now working Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 39507 connected to 172.30.4.76 port 5201 [ ID] Interval Transfer Bitrate Total Datagrams [ 5] 0.00-1.00 sec 26.0 MBytes 218 Mbits/sec 19520 [ 5] 1.00-2.00 sec 30.2 MBytes 253 Mbits/sec 22663 [ 5] 2.00-3.00 sec 33.4 MBytes 279 Mbits/sec 25072 [ 5] 3.00-4.00 sec 30.0 MBytes 253 Mbits/sec 22486 [ 5] 4.00-5.00 sec 34.6 MBytes 291 Mbits/sec 25986 [ 5] 5.00-6.00 sec 35.3 MBytes 296 Mbits/sec 26455 [ 5] 6.00-7.00 sec 35.7 MBytes 299 Mbits/sec 26766 [ 5] 7.00-8.00 sec 33.6 MBytes 281 Mbits/sec 25183 [ 5] 8.00-9.00 sec 37.6 MBytes 317 Mbits/sec 28229 [ 5] 9.00-10.00 sec 36.7 MBytes 308 Mbits/sec 27537 - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Jitter Lost/Total Datagrams [ 5] 0.00-10.00 sec 333 MBytes 279 Mbits/sec 0.000 ms 0/249897 (0%) sender [ 5] 0.00-10.01 sec 286 MBytes 240 Mbits/sec 0.025 ms 35426/249897 (14%) receiver iperf Done. Egress IP address assignment for project egress traffic As a cluster administrator, you can configure the OpenShift SDN default Container Network Interface (CNI) network provider to assign one or more egress IP addresses to a project. By configuring an egress IP address for a project, all outgoing external connections from the specified project will share the same, fixed source IP address. External resources can recognize traffic from a particular project based on the egress IP address. An egress IP address assigned to a project is different from the egress router, which is used to send traffic to specific destinations. Egress IP addresses are implemented as additional IP addresses on the primary network interface of the node and must be in the same subnet as the node’s primary IP address. High availability of nodes is automatic. If a node that hosts an egress IP address is unreachable and there are nodes that are able to host that egress IP address, then the egress IP address will move to a new node. When the unreachable node comes back online, the egress IP address automatically moves to balance egress IP addresses across nodes. Configuring automatically assigned egress IP addresses for a namespace In OpenShift Container Platform you can enable automatic assignment of an egress IP address for a specific namespace across one or more nodes. Test ping from pod in netproj-a to VM outside OpenShift, the source IP Address is the Node IP Address [root@centos7-tools1 ~]# tcpdump -i ens160 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 08:37:18.756559 IP 198.18.1.18 > registry0.example.com: ICMP echo request, id 3, seq 1, length 64 08:37:18.756627 IP registry0.example.com > 198.18.1.18: ICMP echo reply, id 3, seq 1, length 64 08:37:19.757960 IP 198.18.1.18 > registry0.example.com: ICMP echo request, id 3, seq 2, length 64 08:37:19.758014 IP registry0.example.com > 198.18.1.18: ICMP echo reply, id 3, seq 2, length 64 Update the NetNamespace object with the egress IP address using the following JSON: oc patch netnamespace netproj-a --type=merge -p \\ '{\"egressIPs\": [\"198.18.1.241\"]}' You can set egressIPs to two or more IP addresses on different nodes to provide high availability. If multiple egress IP addresses are set, pods use the first IP in the list for egress, but if the node hosting that IP address fails, pods switch to using the next IP in the list after a short delay. Manually assign the egress IP to the node hosts. Set the egressIPs parameter on the HostSubnet object on the node host. Using the following JSON, include as many IPs as you want to assign to that node host: for node in $(oc get nodes | grep '\\-worker' | cut -d' ' -f1); do oc patch hostsubnet $node --type=merge -p '{\"egressCIDRs\": [\"198.18.1.0/24\"]}' done In the previous example, all egress traffic for project1 will be routed to the node hosting the specified egress IP, and then connected (using NAT) to that IP address. Test ping from the same POD in netproj-a again, now the source IP Address is now egressIP assigned to netproj-a [root@centos7-tools1 ~]# tcpdump -i ens160 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 09:08:33.841050 IP 198.18.1.241 > registry0.example.com: ICMP echo request, id 7, seq 130, length 64 09:08:33.841131 IP registry0.example.com > 198.18.1.241: ICMP echo reply, id 7, seq 130, length 64 09:08:34.861757 IP 198.18.1.241 > registry0.example.com: ICMP echo request, id 7, seq 131, length 64 09:08:34.861815 IP registry0.example.com > 198.18.1.241: ICMP echo reply, id 7, seq 131, length 64 Network Logging The network access logging will be done with envoy sidecar proxy, the details will be in OpenShift Service Mesh section. Container Platform Network Segmentation For the network segregation requirement on Container Platform, we can consider multiple choices of strategy for the network segregation levels supported in OpenShift design architectures. The following are the options that we could do to helps network segmentations are enforced at each level Multi-Cluster Level The highest level of separation is to dedicate one cluster per environment per network zone, e.g. 1 Cluster for Private Zone(Internal Only), 1 Cluster for Extranet Zone and 1 Cluster for DMZ Zone (Internet facing). By seprating clusters per network zone, you can ensure that everything is separated from each environemnt. The deployment can be done in a Pipeline to deploy to each cluster in each stage. Pros: Highest level of separation. Resources, RBAC, NetworkPolicy, Storage, etc. are per cluster. You can also dedicate a cluster per enduser team as well Least scope of security breach will be limited within a cluster Cons: Consume more resources for Control Plane, Storage, Networking, Monitoring, Logging, etc. per cluster High maintenance and operations for Multi-Cluster Management, if you don't prepare Multi-Cluster Management tools. Example ArgoCD, Red Hat Advanced Cluster Management Cluster Namespace separation level In the Container Platform, many technology and components are supporting separation of resources within a cluster. The components that can be used for separation are Container Runtime, CGroups, SELinux: Container Host level for Application resource segration on container host Project/Namespace: We can do the resource seprating by RBAC, Resource Quotas Node Seletor: To select node for sepecific resources Router/Ingress Sharding: We can dedicate Router/Ingress Controller to be the ingestion point in each network zone Pros: Resource efficientcy: by using the same cluster to shared control plane and infrastructure components Less management: less number of clusters to manage and upgrade Focused on Environment: by reducing overall resources, you can focused to have an identical, but smaller, OpenShift Cluster for Testing and Development environment. This can help you to have a test bed for testing integration, operation, BCP or upgrade. Cons: More Router/Ingress Infra nodes End user can't get full cluster-admin control when compare with dedicated cluster per team approach, but OpenShift can somehow compensate by having Operators with multi tenants support, such as OpenShift Service Mesh, the user can create their own service mesh control plane Additional Openshift Blog: https://www.openshift.com/blog/network-policies-controlling-cross-project-communication-on-openshift "},"infrastructure-backup-etcd.html":{"url":"infrastructure-backup-etcd.html","title":"OpenShift state backup with etcd snapshot","keywords":"","body":"Infrastructure Basic Infrastructure Basic Prerequisites Backup etcd Backing up etcd data Backup etcd with cron job Restoring to a previous cluster state Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI OpenShift installer Node subnet with DHCP pool DNS NTP Backup etcd etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects. Back up your cluster’s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. After you have an etcd backup, you can restore to a previous cluster state. Backing up etcd data Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd. Prerequisites You have access to the cluster as a user with the cluster-admin role. You have checked whether the cluster-wide proxy is enabled. Procedure Start a debug session for a master node: oc debug node/ Change your root directory to the host: chroot /host If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. Run the cluster-backup.sh script and pass in the location to save the backup to. /usr/local/bin/cluster-backup.sh /home/core/assets/backup Example script output 1bf371f1b5a483927cd01bb593b0e12cff406eb8d7d0acf4ab079c36a0abd3f7 etcdctl version: 3.3.18 API version: 3.3 found latest kube-apiserver-pod: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-7 found latest kube-controller-manager-pod: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8 found latest kube-scheduler-pod: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6 found latest etcd-pod: /etc/kubernetes/static-pod-resources/etcd-pod-2 Snapshot saved at /home/core/assets/backup/snapshot_2020-03-18_220218.db snapshot db and kube resources are successfully saved to /home/core/assets/backup In this example, two files are created in the /home/core/assets/backup/ directory on the master host: snapshot_.db: This file is the etcd snapshot. statickuberesources.tar.gz: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot. Backup etcd with cron job Procedures: Set your sftp target, username and password sftp_target=198.18.134.150 sftp_user=\"root\" sftp_pass=\"b1ndP^ssword\" Create a backup script cat etcd-backup-on-debug-pod.sh #!/bin/sh echo \"chroot to /host\" chroot /host /bin/sh Create etcd backup namespace cat Create config-map etcd-backup-on-debug-pod.sh from cluster-backup.sh oc create configmap etcd-backup-on-debug-pod --from-file=etcd-backup-on-debug-pod.sh Pickup the first master node name to run etcd backup master_node=$(oc get node -l node-role.kubernetes.io/master= -o=jsonpath='{.items[0].metadata.name}') Create a cronjob to run debug pod and backup script cat - quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:091cd1158444af8382312b71150d26e6550c5d52023b993fec6afd2253d2e425 serviceAccount: default volumes: - name: host hostPath: path: / type: Directory - name: etcd-backup-script configMap: name: etcd-backup-on-debug-pod defaultMode: 0744 EOF Restoring to a previous cluster state To restore the cluster to a previous state, you must have previously backed up etcd data by creating a snapshot. You will use this snapshot to restore the cluster state. You can use a saved etcd backup to restore back to a previous cluster state. You use the etcd backup to restore a single control plane host. Then the etcd cluster Operator handles scaling to the remaining master hosts. Prerequisites Access to the cluster as a user with the cluster-admin role. SSH access to master hosts. A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: snapshot.db and static_kuberesources.tar.gz. Procedure Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on. Establish SSH connectivity to each of the control plane nodes, including the recovery host. The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal. Warning: If you do not complete this step, you will not be able to access the master hosts to complete the restore procedure, and you will be unable to recover your cluster from this state. Copy the etcd backup directory to the recovery control plane host. This procedure assumes that you copied the backup directory containing the etcd snapshot and the resources for the static pods to the /home/core/ directory of your recovery control plane host. Stop the static pods on all other control plane nodes. Note: It is not required to manually stop the pods on the recovery host. The recovery script will stop the pods on the recovery host. Access a control plane host that is not the recovery host. Move the existing etcd pod file out of the kubelet manifest directory:sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp Verify that the etcd pods are stopped. sudo crictl ps | grep etcd The output of this command should be empty. If it is not empty, wait a few minutes and check again. Move the existing Kubernetes API server pod file out of the kubelet manifest directory: sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp Verify that the Kubernetes API server pods are stopped.sudo crictl ps | grep kube-apiserver The output of this command should be empty. If it is not empty, wait a few minutes and check again. Move the etcd data directory to a different location:sudo mv /var/lib/etcd/ /tmp Repeat this step on each of the other master hosts that is not the recovery host. Access the recovery control plane host. If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. You can check whether the proxy is enabled by reviewing the output of oc get proxy cluster -o yaml. The proxy is enabled if the httpProxy, httpsProxy, and noProxy fields have values set. Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory: sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup Example script output ...stopping kube-scheduler-pod.yaml ...stopping kube-controller-manager-pod.yaml ...stopping etcd-pod.yaml ...stopping kube-apiserver-pod.yaml Waiting for container etcd to stop .complete Waiting for container etcdctl to stop .............................complete Waiting for container etcd-metrics to stop complete Waiting for container kube-controller-manager to stop complete Waiting for container kube-apiserver to stop ..........................................................................................complete Waiting for container kube-scheduler to stop complete Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup starting restore-etcd static pod starting kube-apiserver-pod.yaml static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml starting kube-controller-manager-pod.yaml static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml starting kube-scheduler-pod.yaml static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml Restart the kubelet service on all master hosts. From the recovery host, run the following command:sudo systemctl restart kubelet.service Repeat this step on all other master hosts. Verify that the single member control plane has started successfully. From the recovery host, verify that the etcd container is running. sudo crictl ps | grep etcd Example output 3ad41b7908e32 36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009 About a minute ago Running etcd 0 7c05f8af362f0 From the recovery host, verify that the etcd pod is running. oc get pods -n openshift-etcd | grep etcd Example output NAME READY STATUS RESTARTS AGE etcd-ip-10-0-143-125.ec2.internal 1/1 Running 1 2m47s If the status is Pending, or the output lists more than one running etcd pod, wait a few minutes and check again. Force etcd redeployment. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc patch etcd cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge The forceRedeploymentReason value must be unique, which is why a timestamp is appended. When the etcd cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up. Verify all nodes are updated to the latest revision. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition for etcd to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 If the output shows a message such as 2 nodes are at revision 3; 1 nodes are at revision 4, this means that the update is still in progress. Wait a few minutes and try again. After etcd is redeployed, force new rollouts for the control plane. The Kubernetes API server will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer. In a terminal that has access to the cluster as a cluster-admin user, run the following commands. Update the kubeapiserver: oc patch kubeapiserver cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Update the kubecontrollermanager: oc patch kubecontrollermanager cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Update the kubescheduler: oc patch kubescheduler cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Verify that all master hosts have started and joined the cluster. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc get pods -n openshift-etcd | grep etcd Example output etcd-ip-10-0-143-125.ec2.internal 2/2 Running 0 9h etcd-ip-10-0-154-194.ec2.internal 2/2 Running 0 9h etcd-ip-10-0-173-171.ec2.internal 2/2 Running 0 9h "},"infrastructure-taint-and-toleration.html":{"url":"infrastructure-taint-and-toleration.html","title":"Pod Taint and Toleration","keywords":"","body":"Taint and Toleration Taint and Toleration Pod Eviction Node UnreachablePod Eviction Pod eviction from node behavior can be configured by adding toleration to pod. This default value is 5 minutes in case of node unreachable or not-ready Node Unreachable Configure pod to evict from node in case node is unreachable. Check pod default toleration Check toleration oc describe pod $(oc get pods | grep Running | tail -n 1 | awk '{print $1}') | \\ grep -A2 -i toleration Output example. node.kubernetes.io/not-ready is 300s (5 minutes) node.kubernetes.io/unreachable is 300s (5 minutes) Tolerations: node.kubernetes.io/memory-pressure:NoSchedule op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Set unreachable to 1 minute by adding toleration for node unreachable to deployment or deployment config. This toleration mean that Pods remain bound to nodes for 60s after unreachable of these problems is detected. Remark: Default value of 5 minutes is reasonable in the assumption for preventing false posivites Example of deployment with unreachable toleration set to 1 minute. template: metadata: labels: app: backend-native deploymentconfig: backend-native spec: containers: - image: image-registry.openshift-image-registry.svc:5000/demo/backend-native@sha256:5b76fdf7113c0db6d7fddea54997dd648a55b2a04383effb82f55cdbb0419dd5 ... ... tolerations: - key: node.kubernetes.io/unreachable operator: Exists effect: NoExecute tolerationSeconds: 60 Check pod toleration node.kubernetes.io/unreachable is chaged to 60s. Pod will be recreate on another node if node is unreachable for 60s Tolerations: node.kubernetes.io/memory-pressure:NoSchedule op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 60s "},"assign-pod-to-node.html":{"url":"assign-pod-to-node.html","title":"Assign pod to node","keywords":"","body":"Assign Pod to Node Assign Pod to Node Node Selector Taints and Tolerations Remove node label and toleration Node Selector You can configure pod to run on particular node by using node label along with nodeSelector Deploy backend application and scale to 5 pods oc new-project demo oc apply -f manifests/backend-v1.yaml -n demo oc scale deployment backend-v1 --replicas=5 -n demo Check that 5 pods is running on which worker nodes watch oc get pods -o 'custom-columns=Name:.metadata.name,PHASE:.status.phase,NODE:.spec.nodeName' -n demo Sample output Name PHASE NODE backend-v1-b564ff66-6spph Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-b564ff66-dh77c Running ip-10-0-XXX-101.us-east-2.compute.internal backend-v1-b564ff66-dtkjn Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-b564ff66-n7kr4 Running ip-10-0-XXX-121.us-east-2.compute.internal backend-v1-b564ff66-thhqv Running ip-10-0-XXX-121.us-east-2.compute.internal Let's say we want to run backend app is belong to project app-x and project app-x need to run on node 179 only then we'll label node 179 with label app-x oc label node ip-10-0-XXX-179.us-east-2.compute.internal app-x='' Add nodeSelector to backend-v1 deployment Remark: You can use another shell/terminal to check status of backend-v1's pods with previously watch command sed is use for set replicas to 5. cat manifests/backend-v1-with-node-selector.yaml | sed s/replicas:\\ 1/replicas:\\ 5/g | oc apply -n demo -f - Sample output from watch command Name PHASE NODE backend-v1-786cd59759-bcmpx Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-dkgxp Pending ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-lpnwq Pending ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-vlqp2 Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-x4nlb Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-b564ff66-dh77c Running ip-10-0-XXX-101.us-east-2.compute.internal backend-v1-b564ff66-thhqv Running ip-10-0-XXX-121.us-east-2.compute.internal When finished all pods will be run on node 179 Name PHASE NODE backend-v1-786cd59759-bcmpx Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-dkgxp Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-lpnwq Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-vlqp2 Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-786cd59759-x4nlb Running ip-10-0-XXX-179.us-east-2.compute.internal Check backend-v1 deployment with nodeAffinity under template spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: app-x operator: Exists Taints and Tolerations Taint is proporty of nodes that allow only pod with toleration can be scheduled to run on that particular node. Let's say we want to allow only project app-x to run on node 179 then we will taint node 179 oc adm taint node ip-10-0-248-179.us-east-2.compute.internal app-x='':NoSchedule Taint is not effect pods those already scheduled then we will force backend-v1 to create all pods again oc delete pods --all -n demo Pods cannot schedule because there is no toleration in deployment Name PHASE NODE backend-v1-786cd59759-79jmh Pending backend-v1-786cd59759-gnwg7 Pending backend-v1-786cd59759-lc4jr Pending backend-v1-786cd59759-lgk8r Pending backend-v1-786cd59759-vcd22 Pending Check Events in OpenShift Console Add toleration to backend-v1 deployment cat manifests/backend-v1-with-node-selector-and-toleration.yaml|sed s/replicas:\\ 1/replicas:\\ 5/g | oc apply -n demo -f - When finished all pods will be run on node 179 Name PHASE NODE backend-v1-647db46bfc-868w6 Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-647db46bfc-krzvs Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-647db46bfc-nssqz Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-647db46bfc-qhvws Running ip-10-0-XXX-179.us-east-2.compute.internal backend-v1-647db46bfc-vnl5l Running ip-10-0-XXX-179.us-east-2.compute.internal Check backend-v1 deployment with nodeAffinity under template tolerations: - effect: NoSchedule key: app-x operator: Exists Remove node label and toleration Following commands show how to remove node label and toleration oc label node ip-10-0-XXX-179.us-east-2.compute.internal app-x- oc adm taint nodes ip-10-0-XXX-179.us-east-2.compute.internal app-x='':NoSchedule- "},"custom-roles.html":{"url":"custom-roles.html","title":"Custom Roles and Service Account","keywords":"","body":"Custom Roles and Service Account Custom Roles and Service Account Service Account Create Service Account Custom Roles Local Role Create cluster role Test Service Account CLI REST API Use Service Account with Deployment Service Account Create custom roles for service account to view,list and watch - configmaps - pods - services - namespaces - endpoints - secrets - *nodes* Remark: nodes need cluster role Create Service Account Create Service Account oc create sa sa-discovery -n demo Output serviceaccount/sa-discovery created Custom Roles Local Role Create role for service account. oc create role app-discovery \\ --verb=get,list,watch \\ --resource=configmaps,pods,services,namespaces,endpoints \\ -n demo oc describe role app-discovery -n demo or create from app-discovery yaml oc create -f manifests/app-discovery-role.yaml -n demo oc describe role app-discovery -n demo oc describe role list-secret -n demo Output role.rbac.authorization.k8s.io/app-discovery created Name: app-discovery Labels: Annotations: PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get list watch] endpoints [] [] [get list watch] namespaces [] [] [get list watch] pods [] [] [get list watch] secrets [] [] [get list watch] services [] [] [get list watch] Assign role to service account oc adm policy add-role-to-user app-discovery \\ system:serviceaccount:demo:sa-discovery --role-namespace=demo -n demo Output role.rbac.authorization.k8s.io/app-discovery added: \"system:serviceaccount:demo:sa-discovery\" Create cluster role Create cluster role to view node oc create clusterrole view-nodes \\ --verb=get,list,watch --resource=nodes or create from view-nodes yaml oc create -f manifests/clusterrole-view-nodes.yaml Output clusterrole.rbac.authorization.k8s.io/view-nodes created Assign role to service account oc adm policy add-cluster-role-to-user \\ view-nodes system:serviceaccount:demo:sa-discovery Output clusterrole.rbac.authorization.k8s.io/view-nodes added: \"system:serviceaccount:demo:sa-discovery\" Test Service Account CLI Test service account sa-discovery with CLI tool Get service account sa-discovery token TOKEN=$(oc sa get-token sa-discovery -n demo) Login with service account token oc login --token=$TOKEN oc whoami Output Using project \"demo\". system:serviceaccount:demo:app-discovery Test list resources clear printf \"List configmaps\\n\" oc get configmaps -n demo echo \"Press any keys to continue...\";read clear printf \"List secrets\\n\" oc get secrets -n demo echo \"Press any keys to continue...\";read clear printf \"List pods\\n\" oc get pods -n demo echo \"Press any keys to continue...\";read clear printf \"List services\\n\" oc get svc -n demo echo \"Press any keys to continue...\";read clear printf \"List nodes\\n\" oc get nodes echo \"Press any keys to continue...\";read clear Test get secret oc describe secrets/$(oc get secrets --no-headers|head -n 1|awk '{print $1}') You will get following error because sa-discovery has only list action Error from server (Forbidden): secrets \"builder-dockercfg-cjfz6\" is forbidden: User \"system:serviceaccount:demo:sa-discovery\" cannot get resource \"secrets\" in API group \"\" in the namespace \"demo\" REST API List pods API=$(oc whoami --show-server) NAMESPACE=demo curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/namespaces/$NAMESPACE/pods Output \"items\": [ { \"metadata\": { \"name\": \"backend-797f8bfdcc-xrzkw\", \"generateName\": \"backend-797f8bfdcc-\", \"namespace\": \"demo\", \"selfLink\": \"/api/v1/namespaces/demo/pods/backend-797f8bfdcc-xrzkw\", \"uid\": \"e6845671-6e46-4b20-aa7b-ced5839341e2\", \"resourceVersion\": \"56509\", \"creationTimestamp\": \"2021-06-10T09:10:10Z\", \"labels\": { \"app\": \"backend\", \"pod-template-hash\": \"797f8bfdcc\", \"version\": \"v1\" }, Get sepcified pod curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/namespaces/$NAMESPACE/pods/ Get node curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/nodes/$(oc get nodes --no-headers|head -n 1|awk '{print $1}') Use Service Account with Deployment Backend deployment (backend-discovery-sa.yaml) with custom service account spec: replicas: 1 selector: matchLabels: app: backend version: v1 template: metadata: creationTimestamp: null labels: app: backend version: v1 annotations: sidecar.istio.io/inject: \"false\" spec: serviceAccountName: svip-ignite-discovery automountServiceAccountToken: false containers: - name: backend Check service account used by pod oc get pod/ -o jsonpath='{.spec.serviceAccountName}' "},"custom-alert.html":{"url":"custom-alert.html","title":"Custom Alert","keywords":"","body":"Custom Monitoring Custom Monitoring Monitor Pod Creation Monitor Project Quotas Cluster Level Test Alert CrashLoopBackOff and ImagePullBackOff OOMKilled Alert with LINE LINE BOT Configuration Monitor Pod Creation Create custom alerts to monitor for pod creating status with PrometheusRule pod-stuck-alerts.yaml This PrometheusRule will sending alerts if pod status PodStuckContainerCreating for 2 minutes PodStuckImagePullBackOff for 30 seconds PodStuckErrImagePull for 2 minuts PodStuckCrashLoopBackOff for 2 minutes PodStuckCreateContainerError for 2 minutes OOMKilled for 3 minutes Monitor Project Quotas Create custom alerts to monitor for project quotas with PrometheusRule quota-alert.yaml This PrometheusRule will sending alerts if Project used CPU/memory request/limits more than 90% will alert with critical severity Project used CPU/memory request/limits more than 80% and less than 90% with warning severity Cluster Level Create PrometheusRule in namespace openshift-monitoring oc create -f manifests/pod-stuck-alerts.yaml oc create -f manifests/quota-alert.yaml Check alerting rules View alerting rules cpuRequestQuotaCritical Test Alert CrashLoopBackOff and ImagePullBackOff Create following pod-stuck deployments. These deployments intentionally put pods into error state. oc create -f manifests/pod-stuck.yaml -n demo Check for result oc get pods -n demo Sample result NAME READY STATUS RESTARTS AGE backend-v5-65569d96b9-ht5zl 0/1 CrashLoopBackOff 1 (8s ago) 13s backend-v6-794c9fc748-hgpl2 0/1 ImagePullBackOff 0 13s Check for alerts on Notifications menu Administrator -> Overview Check for details of an alert OOMKilled Create following memory-hungry deployments. These deployments intentionally put pods into error state. oc create -f manifests/memory-hungry.yaml -n demo Check for result oc get pods -n demo Get route to access memory-hungry app HUNGER=https://$(oc get route memory-hungry -n demo -o jsonpath='{.spec.host}') Run following command curl -s $HUNGER/eat/6 Check application log 2022-10-25 09:03:05,745 INFO [io.quarkus] (main) leak 1.0.0-SNAPSHOT native (powered by Quarkus 2.13.1.Final) started in 0.202s. Listening on: http://0.0.0.0:8080 2022-10-25 09:03:05,745 INFO [io.quarkus] (main) Profile prod activated. 2022-10-25 09:03:05,745 INFO [io.quarkus] (main) Installed features: [cdi, resteasy, smallrye-context-propagation, smallrye-health, smallrye-metrics, smallrye-openapi, vertx] 2022-10-25 09:55:54,697 INFO [com.exa.HungryResource] (executor-thread-0) Prepare meal for dish no. 1 2022-10-25 09:55:54,845 INFO [com.exa.HungryResource] (executor-thread-0) Allocated 10485760 bytes 2022-10-25 09:55:54,845 INFO [com.exa.HungryResource] (executor-thread-0) Prepare meal for dish no. 2 2022-10-25 09:55:55,141 INFO [com.exa.HungryResource] (executor-thread-0) Allocated 10485760 bytes 2022-10-25 09:55:55,142 INFO [com.exa.HungryResource] (executor-thread-0) Prepare meal for dish no. 3 2022-10-25 09:55:55,346 INFO [com.exa.HungryResource] (executor-thread-0) Allocated 10485760 bytes 2022-10-25 09:55:55,346 INFO [com.exa.HungryResource] (executor-thread-0) Prepare meal for dish no. 4 2022-10-25 09:55:55,641 INFO [com.exa.HungryResource] (executor-thread-0) Allocated 10485760 bytes 2022-10-25 09:55:55,641 INFO [com.exa.HungryResource] (executor-thread-0) Prepare meal for dish no. 5 Check for alert in console Check pod with oc get pod -o yaml containerStatuses: - containerID: cri-o://c3cb6a9b2a967f35bda906e5e20b1d22c1c4f8f1dc15d2e797618e1f8438f7fb image: quay.io/voravitl/leak:native imageID: quay.io/voravitl/leak@sha256:f74d7653c2ebf71144f16019143b9849fff3f3491e4ec199fab6db51dab02b8f lastState: terminated: containerID: cri-o://08f70b1f69bc00906edaa17241d300abf2df4b356c13b7dd1896eae5b0bb6760 exitCode: 137 finishedAt: \"2022-11-03T07:03:06Z\" reason: OOMKilled startedAt: \"2022-11-03T06:56:55Z\" Alert with LINE Login to LINE Developer and create Channel Deploy LINE BOT app oc new-project line-alert oc create -f manifests/line-bot.yaml -n line-alert Verify deployment Update line-bot deployment environment variable API_LINE_TOKEN with your channel access token Channel access token Update Environment variable Developer Console CLI oc set env -n line-alert deployment/line-bot API_LINE_TOKEN- oc set env -n line-alert deployment/line-bot API_LINE_TOKEN=$API_LINE_TOKEN Update your Channel's Webhook with line-bot route Webhook URL LINE_WEBHOOK=https://$(oc get route line-bot -n line-alert -o jsonpath='{.spec.host}')/webhook echo $LINE_WEBHOOK Configure Line BOT Webhook to your channel Send some message to your LINE BOT and check line-bot pod's log ```bash 2022-09-06 06:47:14,766 INFO [com.vor.LineBotResource] (executor-thread-0) Message Type: text 2022-09-06 06:47:14,766 INFO [com.vor.LineBotResource] (executor-thread-0) Message: Hi 2022-09-06 06:47:14,766 INFO [com.vor.LineBotResource] (executor-thread-0) userId: U*************, userType: user 2022-09-06 06:47:14,767 INFO [com.vor.LineBotResource] (executor-thread-0) replyToken: 0a5b7********* ``` Register your LINE account to receiving alert by send message register to LINE BOT line-bot pod's log 2022-09-06 07:14:04,915 INFO [com.vor.LineBotResource] (executor-thread-0) destination: Uef7db62e42ed955b58d9810f64955806 2022-09-06 07:14:04,916 INFO [com.vor.LineBotResource] (executor-thread-0) Message Type: text 2022-09-06 07:14:04,916 INFO [com.vor.LineBotResource] (executor-thread-0) Message: Register 2022-09-06 07:14:07,142 INFO [com.vor.LineBotResource] (executor-thread-0) Register user: U************* 2022-09-06 07:14:07,143 INFO [com.vor.LineBotResource] (executor-thread-0) userId: U*************, userType: user 2022-09-06 07:14:07,143 INFO [com.vor.LineBotResource] (executor-thread-0) replyToken: 741b1********* Configure Alert Manger Webhook Administrator Console, Administration->Cluster Settings->Configuration and select Alertmanager Create Receiver Check that PrometheusRule pod-stuck contains label receiver with value equals to line for each alert - alert: PodStuckErrImagePull annotations: message: Pod in project project stuck at ErrImagePull description: Pod in project project stuck at ErrImagePull expr: kube_pod_container_status_waiting_reason{reason=\"ErrImagePull\"} == 1 for: 30s labels: severity: critical receiver: 'line' Create deployment with CrashLoopBackoff oc create -f manifests/pod-stuck -n demo watch oc get pods -n demo Check LINE message LINE BOT Configuration Use following enviroment variables to configure LINE BOT Variable Description APP_LINE_TOKEN LINE Channel Token QUARKUS_LOG_CATEGORYCOM_VORAVIZLEVEL Set to DEBUG if you want to log whole JSON message from AlertManager APP_ALERT_ANNOTATIONS List of attributes from Annotations to including in message Alert Rule annotaions - alert: PodStuckCrashLoopBackOff annotations: summary: CrashLoopBackOff in project {{ $labels.namespace }} message: Pod {{ $labels.pod }} in project {{ $labels.namespace }} project stuck at CrashLoopBackOff description: Pod {{ $labels.pod }} in project {{ $labels.namespace }} project stuck at CrashLoopBackOff "},"compliance-operator.html":{"url":"compliance-operator.html","title":"Compliance","keywords":"","body":"OpenShift Compliance with Compliance Operator OpenShift Compliance with Compliance Operator Prerequisites Compliance Operator CIS Profile Openscap Report CIS PCI-DSS Reports Prerequisites OpenShift 4.6+ Cluster-admin user access Compliance Operator Install Compliance Operator from OperatorHub [Optional] Verify Compliance Operator Check compliance profile oc get profiles.compliance -n openshift-compliance Output example NAME AGE ocp4-cis 8m8s ocp4-cis-node 8m8s ocp4-e8 8m8s ocp4-moderate 8m7s ocp4-moderate-node 8m7s ocp4-nerc-cip 8m7s ocp4-nerc-cip-node 8m7s ocp4-pci-dss 8m7s ocp4-pci-dss-node 8m7s rhcos4-e8 8m2s rhcos4-moderate 8m2s rhcos4-nerc-cip 8m1s Check detail of profile oc get -o yaml profiles.compliance ocp4-cis -n openshift-compliance Output example ... rules: - ocp4-accounts-restrict-service-account-tokens - ocp4-accounts-unique-service-account - ocp4-api-server-admission-control-plugin-alwaysadmit - ocp4-api-server-admission-control-plugin-alwayspullimages - ocp4-api-server-admission-control-plugin-namespacelifecycle - ocp4-api-server-admission-control-plugin-noderestriction ... Check details of rule oc get -o yaml rules.compliance ocp4-accounts-unique-service-account -n openshift-compliance Check for default ScanSetting List all ScanSetting oc get scansettings -n openshift-compliance Result NAME AGE default 35m default-auto-apply 35m Check for default ScanSetting oc describe scansettings default -n openshift-compliance Output example, scheduled at 1AM everyday and apply to both master and worker node and use block storage (RWO) for stored result Raw Result Storage: Pv Access Modes: ReadWriteOnce Rotation: 3 Size: 1Gi Roles: worker master Scan Tolerations: Effect: NoSchedule Key: node-role.kubernetes.io/master Operator: Exists Schedule: 0 1 * * * Events: CIS Profile To start scan, create ScanSettingBinding. Scan will be started immediately after save Use Admin Console to create ScanSettingBinding, default is rhcos4-moderate and use default ScanSetting Add opc4-cis and ocp4-cis-node profiles for CIS compliance to ScanSettingBinding or add ocp4-pci-dss and ocp4-pci-dss-node for PCI-DSS compliance apiVersion: compliance.openshift.io/v1alpha1 profiles: - apiGroup: compliance.openshift.io/v1alpha1 name: ocp4-cis-node kind: Profile - apiGroup: compliance.openshift.io/v1alpha1 name: ocp4-cis kind: Profile settingsRef: apiGroup: compliance.openshift.io/v1alpha1 name: default kind: ScanSetting kind: ScanSettingBinding metadata: name: cis-profile namespace: openshift-compliance or use CLI oc apply -f manifests/cis-profile.yaml oc apply -f manifests/pci-dss-profile.yaml oc describe scansettingbinding/cis-profile -n openshift-compliance|grep -A14 \"Status:\" oc describe scansettingbinding/pci-dss-profile -n openshift-compliance|grep -A14 \"Status:\" Check for status Status: Conditions: Last Transition Time: 2022-04-12T08:00:27Z Message: The scan setting binding was successfully processed Reason: Processed Status: True Type: Ready Output Ref: API Group: compliance.openshift.io Kind: ComplianceSuite Name: cis-profile Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuiteCreated 10s scansettingbindingctrl ComplianceSuite openshift-compliance/cis-profile created Check ComplianceScan tab or use CLI watch -d oc get compliancescan -n openshift-compliance Output NAME PHASE RESULT ocp4-cis RUNNING NOT-AVAILABLE ocp4-cis-node-master RUNNING NOT-AVAILABLE ocp4-cis-node-worker RUNNING NOT-AVAILABLE ocp4-pci-dss RUNNING NOT-AVAILABLE ocp4-pci-dss-node-master LAUNCHING NOT-AVAILABLE ocp4-pci-dss-node-worker LAUNCHING NOT-AVAILABLE When compliance scan is completed NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE NON-COMPLIANT ocp4-pci-dss DONE NON-COMPLIANT ocp4-pci-dss-node-master DONE NON-COMPLIANT ocp4-pci-dss-node-worker DONE NON-COMPLIANT Check result Count for FAIL oc get compliancecheckresult -n openshift-compliance | grep FAIL NUM_OF_CIS_FAILED_BEFORE_REMIDIATE=$(oc get compliancecheckresult -n openshift-compliance | grep FAIL|grep cis|wc -l) NUM_OF_PCI_DSS_FAILED_BEFORE_REMIDIATE=$(oc get compliancecheckresult -n openshift-compliance | grep FAIL|grep pci-dss|wc -l) Output ocp4-pci-dss-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-pci-dss-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-pci-dss-node-worker-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-pci-dss-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-pci-dss-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium Check for result description for ocp4-cis-api-server-encryption-provider-config oc describe compliancecheckresult/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance Output ... Description: Configure the Encryption Provider etcd is a highly available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_config Instructions: Run the following command: $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type' The output should return aescdc as the encryption type. ... Severity: medium Status: FAILED Events: Fix failed policies with ComplianceRemediation List ComplianceRemediation oc get ComplianceRemediation -n openshift-compliance Output NAME STATE ocp4-cis-api-server-encryption-provider-cipher NotApplied ocp4-cis-api-server-encryption-provider-config NotApplied ocp4-cis-node-master-kubelet-configure-event-creation NotApplied ocp4-cis-node-master-kubelet-configure-tls-cipher-suites NotApplied ocp4-cis-node-master-kubelet-enable-iptables-util-chains NotApplied ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults NotApplied ocp4-cis-node-master-kubelet-enable-protect-kernel-sysctl NotApplied ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available NotApplied ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available-1 NotApplied ... Fix failed ocp4-cis-api-server-encryption-provider-config and ocp4-cis-api-server-encryption-provider-cipher policy with ComplianceRemidiation oc patch -n openshift-compliance complianceremediation \\ ocp4-cis-api-server-encryption-provider-config -p '{\"spec\":{\"apply\":true}}' --type='merge' oc patch -n openshift-compliance complianceremediation \\ ocp4-cis-api-server-encryption-provider-cipher -p '{\"spec\":{\"apply\":true}}' --type='merge' Check result oc get ComplianceRemediation/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance Output NAME STATE ocp4-cis-api-server-encryption-provider-config Applied Re-run scan Annotate ComplianceScans to rescan or use script for scan in $(oc get compliancescans -n openshift-compliance -o custom-columns=NAME:.metadata.name --no-headers) do oc annotate compliancescans $scan compliance.openshift.io/rescan= -n openshift-compliance done watch -d oc get compliancescans -n openshift-compliance Result NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE NON-COMPLIANT ocp4-pci-dss DONE NON-COMPLIANT ocp4-pci-dss-node-master AGGREGATING NOT-AVAILABLE ocp4-pci-dss-node-worker AGGREGATING NOT-AVAILABLE Recheck policy ocp4-cis-api-server-encryption-provider-config oc describe compliancecheckresult/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance | grep -A3 Severity Output Severity: medium Status: PASS Events: Change ScanSettingBinding cis-and-moderate-profile to use ScanSetting default-auto-apply oc patch -n openshift-compliance ScanSettingBinding cis-profile -p '{\"settingsRef\":{\"name\":\"default-auto-apply\"}}' --type='merge' oc patch -n openshift-compliance ScanSettingBinding pci-dss-profile -p '{\"settingsRef\":{\"name\":\"default-auto-apply\"}}' --type='merge' Output scansettingbinding.compliance.openshift.io/cis-profile patched scansettingbinding.compliance.openshift.io/pci-dss-profile patched Compare number of failed compliance before and after remidiate NUM_OF_CIS_FAILED_AFTER_REMIDIATE=$(oc get compliancecheckresult -n openshift-compliance | grep FAIL|grep cis|wc -l) NUM_OF_PCI_DSS_FAILED_AFTER_REMIDIATE=$(oc get compliancecheckresult -n openshift-compliance | grep FAIL|grep pci-dss|wc -l) echo \"Number of failed CIS compliance reduce from $NUM_OF_CIS_FAILED_BEFORE_REMIDIATE to $NUM_OF_CIS_FAILED_AFTER_REMIDIATE\" Openscap Report Generate HTML reports for latest scan results by using oscap tools. Container image with oscap tools already build with this Dockerfile CIS Create pods to mount to CIS reports PVC oc create -f manifests/cis-report.yaml -n openshift-compliance watch oc get pods -l app=report-generator -n openshift-compliance Output NAME READY STATUS RESTARTS AGE cis-master-report 1/1 Running 0 54s cis-report 1/1 Running 0 55s cis-worker-report 1/1 Running 0 55s Generate CIS reports with oscap REPORTS_DIR=compliance-operator-reports mkdir -p $REPORTS_DIR reports=(cis-report cis-worker-report cis-master-report) for report in ${reports[@]} do DIR=$(oc exec -n openshift-compliance $report -- ls -1t /reports|grep -v \"lost+found\"|head -n 1) for file in $(oc exec -n openshift-compliance $report -- ls -1t /reports/$DIR) do echo \"Generate report for $report from $file\" oc exec -n openshift-compliance $report -- oscap xccdf generate report /reports/$DIR/$file > $REPORTS_DIR/$report-$file.html done done oc delete pods -l app=report-generator -n openshift-compliance Sample output Generate report for cis-report from ocp4-cis-api-checks-pod.xml.bzip2 Generate report for cis-worker-report from openscap-pod-f73cef8b1e6a98fa8233b84163f62300c60df10e.xml.bzip2 Generate report for cis-worker-report from openscap-pod-ac5e7838c12d9bea905d474069522b5b502ad724.xml.bzip2 Generate report for cis-master-report from openscap-pod-47877a9e79536f85e552662526e0cd247278bf47.xml.bzip2 Generate report for cis-master-report from openscap-pod-3c5d5e72bf73ebbdbc4ff5cf27f6c3443534e9d6.xml.bzip2 Generate report for cis-master-report from openscap-pod-cd506d793bc03ad62909572b95df1d2d94d13a3e.xml.bzip2 PCI-DSS Create pods to mount to PCI-DSS reports PVC oc create -f manifests/pci-dss-report.yaml -n openshift-compliance watch oc get pods -l app=report-generator -n openshift-compliance Output NAME READY STATUS RESTARTS AGE pci-dss-master-report 1/1 Running 0 11s pci-dss-report 1/1 Running 0 12s pci-dss-worker-report 1/1 Running 0 12 - Generate PCI-DSS reports with *oscap* ```bash REPORTS_DIR=compliance-operator-reports mkdir -p $REPORTS_DIR reports=(pci-dss-report pci-dss-worker-report pci-dss-master-report) for report in ${reports[@]} do DIR=$(oc exec -n openshift-compliance $report -- ls -1t /reports|grep -v \"lost+found\"|head -n 1) for file in $(oc exec -n openshift-compliance $report -- ls -1t /reports/$DIR) do echo \"Generate report for $report from $file\" oc exec -n openshift-compliance $report -- oscap xccdf generate report /reports/$DIR/$file > $REPORTS_DIR/$report-$file.html done done oc delete pods -l app=report-generator -n openshift-compliance ``` Sample output ```bash Generate report for cis-report from ocp4-cis-api-checks-pod.xml.bzip2 Generate report for cis-worker-report from openscap-pod-f73cef8b1e6a98fa8233b84163f62300c60df10e.xml.bzip2 Generate report for cis-worker-report from openscap-pod-ac5e7838c12d9bea905d474069522b5b502ad724.xml.bzip2 Generate report for cis-master-report from openscap-pod-47877a9e79536f85e552662526e0cd247278bf47.xml.bzip2 Generate report for cis-master-report from openscap-pod-3c5d5e72bf73ebbdbc4ff5cf27f6c3443534e9d6.xml.bzip2 Generate report for cis-master-report from openscap-pod-cd506d793bc03ad62909572b95df1d2d94d13a3e.xml.bzip2 ``` Reports Sample reports of OpenShift clusters with auto-remidiation CIS PCI-DSS HTML version here Sample reports of OpenShift clusters with secure kubelet config for machine config master and worker. kubelet config kubeletConfig: eventRecordQPS: 5 tlsCipherSuites: - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 Secure /etc/sysctl.d/90-kubelet.conf for vm.overcommit_memory=1 vm.panic_on_oom=0 kernel.panic=10 kernel.panic_on_oops=1 kernel.keys.root_maxkeys=1000000 kernel.keys.root_maxbytes=25000000 "},"netobserv.html":{"url":"netobserv.html","title":"Network Observability","keywords":"","body":"Network Observability Network Observability Install Operators Network Observability Operator Loki Operator Configure Loki for Network Observability Create Flow Collector Test Install Operators Network Observability Operator Install Network Observability Operator Command Line oc create -f manifests/netobserv-operator.yaml sleep 60 oc wait --for condition=established --timeout=180s \\ crd/flowcollectors.flows.netobserv.io oc get csv -n openshift-netobserv-operator Output customresourcedefinition.apiextensions.k8s.io/flowcollectors.flows.netobserv.io condition met NAME DISPLAY VERSION REPLACES PHASE network-observability-operator.v1.4.2 Network Observability 1.4.2 network-observability-operator.v1.4.1 Succeeded Enable console plugin oc patch console.operator cluster \\ --type json -p '[{\"op\": \"add\", \"path\": \"/spec/plugins/-\", \"value\": \"netobserv-plugin\"}]' Admin Console Enable console plugin Loki Operator Install Loki Operator and config Loki instance Install Loki Operator oc create -f manifests/loki-operator.yaml sleep 30 oc wait --for condition=established --timeout=180s \\ crd/lokistacks.loki.grafana.com oc get csv Output customresourcedefinition.apiextensions.k8s.io/lokistacks.loki.grafana.com condition met NAME DISPLAY VERSION REPLACES PHASE loki-operator.v5.8.1 Loki Operator 5.8.1 loki-operator.v5.8.0 Succeeded Configure Loki for Network Observability Prepare Object Storage configuration including S3 access Key ID, access Key Secret, Bucket Name, endpoint and Region In case of using ODF - Create Bucket - Admin Console - Navigate to Storage -> Object Storage -> Object Bucket Claims - Create ObjectBucketClaim - Claim Name: *netobserv* - StorageClass: *openshift-storage.nooba.io* - BucketClass: *nooba-default-bucket-class* Command line with netobserv-odf-bucket.yaml oc create -f manifests/netobserv-odf-bucket.yaml Retrieve configuration into environment variables S3_BUCKET=$(oc get ObjectBucketClaim netobserv -n openshift-storage -o jsonpath='{.spec.bucketName}') REGION=\"''\" ACCESS_KEY_ID=$(oc get secret netobserv -n openshift-storage -o jsonpath='{.data.AWS_ACCESS_KEY_ID}'|base64 -d) SECRET_ACCESS_KEY=$(oc get secret netobserv -n openshift-storage -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}'|base64 -d) ENDPOINT=\"https://s3.openshift-storage.svc:443\" DEFAULT_STORAGE_CLASS=$(oc get sc -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}') If you have existing S3 bucket used by OpenShift Image Registry S3_BUCKET=$(oc get configs.imageregistry.operator.openshift.io/cluster -o jsonpath='{.spec.storage.s3.bucket}' -n openshift-image-registry) REGION=$(oc get configs.imageregistry.operator.openshift.io/cluster -o jsonpath='{.spec.storage.s3.region}' -n openshift-image-registry) ACCESS_KEY_ID=$(oc get secret image-registry-private-configuration -o jsonpath='{.data.credentials}' -n openshift-image-registry|base64 -d|grep aws_access_key_id|awk -F'=' '{print $2}'|sed 's/^[ ]*//') SECRET_ACCESS_KEY=$(oc get secret image-registry-private-configuration -o jsonpath='{.data.credentials}' -n openshift-image-registry|base64 -d|grep aws_secret_access_key|awk -F'=' '{print $2}'|sed 's/^[ ]*//') ENDPOINT=$(echo \"https://s3.$REGION.amazonaws.com\") DEFAULT_STORAGE_CLASS=$(oc get sc -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}') Create Loki Instance cat manifests/netobserv-loki-s3.yaml \\ |sed 's/S3_BUCKET/'$S3_BUCKET'/' \\ |sed 's/REGION/'$REGION'/' \\ |sed 's|ACCESS_KEY_ID|'$ACCESS_KEY_ID'|' \\ |sed 's|SECRET_ACCESS_KEY|'$SECRET_ACCESS_KEY'|' \\ |sed 's|ENDPOINT|'$ENDPOINT'|'\\ |sed 's|DEFAULT_STORAGE_CLASS|'$DEFAULT_STORAGE_CLASS'|' \\ |oc apply -f - watch oc get po -n netobserv Output NAME READY STATUS RESTARTS AGE loki-compactor-0 0/1 ContainerCreating 0 9s loki-distributor-57476f98bf-vhw9q 0/1 Running 0 9s loki-gateway-54cf794dcf-5pqgd 0/2 ContainerCreating 0 9s loki-gateway-54cf794dcf-6pw4g 0/2 ContainerCreating 0 9s loki-index-gateway-0 0/1 ContainerCreating 0 9s loki-ingester-0 0/1 ContainerCreating 0 9s loki-querier-6fdbf9bf5c-gw8c7 0/1 ContainerCreating 0 9s loki-query-frontend-66d97f7c68-jsgc8 0/1 Running 0 9s Create Flow Collector Create FlowCollector oc create -f manifests/FlowCollector.yaml oc get flowcollector -n netobserv Output flowcollector.flows.netobserv.io/cluster created NAME AGENT SAMPLING (EBPF) DEPLOYMENT MODEL STATUS cluster EBPF 1 DIRECT Ready Remark: This FlowCollector configuration enabled privileged mode with features PacketDrop, DNSTracking and FlowRTT features: - PacketDrop - DNSTracking - FlowRTT privileged: true Test Check Network Observability by Open Administrator -> Observe -> Network Traffic Overview Add filtering by namespace name i.e. monitor for namespace ui and api Config console to show advanced options Flow Rate Topology Network Traffic Pod Raw Data from backend pod request to external system i.e. httpbin.org { \"AgentIP\": \"192.xx.xx.2\", \"Bytes\": 1335, \"DstAddr\": \"3.xx.xx.95\", \"DstMac\": \"0A:xx:xx:xx:xx:01\", \"DstPort\": 443, \"Duplicate\": false, \"Etype\": 2048, \"Flags\": 18, \"FlowDirection\": \"1\", \"IfDirection\": 1, \"Interface\": \"eth0\", \"K8S_ClusterName\": \"842e6829-4726-4a05-90b8-9xxxxa\", \"Packets\": 8, \"Proto\": 6, \"SrcAddr\": \"10.xx.2.24\", \"SrcK8S_HostIP\": \"10.xx.10.20\", \"SrcK8S_HostName\": \"worker-cluster-qtxtv-1\", \"SrcK8S_Name\": \"backend-v1-745bfc554-xgpkc\", \"SrcK8S_Namespace\": \"api\", \"SrcK8S_OwnerName\": \"backend-v1\", \"SrcK8S_OwnerType\": \"Deployment\", \"SrcK8S_Type\": \"Pod\", \"SrcMac\": \"0A:xx:xx:xx:02:18\", \"SrcPort\": 39970, \"TimeFlowEndMs\": 1704805842684, \"TimeFlowRttNs\": 1426096, \"TimeFlowStartMs\": 1704805842547, \"TimeReceived\": 1704805843, \"app\": \"netobserv-flowcollector\" } "},"network-policy.html":{"url":"network-policy.html","title":"Network Policy","keywords":"","body":"Network Policy Network Policy Deploy sample app Create Network Policy Namespace Database Audit Log Audit Log Configuration Deploy sample app Deploy PostgreSQL to namespace database and deploy application todo to namespace app oc new-project database cat manifests/todo-kustomize/base/todo-db.yaml|sed -r 's/(.*)(namespace:)(.+)/\\1\\2 database/' | \\ oc create -f - oc wait --for condition=ready pod -l app=todo-db --timeout=300s -n database oc new-project app oc get secret todo-db -n database -o yaml|sed -r 's/(.*)(namespace:)(.+)/\\1\\2 app/'|\\ oc create -f - cat manifests/todo-kustomize/base/todo.yaml|sed -r 's/(.*)(namespace:)(.+)/\\1\\2 app/'| \\ oc create -f - oc -n app set env deploy/todo quarkus.http.access-log.enabled=true oc wait --for condition=ready pod -l app=todo --timeout=300s -n app oc -n app set env deploy/todo quarkus.datasource.jdbc.url=jdbc:postgresql://todo-db.database.svc.cluster.local/todo oc wait --for condition=ready pod -l app=todo --timeout=300s -n app Create Network Policy Namespace Database Label namespace database for using in network policy oc label ns app tier=api name=app oc label ns database tier=database name=database Enable network policy audit log on namespace database. Audit log is located at /var/log/ovn/acl-audit-log.log on node that pod run oc annotate ns database k8s.ovn.org/acl-logging='{\"deny\": \"info\",\"allow\": \"info\"}' Create policy to deny all incoming traffic to namespace database oc create -f manifests/network-policy-deny-all.yaml -n database Delete todo pod because network policy will not applied for already establised connection oc delete po --all -n app Check status of application todo and its log. NAME READY STATUS RESTARTS AGE todo-8686ddf468-pfnbh 0/1 Running 0 21m Check pod's log and you will find connnection failed 2024-07-12 07:39:01,015 INFO [io.sma.health] (vert.x-eventloop-thread-1) SRHCK01001: Reporting health down status: {\"status\":\"DOWN\",\"checks\":[{\"name\":\"Database connections health check\",\"status\":\"DOWN\",\"data\":{\"\":\"Unable to execute the validation check for the default DataSource: Acquisition timeout while waiting for new connection\"}}]} 2024-07-12 07:43:30,127 WARN [io.agr.pool] (agroal-11) Datasource '': The connection attempt failed. 2024-07-12 07:43:30,127 WARN [io.agr.pool] (agroal-11) Datasource '': Closing connection in incorrect state VALIDATION Create policy to allow traffic from same namespace Use CLI with YAML file oc create -f manifests/network-policy-allow-from-same-namespace.yaml -n database Use Administrator Console Navigate to Networking -> Network Policies and Select Create Create NetworkPolicy Select Ingress Policy with allow from same namespace and there is no need to config pod selector because we want to this rule to apply to all pods Create policy for allow ingress traffic to PostgreSQL pod in namespace database Applied to pod in namespace database with label | Label | Value | |-----------|-------| | app| todo-db | Allow namespace with label | Label | Value | |-----------|-------| | name | app | | tier| api | Allow only pod in above namespace with label | Label | Value | |-----------|-------| | app | todo | Use CLI with YAML fileoc create -f manifests/network-policy-allow-from-todo.yaml -n database Use Administrator Console Navigate to Networking -> Network Policies and Select Create Create NetworkPolicy Config pod selector for database namespace Remark: Click affected pod to previews which pods will be affected by this rule Config allow ingress traffic from namespace app ![](images/network-policy-ingress-rule-2.png) - Config allow port ![](images/network-policy-ingress-rule-3.png) Check all network policies applied to namespace database Check application todo log again. 2024-07-12 08:20:56,016 INFO [io.qua.htt.access-log] (vert.x-eventloop-thread-1) 10.132.0.2 - - [12/Jul/2024:08:20:56 +0000] \"GET /q/health/ready HTTP/1.1\" 200 220 \"-\" \"kube-probe/1.27\" Audit Log Check todo and todo-db pod IP addresses todo NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES todo-8686ddf468-pfnbh 1/1 Running 0 44m 10.132.0.33 cluster2-42cfa8e0-5zhxq todo-db NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES todo-db-85dff77b8b-gxrrh 1/1 Running 0 45m 10.132.0.30 cluster2-42cfa8e0-5zhxq Check audit log on node which todb-db run by using command oc debug/ or with Administrator Console (Compute->Nodes). With both method you need to run comand chroot /host and log is located at /var/log/ovn/acl-audit-log.log Deny 2024-07-12T08:00:50.098Z|00040|acl_log(ovn_pinctrl0)|INFO|name=\"NP:database:Ingress\", verdict=drop, severity=info, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:84:00:01,dl_dst=0a:58:0a:84:00:1e,nw_src=10.132.0.33,nw_dst=10.132.0.30,nw_tos=0,nw_ecn=0,nw_ttl=63,nw_frag=no,tp_src=40504,tp_dst=5432,tcp_flags=psh|ack 2024-07-12T08:00:50.098Z|00041|acl_log(ovn_pinctrl0)|INFO|name=\"NP:database:Ingress\", verdict=drop, severity=info, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:84:00:01,dl_dst=0a:58:0a:84:00:1e,nw_src=10.132.0.33,nw_dst=10.132.0.30,nw_tos=0,nw_ecn=0,nw_ttl=63,nw_frag=no,tp_src=40504,tp_dst=5432,tcp_flags=ack Details: | Parameter | Value | |-----------|-------| |Timestamp |2024-07-12T08:00:50.098Z| |Serial (Running number)| 00040 | |ACL Name | NP:database:Ingress | |Allow or Drop | verdict=drop | |Direction| to-lport (incoming to pod) or from-lport (outgoing from pod)| |Details in OpenFlow format|tcp,vlan_tci=0x0000,dl_src=0a:58:0a:84:00:01,dl_dst=0a:58:0a:84:00:1e,nw_src=10.132.0.33,nw_dst=10.132.0.30,nw_tos=0,nw_ecn=0,nw_ttl=63,nw_frag=no,tp_src=40504,tp_dst=5432,tcp_flags=ack| Details: | Parameter | Value | |-----------|-------| |Source MAC |dl_src=0a:58:0a:84:00:01| |Destination MAC| dl_dst=0a:58:0a:84:00| |Source IP| nw_src=10.132.0.33| |Destination IP| nw_dst=10.132.0.30| |DSCP bits|nw_tos=0| |ECN bits|nw_ecn=0| |TTL|nw_ttl=63| |Source Port|tp_src=40504| |Destination Port|tp_dst=5432| |TCP Flags|tcp_flags=ack| Allow 2024-07-12T07:56:50.098Z|00017|acl_log(ovn_pinctrl0)|INFO|name=\"NP:database:allow-from-todo:Ingress:0\", verdict=allow, severity=info, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:84:00:01,dl_dst=0a:58:0a:84:00:1e,nw_src=10.132.0.33,nw_dst=10.132.0.30,nw_tos=0,nw_ecn=0,nw_ttl=63,nw_frag=no,tp_src=40504,tp_dst=5432,tcp_flags=ack 2024-07-12T07:56:50.098Z|00018|acl_log(ovn_pinctrl0)|INFO|name=\"NP:database:allow-from-todo:Ingress:0\", verdict=allow, severity=info, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:84:00:01,dl_dst=0a:58:0a:84:00:1e,nw_src=10.132.0.33,nw_dst=10.132.0.30,nw_tos=0,nw_ecn=0,nw_ttl=63,nw_frag=no,tp_src=57262,tp_dst=5432,tcp_flags=psh|ack Remark name=\"NP:database:allow-from-todo:Ingress:0\" indicates that network policy allow-from-todo allow incoming traffic Audit Log Configuration Log size, number of retained log files, rate limit and syslog server can be configured. Following YAML show default configuration apiVersion: operator.openshift.io/v1 kind: Network metadata: name: cluster spec: defaultNetwork: ovnKubernetesConfig: policyAuditConfig: destination: \"null\" # libc -> journald / udp:: -> external syslog server maxFileSize: 50 rateLimit: 20 # messages per sec syslogFacility: local0 # kern - RFC5424 "},"acm-application-management.html":{"url":"acm-application-management.html","title":"Application Manageement","keywords":"","body":"Application Management with RHACM Application Management with RHACM RHACM Environment Demo Application with Kustomize RHACM Configuration ACM Console RHACM Environment RHACM with 2 managed clusters Production Cluster(s) labeled with environment=prod Development Cluster(s) labled with environment=dev Demo Application with Kustomize Frontend/Backend App with 2 overlays for environment dev and prod Number of replicas Backend's environment variables Frontend's environment variables . ├── base │ ├── backend-service.yaml │ ├── backend.yaml │ ├── frontend-service.yaml │ ├── frontend.yaml │ ├── kustomization.yaml │ ├── namespace.yaml │ └── route.yaml └── overlays ├── dev │ ├── backend.yaml │ ├── frontend.yaml │ └── kustomization.yaml └── prod ├── backend.yaml ├── frontend.yaml └── kustomization.yaml RHACM Configuration RHACM application managment configuration Create Namespace Create Channel Create Applicatoion Create subscription for production and development environment Create placement rule for production and development environment Deploy oc apply -f manifests/acm-app-management/01_namespace.yaml oc apply -f manifests/acm-app-management/02_channel.yaml oc apply -f manifests/acm-app-management/03_application_demo_app.yaml oc apply -f manifests/acm-app-management/04_subscription_dev.yaml oc apply -f manifests/acm-app-management/04_subscription_prod.yaml oc apply -f manifests/acm-app-management/05_placement_dev.yaml oc apply -f manifests/acm-app-management/05_placement_prod.yaml or for i in $(ls -1 manifests/acm-app-management) do oc apply -f manifests/acm-app-management/$i done ACM Console Demo App topology Check number of replicas for prod Filter by subscription "},"acm-hibernate.html":{"url":"acm-hibernate.html","title":"Cost saving with hibernating OpenShift","keywords":"","body":"Hibernate OpenShift on Cloud Providers for Cost Saving Hibernate OpenShift on Cloud Providers for Cost Saving Prerequisites Introduction How ACM acheive Hibernate policies for OpenShift Steps 1-2-3 Prerequisites OpenShift 4.6 or 4.7 Cluster-admin user access Red Hat Advanced Cluster Management Cloud Provider Credentials Introduction Original article from Hibernate for cost savings for Advanced Cluster Management Provisioned Clusters with Subscriptions OpenShift 4 has the ability to suspend and resume clusters on Cloud Providers. Red Hat Advanced Cluster Management extends this capability through its Cluster Lifecycle Management (Hive), where you can have a policy to hibernate the clusters to save the cost saving in non-working hours (ex. 16 hours in a day is 16/24 = 66% saving) How ACM acheive Hibernate policies for OpenShift We will walkthrough high-level process how ACM can define the herbernate policy for the cluster on Clouds Clone the cluster-hibernate repository to your environment Create the Running/Hibernate manifest files and put them in your Channel Create the Subscription to local hub cluster with your desire time windows Sample picture Steps 1-2-3 Steps: a b c d Code Block uname -a --- sample: yaml spec: key1: value1 item_list: - name: item1 property: prop1 - name: item2 property: prop2 Sample inline yaml to oc apply cat "},"acs.html":{"url":"acs.html","title":"ACS","keywords":"","body":"Advanced Cluster Security for Kubernetes (ACS) Advanced Cluster Security for Kubernetes (ACS) Installation Central Installation [Optional] Create Central at Infra Nodes Access Central Single Sign-On with OpenShift Secured Cluster Services (Managed Cluster) Operator Install Secure Cluster Services on remote cluster CLI roxctl and Helm View Managed Cluster Integration with Nexus Setup Nexus Config ACS Container Image with Vulnerabilities Shift Left Security kube-linter Scan and check image with roxctl Sample of Default Policies Enforce policy on Buid Stage Use roxctl in Pipeline Stackrox Jenkins Plugin Enable Policy Enforce policy on Deployment Stage Enforce policy on Runtime Stage Exec into Pod NMAP Compliance Overall reports Compliance Operator Network Graph Sample Application Network Policies Installation Central Installation Install Operator Web Console Select Advanced Cluster Security for Kubernetes ![](images/acs-install-operator-01.png) - Accept default parameters ![](images/acs-install-operator-02.png) Use CLI to install operator oc create -f manifests/acs-subscription.yaml oc get csv -n rhacs-operator Output namespace/rhacs-operator created operatorgroup.operators.coreos.com/rhacs-operator-bqbtj created subscription.operators.coreos.com/rhacs-operator created NAME DISPLAY VERSION REPLACES PHASE rhacs-operator.v4.2.2 Advanced Cluster Security for Kubernetes 4.2.2 rhacs-operator.v4.2.1 Succeeded Remark: It should be better if you choose Manual for Update Approval instead of Automatic for production environment Install roxctl CLI Download latest binary from here For OSX curl -O https://mirror.openshift.com/pub/rhacs/assets/latest/bin/Darwin/roxctl Or use roxctl from container podman run docker://quay.io/stackrox-io/roxctl Create ACS Central with acs-central.yaml If you want to use custom certificate storedfor central add following section to acs-central.yaml spec: central: defaultTLSSecret: name: acs-central Create Central oc create -f manifests/acs-central.yaml Remark Central is configured with memory limit 8 Gi Default RWO storage for central is 100 GB Output central.platform.stackrox.io/stackrox-central-services created Check status oc describe central/stackrox-central-services -n stackrox watch oc get pods -n stackrox Output NAME READY STATUS RESTARTS AGE central-9c5567677-s9ggb 1/1 Running 0 2m44s central-db-77b8c8d6c9-2jcr2 1/1 Running 0 2m44s scanner-566f5f5b5b-d6t6d 1/1 Running 0 2m44s scanner-566f5f5b5b-tfgng 1/1 Running 0 2m44s scanner-db-69cd9c4949-hpc4f 1/1 Running 0 2m44s Check PVC oc get pvc -n stackrox Output NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE central-db Bound pvc-a4b5a0ec-7e28-495c-860c-c715aefd836c 100Gi RWO gp3-csi 100s Resources consumed by ACS central CPU Memory [Optional] Create Central at Infra Nodes Infra Nodes preparation Label Infra nodes oc label node node-role.kubernetes.io/infra=\"\" oc label node node-role.kubernetes.io/acs=\"\" Taint infra nodes with infra-acs oc adm taint node infra-acs=reserved:NoSchedule Create Central with acs-central-infra.yaml oc create -f manifests/acs-central-infra.yaml -n stackrox Access Central URL and password to access ACS Console ROX_URL=https://$(oc get route central -n stackrox -o jsonpath='{.spec.host}') ROX_CENTRAL_ADDRESS=$(oc get route central -n stackrox -o jsonpath='{.spec.host}'):443 ROX_PASSWORD=$(oc get secret central-htpasswd -n stackrox -o jsonpath='{.data.password}'|base64 -d) Single Sign-On with OpenShift Navigate to Platform Configuration -> Access Control then click Create auth Provider and select OpenShift Auth Input configuration then click save Name: OpenShift Minium access role: Analyst Rules: mapped spcific user to Admin role Logout and refresh your browser. OpenShift provider will be available for you to login with OpenShift's user account Secured Cluster Services (Managed Cluster) Operator Login to ACS console Generate cluster init bundle Platform Configuration -> Integrations -> Cluster Init Bundle -> Generate Bundle Input cluster name download Kubernetes Secrets file for installation with Operator or Helm values file for installation with roxctl Create namespace for Secured Cluster Services oc new-project stackrox-secured-cluster Create secret from previously downloaded Kubernetes Secrets file oc create -f cluster1-cluster-init-secrets.yaml -n stackrox-secured-cluster Output secret/admission-control-tls created secret/collector-tls created secret/sensor-tls created Install Secure Cluster Services on local cluster Create Secured Cluster Service with acs-secured-cluster.yaml oc create -f manifests/acs-secured-cluster.yaml -n stackrox-secured-cluster Remark: acs-secured-cluster.yaml is prepared for install Secured Cluster Service within the same cluster with Central. If you want Admission Control run on Infra Nodes with acs-secured-cluster-infra.yaml oc create -f manifests/acs-secured-cluster-infra.yaml -n stackrox-secured-cluster Check status oc get securedcluster/cluster1 -n stackrox-secured-cluster -o jsonpath='{.status.conditions[0]}' oc get pods -n stackrox-secured-cluster Output {\"lastTransitionTime\":\"2023-11-07T06:13:02Z\",\"message\":\"StackRox Secured Cluster Services 4.2.2 has been installed.\\n\\n\\n\\nThank you for using StackRox!\\n\",\"reason\":\"InstallSuccessful\",\"status\":\"True\",\"type\":\"Deployed\"} NAME READY STATUS RESTARTS AGE admission-control-64487c7986-ff6j9 1/1 Running 0 10m admission-control-64487c7986-t25ng 1/1 Running 0 10m admission-control-64487c7986-wjl7w 1/1 Running 0 10m collector-dpjfk 3/3 Running 0 10m collector-mj778 3/3 Running 0 10m collector-n8cch 3/3 Running 0 10m collector-z49pb 3/3 Running 0 10m scanner-7f75dd5879-m6cfm 1/1 Running 0 10m scanner-7f75dd5879-vpkcj 1/1 Running 0 10m scanner-db-6c555b4b7d-x49hl 1/1 Running 0 10m sensor-65c777cf9f-c8zvx 1/1 Running 0 10m Remark Adminission control is high availability with default 3 pods Collector is run on every nodes including control plane Resources consumed by admission control and collector CPU Memory Install Secure Cluster Services on remote cluster Generate cluster init bundle Create secret from previously downloaded Kubernetes Secrets file oc new-project stackrox-secured-cluster oc create -f cluster2-cluster-init-secrets.yaml -n stackrox-secured-cluster Install ACS operator Create Secured Cluster Service with centralEndpoint set to Central's route. Get Central's route and save to ROX_HOST environment variable ROX_HOST=$(oc get route central -n stackrox -o jsonpath='{.spec.host}') Login to remote cluster and run following command. cat manifests/acs-secured-cluster.yaml | \\ sed 's/central.stackrox.svc/'$ROX_HOST'/' | \\ sed s/cluster1/cluster2/ | \\ oc create -n stackrox-secured-cluster -f - CLI roxctl and Helm Create authentication token Login to Central echo \"ACS Console: https://$(oc get route central -n stackrox -o jsonpath='{.spec.host}')\" Platform Configuration -> Integrations -> Authentication Tokens. Select StackRox API Token then generate token and copy token to clipboard Token Name: admin Role: Admin Set environment variable export ROX_API_TOKEN= export ROX_CENTRAL_ADDRESS=$(oc get route central -n stackrox -o jsonpath='{.spec.host}'):443 Add Helm repository helm repo add rhacs https://mirror.openshift.com/pub/rhacs/charts/ Install Secure Cluster Services on local cluster Generate cluster init bundle CLUSTER_NAME=cluster1 roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" central init-bundles generate $CLUSTER_NAME \\ --output $CLUSTER_NAME-init-bundle.yaml Example of output INFO: Successfully generated new init bundle. Name: cluster1 Created at: 2022-05-22T07:43:47.645062387Z Expires at: 2023-05-22T07:44:00Z Created By: admin ID: 84c50c04-de36-450d-a5d6-7a23f1dd563c INFO: The newly generated init bundle has been written to file \"cluster1-init-bundle.yaml\". INFO: The init bundle needs to be stored securely, since it contains secrets. INFO: It is not possible to retrieve previously generated init bundles. Create collectors helm install -n stackrox-cluster --create-namespace stackrox-secured-cluster-services rhacs/secured-cluster-services \\ -f ${CLUSTER_NAME}-init-bundle.yaml \\ --set clusterName=${CLUSTER_NAME} \\ --set imagePullSecrets.allowNone=true Install Secure Cluster Services on Remote cluster Generate cluster init bundle CLUSTER_NAME=cluster2 roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" central init-bundles generate $CLUSTER_NAME \\ --output $CLUSTER_NAME-init-bundle.yaml Create collectors helm install -n stackrox --create-namespace stackrox-secured-cluster-services rhacs/secured-cluster-services \\ -f ${CLUSTER_NAME}-init-bundle.yaml \\ --set centralEndpoint=${ROX_CENTRAL_ADDRESS} \\ --set clusterName=${CLUSTER_NAME} \\ --set imagePullSecrets.allowNone=true Check collector pods oc get pods -n stackrox -l app=collector,app.kubernetes.io/name=stackrox Output NAME READY STATUS RESTARTS AGE collector-5hmzt 2/2 Running 0 87s collector-dmpps 2/2 Running 0 87s collector-ffpdg 2/2 Running 0 87s collector-rfkq2 2/2 Running 0 87s collector-x4gtb 2/2 Running 0 87s View Managed Cluster Check ACS Console Dashboard Platform Configuration -> Clusters Overall status ![](images/acs-manged-cluster-dynamic-configuration-01.png) Dynamic configuration ![](images/acs-manged-cluster-dynamic-configuration-02.png) Helm-managed cluster ![](images/acs-console-managed-clusters-helm.png) Integration with Nexus Setup Nexus Create namespace oc new-project ci-cd Create nexus cd bin ./setup_nexus.sh Example of output expose port 5000 for container registry service/nexus-registry exposed route.route.openshift.io/nexus-registry created NEXUS URL = nexus-ci-cd.apps.cluster-**tlc.com NEXUS User admin: ***** NEXUS User jenkins: ********** Nexus password is stored at nexus_password.txt Login to nexus with user admin and initial password and set new admin password. Browse repository Copy sample container images to nexus NEXUS=$(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')/repository/container allImages=(backend:v1 backend:native backend:CVE-2020-36518 frontend-js:v1 frontend-js:node log4shell:latest backend-native:distroless) for image in $allImages do echo \"############## Copy $image ##############\" podman run quay.io/skopeo/stable \\ copy --src-tls-verify=true \\ --dest-tls-verify=false \\ --src-no-creds \\ --dest-username admin \\ --dest-password $NEXUS_PASSWORD \\ docker://quay.io/voravitl/$image \\ docker://$NEXUS/$image echo \"##########################################\" done Check Nexus docker repository Config ACS Login to ACS Central Platform Configuration -> Integrations -> Sonatype Nexus -> New Integration Check for Nexus Container Registry address echo \"Endpoint: $(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')\" Input User, Password and Nexus Registry address then click Test and Save Container Image with Vulnerabilities Deploy sample application oc new-project test oc run log4shell --labels=app=log4shell --image=$(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')/log4shell:latest -n test oc run backend --labels=app=CVE-2020-36518 --image=$(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')/backend:CVE-2020-36518 -n test watch oc get pods -n test Check ACS Dashboard. 1 Criticals violation will be found. Drill down for more information ![](images/acs-dashborad-log4shell-1.png) CVE Information ![](images/acs-dashborad-log4shell-2.png) CVSS score: https://nvd.nist.gov/vuln-metrics/cvss Search by CVE. Vulnerability Management -> Dashboard -> IMAGES -> Search for CVE-2021-44228 Details information Naviate to Violations, You will find Fixable at least important that is alert for deployment with fixable vulnerbilities on backend deployment Affected deployment Drilled down to integrated nexus Shift Left Security kube-linter Try kube-linter with deployment YAML kube-linter lint manifests/mr-white.yaml Download kube-linter from this link Sample recommendation ``` KubeLinter 0.6.5 manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) environment variable SECRET in container \"mr-white\" found (check: env-var-secret, remediation: Do not use raw secrets in environment variables. Instead, either mount the secret as a file or use a secretKeyRef. Refer to https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets for details.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) The container \"mr-white\" is using an invalid container image, \"quay.io/voravitl/mr-white:latest\". Please use images that are not blocked by the BlockList criteria : [\".:(latest)$\" \"^:$\" \"(.*/:]+)$\" (check: latest-tag, remediation: Use a container image with a specific tag other than latest.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in the container securityContext.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" is not set to runAsNonRoot (check: run-as-non-root, remediation: Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" has cpu request 0 (check: unset-cpu-requirements, remediation: Set CPU requests and limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" has cpu limit 0 (check: unset-cpu-requirements, remediation: Set CPU requests and limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" has memory request 0 (check: unset-memory-requirements, remediation: Set memory requests and limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.) manifests/mr-white.yaml: (object: /mr-white apps/v1, Kind=Deployment) container \"mr-white\" has memory limit 0 (check: unset-memory-requirements, remediation: Set memory requests and limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.) Error: found 8 lint errors - Try kube-linter with [backend-v1.yaml](manifests/backend-v1.yaml) ```bash kube-linter lint manifests/backend-v1.yaml Output manifests/backend.yaml: (object: /backend-v1 apps/v1, Kind=Deployment) container \"backend\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in the container securityContext.) Container \"backend\" still does not have a read-only root file system because Vert.X still need to write /tmp then try backend deployment with emptyDir Try agin with backend-v1-emptyDir.yaml which set readOnlyRootFilesystem to true kube-linter lint manifests/backend-v1-emptyDir.yaml Output KubeLinter 0.6.5 No lint errors found! Scan and check image with roxctl Create token for DevOps tools Navigate to Platform Configuration -> Integrations -> Authentication Token -> API Token Click Generate Token Input token name and select role Continuous Integration Copy and save token. Set API token to environment variable export ROX_API_TOKEN= ROX_CENTRAL_ADDRESS=$(oc get route central -n stackrox -o jsonpath='{.spec.host}'):443 Scan image to check for vulnerbilities roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" image scan --image $(oc get -n ci-cd route nexus-registry -o jsonpath='{.spec.host}')/backend:v1 --output=table roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" image scan --image $(oc get -n ci-cd route nexus-registry -o jsonpath='{.spec.host}')/backend:CVE-2020-36518 --output=json| jq '.result.summary.CRITICAL' Scan all images in Nexus registry ROX_CENTRAL_ADDRESS=$(oc get route central -n stackrox -o jsonpath='{.spec.host}'):443 allImages=(backend:v1 backend:11-ubuntu backend:CVE-2020-36518 frontend-js:v1 frontend-js:node frontend-js:CVE-2020-28471 log4shell:latest backend-native:v1 backend-native:distroless) for image in $allImages do roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" image scan --image $(oc get -n ci-cd route nexus-registry -o jsonpath='{.spec.host}')/$image --output=table done Resources comsumed by ACS Central Check images in image registry Stackrox can check for vulnerbilities in libraries used by Java applicaion. Check for image backend:CVE-2020-36518 roxctl --insecure-skip-tls-verify \\ -e \"$ROX_CENTRAL_ADDRESS\" image check \\ --image $(oc get -n ci-cd route nexus-registry -o jsonpath='{.spec.host}')/backend:CVE-2020-36518 \\ --output=table Output Remark: Column *BREAKS BUILD* indicate that this violation will be stop build process or not Image backend:v1 roxctl --insecure-skip-tls-verify \\ -e \"$ROX_CENTRAL_ADDRESS\" image check \\ --image $(oc get -n ci-cd route nexus-registry -o jsonpath='{.spec.host}')/backend:v1 \\ --output=table Output Deployment check roxctl --insecure-skip-tls-verify -e \"$ROX_CENTRAL_ADDRESS\" deployment check --file=manifests/backend-bad-example.yaml Remark: BREAKS DEPLOY column indicate that deployment will be blocked by ACS or not Policies. Enter Policy, press tab then input label. ![](images/acs-search-policy-label.png) - Clone policy - Input name and set severity ![](images/acs-label-policy-01.png) - Set policy behavior for build time and runtime ![](images/acs-label-policy-02.png) - Add criterias to check label app and version - --> Sample of Default Policies Deploy Mr.White to namespace bad-ns oc new-project bad-ns --display-name=\"This is Mr. White Namespace\" oc apply -f manifests/mr-white.yaml -n bad-ns oc expose deploy mr-white -n bad-ns oc expose svc mr-white -n bad-ns MR_WHITE=$(oc get route mr-white -o jsonpath='{.spec.host}' -n bad-ns) Run following command curl http://$MR_WHITE/exec/whoami curl http://$MR_WHITE/exec/useradd%20jessy-pinkman curl http://$MR_WHITE/exec/groupadd%20van-cooking-station oc exec -n bad-ns $( oc get po -o custom-columns='Name:.metadata.name' -n bad-ns --no-headers) -- pwd Output Check for policies Shell Spawned by Java Application Linux Group Add Execution Linux User Add Execution Environment Variable Contains Secret Secret Mounted as Environment Variable Kubernetes Actions: Exec into Pod Latest tag Enforce policy on Buid Stage Setup Jenkins and SonarQube cd bin ./setup_cicd_projects.sh ./setup_jenkins.sh ./setup_sonar.sh Remark: This demo need Nexus Use roxctl in Pipeline Create buildConfig backend-build-stackrox-pipeline with Jenkins. cat manifests/backend-build-stackrox-pipeline.yaml| \\ sed 's/value: NEXUS_REGISTRY/value: '$(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')'/' | \\ oc create -n ci-cd -f - Create build config pipelines oc create -f manifests/backend-build-stackrox-pipeline.yaml -n ci-cd Create secret name stackrox-token in namespace ci-cd to store Stackrox API token echo \"...Token..\" > token oc create secret generic stackrox-token -n ci-cd --from-file=token rm -f token Login to Jenkins echo \"Jenkins URL: https://$(oc get route jenkins -n ci-cd -o jsonpath='{.spec.host}')\" Start backend-build-stackrox-pipeline. Pipeline will failed because image contains CVEs and violate policy Fixable Severity at least Important Detail Remark: Check backend-build-stackrox-pipeline Jenkinsfile for roxctl command in Pipeline. stage('Scan Image') { steps { container(\"tools\") { echo \"Scan image: ${NEXUS_REGISTRY}/${imageName}:${devTag}}\" echo \"Central: ${env.ROX_CENTRAL_ADDRESS}\" sh \"export ROX_API_TOKEN=${ROX_API_TOKEN};roxctl --insecure-skip-tls-verify -e ${ROX_CENTRAL_ADDRESS} image check --image=${NEXUS_REGISTRY}/${imageName}:${devTag} --output=table\" } } } backend app intentionally use jackson-bind that contains CVEs. You can check this in pom.xml Change ref from cve to master for backend. Branch master already update jackson-bind to updated version. source: contextDir: Jenkinsfile/build-stackrox git: ref: master uri: https://gitlab.com/ocp-demo/backend_quarkus.git Stackrox Jenkins Plugin Install Stackrox plugin and restart Jenkins Create buildConfig backend-build-stackrox-with-plugin-pipeline with Jenkins. cat manifests/backend-build-stackrox-with-plugin-pipeline.yaml| \\ sed 's/value: NEXUS_REGISTRY/value: '$(oc get route nexus-registry -n ci-cd -o jsonpath='{.spec.host}')'/' | \\ oc create -n ci-cd -f - Start backend-build-stackrox-with-plugin-pipeline. Pipeline will failed because image contains CVEs and violate policy Fixable Severity at least Important Detailed report in Jenkins Remark: Jenkinsfile for backend-build-stackrox-with-plugin-pipeline stage('Scan Image') { steps { script { echo \"Scan image: ${NEXUS_REGISTRY}/${imageName}:${devTag}}\" stackrox ( apiToken: \"${ROX_API_TOKEN}\", caCertPEM: '', enableTLSVerification: false, failOnCriticalPluginError: true, failOnPolicyEvalFailure: true, portalAddress: \"${env.ROX_CENTRAL_ADDRESS}\", imageNames: \"${NEXUS_REGISTRY}/${imageName}:${devTag}\" ) } } } Enable Policy Login to ACS Console, Select Menu Platform -> Configuration, type policy in search bar then input curl Select policy Curl in image and edit policy Select policy behavior select inform and enforce enable on build Enable policy curl in image Re-run Jenkins pipeline backend-build-stackrox-pipeline and check for report Enforce policy on Deployment Stage Create project demo Open ACS Central. Navigate to Platform Configuration->Policies At the search filter. Search for policy No resources requests or limits specified Click Action->Clone Policy and set name to Request and Limit are required Edit Policy behavior and set response method to Inform and Enforce Navigate to Policy Scope and Add inclusion scope input following values Cluster: Cluster1 Namespace: demo Navigate to Policy Scope and check for Preview violations Deploy following app oc create -f manifests/mr-white.yaml -n demo Output ```bash Error from server (Failed currently enforced policies from StackRox): error when creating \"manifests/mr-white.yaml\": admission webhook \"policyeval.stackrox.io\" denied the request: The attempted operation violated 1 enforced policy, described below: Policy: Request and Limit are required Description: ↳ Alert on deployments that have containers without resource requests and limits Rationale: ↳ If a container does not have resource requests or limits specified then the hostmay become over-provisioned. Remediation: ↳ Specify the requests and limits of CPU and Memory for your deployment. Violations: CPU limit set to 0 cores for container 'mr-white' CPU request set to 0 cores for container 'mr-white' Memory limit set to 0 MB for container 'mr-white' Memory request set to 0 MB for container 'mr-white' In case of emergency, add the annotation {\"admission.stackrox.io/break-glass\": \"ticket-1234\"} to your deployment with an updated ticket number - ACS Console, navigate to Violations ![](images/acs-policy-violation-no-request-limit.png) ## Enforce policy on Runtime Stage ### Exec into Pod - Platform configuration -> Policies - Search for Policy Kubernetes Actions: Exec into Pod - Click Action -> Edit Policy - Click Next to Policy Behavior and enable Enforce on runtime. This will make ACS kill the offend pod that try to run exec. ![](images/acs-enforce-on-runtime.png) - Save Policy - Run curl inside backend's pod ```bash oc new-project project1 oc apply -f manifests/backend-v1.yaml -n project1 oc exec -n project1 $(oc get pods -n project1 -l app=backend --no-headers | head -n 1 | awk '{print $1}') -- curl -s http://backend:8080 Output ```bash command terminated with exit code 6 ``` Check Console Navigate to Dashboard -> Violation Details information NMAP Platform configuration -> Policies Search for nmap Execution Verify that status is enabled Deploy container tools oc apply -f manifests/network-tools.yaml -n project1 Execute namp oc exec $(oc get pods -l app=network-tools --no-headers -n project1 | head -n 1 | awk '{print $1}') -n project1 -- nmap -v -Pn backend.prod-app.svc Output Starting Nmap 7.70 ( https://nmap.org ) at 2022-05-26 02:05 UTC Initiating Parallel DNS resolution of 1 host. at 02:05 Completed Parallel DNS resolution of 1 host. at 02:05, 0.00s elapsed Initiating Connect Scan at 02:05 Scanning backend.prod-app.svc (172.30.14.34) [1000 ports] Discovered open port 8080/tcp on 172.30.14.34 Completed Connect Scan at 02:05, 4.31s elapsed (1000 total ports) Nmap scan report for backend.prod-app.svc (172.30.14.34) Host is up (0.0019s latency). rDNS record for 172.30.14.34: backend.prod-app.svc.cluster.local Not shown: 999 filtered ports PORT STATE SERVICE 8080/tcp open http-proxy Read data files from: /usr/bin/../share/nmap Nmap done: 1 IP address (1 host up) scanned in 4.44 seconds Check ACS Central. Navigate to Violations nmap execution is detected details information Violation Deployment Compliance Initial compliance scan Overall reports Compliance Operator ACS integrated with OpenShift Compliance Operator. Following show result for OpenShift Compliance Operator with CIS profile and already remidiated by Operator Network Graph Sample Application Deploy frontend and backend app on namespace ui and api respectively oc new-project ui oc new-project api oc create -f manifests/frontend.yaml -n ui oc create -f manifests/backend-v1.yaml -n api oc expose deployment/backend-v1 -n api oc set env deployment/frontend-v1 BACKEND_URL=http://backend-v1.api.svc:8080 -n ui oc set env deployment/frontend-v2 BACKEND_URL=http://backend-v1.api.svc:8080 -n ui Check for frontend URL FRONTEND_URL=$(oc get route/frontend -o jsonpath='{.spec.host}' -n ui) Test with curl curl https://$FRONTEND_URL Network Policies Check Network Graph in ACS Console Select cluster1 then select namespace ui and api Click deployment backend-v1 Select tab Flows Ingress from frontend-v1 in namespace ui Ingress from frontend-v2 in namespace ui Egress from backend-v1 in namespace api Egress from backend-v1 to external entities Select above entries and Add to Baseline Click tab Baselines then click Download baseline as network policy Check network policies which created from actual traffic baseline apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: creationTimestamp: \"2023-04-18T16:04:11Z\" labels: network-policy-generator.stackrox.io/from-baseline: \"true\" name: stackrox-baseline-generated-backend-v1 namespace: api spec: ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: ui podSelector: matchLabels: app: frontend version: v2 - namespaceSelector: matchLabels: kubernetes.io/metadata.name: ui podSelector: matchLabels: app: frontend version: v1 - podSelector: matchLabels: app: backend version: v1 ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: backend version: v1 policyTypes: - Ingress status: {} "},"build-with-dev-console.html":{"url":"build-with-dev-console.html","title":"Developer Console","keywords":"","body":"Developer Console Developer Console Upload Uber JARUpload Uber JAR Select upload JAR Upload your Uber JAR and select Builder Image Name your application and select parameters e.g. Ingress (Route), request/limit View build logs Build log Check for build config by select menu Builds->backend->YAML Base image is from sourceStrategy Source type is binary strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: openshift name: 'java:openjdk-11-ubi8' postCommit: {} source: type: Binary binary: {} Check your deployed application "},"build-with-oc.html":{"url":"build-with-oc.html","title":"Application Build & Deployment with oc","keywords":"","body":"Build Container Image with OC CLI ______ ______ ______ __ __ / __ \\ / | / || | | | | | | | | ,----' | ,----'| | | | | | | | | | | | | | | | | `--' | | `----. | `----.| `----.| | \\______/ \\______| \\______||_______||__| Build Container Image with OC CLI Configure OpenShift with external registry (optional) Source build All-in-One Build and Deploy Binary build with Dockerfile Configure OpenShift with external registry (optional) Create docker secret to access external registry With user and password NEXUS_REGISTRY=external_registry.example.com oc create secret docker-registry nexus-registry --docker-server=$NEXUS_REGISTRY \\ --docker-username=$CICD_NEXUS_USER \\ --docker-password=$CICD_NEXUS_PASSWORD \\ --docker-email=unused \\ From dockercfg file apiVersion: v1 kind: Secret metadata: name: nexus-registry type: kubernetes.io/dockercfg data: .dockercfg: | \"\" Link secret for builder oc secrets link default nexus-registry --for=pull Link secret for pull image oc secrets link builder nexus-registry For insecure registry Edit image.config.openshift.io/cluster oc edit image.config.openshift.io/cluster Add insecure registry to spec spec: registrySources: insecureRegistries: - nexus-registry.ci-cd.svc.cluster.local - nexus-registry.example.com Source build All-in-One Use source-to-image from git this will create image stream build config deployment service oc new-app https://gitlab.com/ocp-demo/frontend-js \\ --name=frontend Check build log oc logs bc/frontend --follow Build and Deploy Create build config oc new-build --name=frontend-v1 -l app=frontend-v1 \\ https://gitlab.com/ocp-demo/frontend-js Create deployment and service oc new-app frontend-v1 Binary build with Dockerfile Clone sample Backend Quarkus git clone https://gitlab.com/ocp-demo/backend_quarkus Create application binary cd code mvn clean package -DskipTests=true Create Build Config Push to OpenShift's internal image registry APP_NAME=backend oc new-build --binary --name=$APP_NAME -l app=$APP_NAME Push to OpenShift's external image registry APP_NAME=backend EXTERNAL_REGISTRY=nexus-registry.example.com EXTERNAL_REGISTRY_SECRET=nexus-registry TAG=latest oc new-build --binary --to-docker=true \\ --to=$EXTERNAL_REGISTRY/$APP_NAME:$TAG \\ --push-secret=$EXTERNAL_REGISTRY_SECRET \\ --name=$APP_NAME \\ -l app=$APP_NAME Change build strategy to DockerStrategy oc patch bc/$APP_NAME \\ -p \"{\\\"spec\\\":{\\\"strategy\\\":{\\\"dockerStrategy\\\":{\\\"dockerfilePath\\\":\\\"src/main/docker/Dockerfile.jvm\\\"}}}}\" Build container image oc start-build $APP_NAME --from-dir=. --follow Create Application from internal image registry oc new-app --image-stream=${APP_NAME} \\ --labels=app.openshift.io/runtime=quarkus,app.openshift.io/runtime-version=11,app.kubernetes.io/part-of=Demo Pause rollout deployment oc expose svc $APP_NAME Create liveness and readiness probe oc set probe deployment/$APP_NAME --readiness \\ --get-url=http://:8080/q/health/ready \\ --initial-delay-seconds=8 \\ --failure-threshold=1 --period-seconds=10 oc set probe deployment/$APP_NAME --liveness \\ --get-url=http://:8080/q/health/live \\ --initial-delay-seconds=5 -\\ -failure-threshold=3 --period-seconds=10 Set request and limit oc set resources deployment $APP_NAME --requests=\"cpu=50m,memory=100Mi\" oc set resources deployment $APP_NAME --limits=\"cpu=150m,memory=150Mi\" Create configmap oc create configmap $APP_NAME --from-file=config/application.properties oc set volume deployment/{APP_NAME --add --name=$APP_NAME-config \\ --mount-path=/deployments/config/application.properties \\ --sub-path=application.properties \\ --configmap-name=$APP_NAME Set HPA oc autoscale deployment $APP_NAME --min 2 --max 4 --cpu-percent=60 Resume rollout deployment oc rollout resume deployment $APP_NAME Create route Expose service oc expose svc $APP_NAME Create route with edge TLS oc create route edge $APP_NAME --service=$APP_NAME --port=8080 "},"build-with-odo.html":{"url":"build-with-odo.html","title":"Application Build & Deployment with odo","keywords":"","body":"Build Container Image with OpenShift DO ___ ____ _ _ __ _ ____ ___ / _ \\ _ __ ___ _ __ / ___|| |__ (_)/ _| |_ | _ \\ / _ \\ | | | | '_ \\ / _ \\ '_ \\\\___ \\| '_ \\| | |_| __| | | | | | | | | |_| | |_) | __/ | | |___) | | | | | _| |_ | |_| | |_| | \\___/| .__/ \\___|_| |_|____/|_| |_|_|_| \\__| |____/ \\___/ |_| Build Container Image with OpenShift DO ODO Catalog Sample Java ODO Catalog list odo catalog odo catalog list components Catalog Odo Devfile Components: NAME DESCRIPTION REGISTRY java-maven Upstream Maven and OpenJDK 11 DefaultDevfileRegistry java-openliberty Open Liberty microservice in Java DefaultDevfileRegistry java-quarkus Upstream Quarkus with Java+GraalVM DefaultDevfileRegistry java-springboot Spring Boot® using Java DefaultDevfileRegistry java-vertx Upstream Vert.x using Java DefaultDevfileRegistry java-wildfly Upstream WildFly DefaultDevfileRegistry java-wildfly-bootable-jar Java stack with WildFly in bootable Jar mode, OpenJDK 11 and... DefaultDevfileRegistry nodejs Stack with NodeJS 12 DefaultDevfileRegistry python Python Stack with Python 3.7 DefaultDevfileRegistry python-django Python3.7 with Django DefaultDevfileRegistry Odo S2I Components: NAME PROJECT TAGS SUPPORTED java openshift latest,openjdk-11-el7,openjdk-11-ubi8,openjdk-8-el7 YES nodejs openshift 12-ubi8,14-ubi8,latest YES dotnet openshift 2.1-el7,2.1-ubi8,3.1-el7,3.1-ubi8 NO golang openshift 1.13.4-ubi7,1.14.7-ubi8,latest NO httpd openshift 2.4-el7,2.4-el8,latest NO java openshift openjdk-8-ubi8 NO nginx openshift 1.14-el8,1.16-el7,1.16-el8,1.18-ubi7,1.18-ubi8,latest NO nodejs openshift 10-ubi7,10-ubi8,12-ubi7,14-ubi7 NO perl openshift 5.26-el7,5.26-ubi8,5.30-el7,5.30-ubi8,latest NO php openshift 7.2-ubi8,7.3-ubi7,7.3-ubi8,7.4-ubi8,latest NO python openshift 2.7-ubi7,2.7-ubi8,3.6-ubi8,3.8-ubi7,3.8-ubi8,latest NO ruby openshift 2.5-ubi7,2.5-ubi8,2.6-ubi7,2.6-ubi8,2.7-ubi7,2.7-ubi8,latest NO Sample Java Create project odo project create odo-demo Create Application From binary git clone https://gitlab.com/ocp-demo/backend_quarkus && cd backend_quarkus cd code mvn clean package -DskipTests=true -Dquarkus.package.uber-jar=true odo create java backend --s2i --binary target/*.jar Sample output Validation ✓ Validating component [75ms] Please use `odo push` command to create the component with source deployed From source code odo create nodejs frontend --s2i --git https://gitlab.com/ocp-demo/frontend-js Check for odo configuration kind: LocalConfig apiversion: odo.dev/v1alpha1 ComponentSettings: Type: nodejs SourceLocation: https://gitlab.com/ocp-demo/frontend-js SourceType: git Ports: - 8080/TCP Application: app Project: demo Name: frontend Deploy odo push Sample outout Validation ✓ Checking component [125ms] Configuration changes ✓ Initializing component ✓ Creating component [458ms] Applying URL changes ✓ URLs are synced with the cluster, no changes are required. Pushing to component backend of type binary ✓ Checking files for pushing [19ms] ✓ Waiting for component to start [2m] ✓ Syncing files to the component [2s] ✓ Building component [2s] Expose service ( create route) odo url create --port 8080 Sample outout ✓ URL backend-8080 created for component: backend To apply the URL configuration changes, please use `odo push` Remark: you need to run odo push to propagate change to OpenShift ✓ Checking component [150ms] Configuration changes ✓ Retrieving component data [213ms] ✓ Applying configuration [184ms] Applying URL changes ✓ URL backend-8080: http://backend-8080-app-backend-quarkus.apps.cluster-69f4.69f4.sandbox957.opentlc.com/ created Pushing to component backend of type binary ✓ Checking file changes for pushing [10ms] ✓ Waiting for component to start [41ms] ✓ Syncing files to the component [2s] ✓ Building component [3s] "},"helm.html":{"url":"helm.html","title":"Application Deployment with Helm","keywords":"","body":"Deploy Backend app with Helm Chart Deploy backend app using helm chart => backend-chart Test with dry run oc project project1 helm install --dry-run test ./manifests/backend-chart Install chart helm install backend-helm --namespace=project1 ./manifests/backend-chart Sample Output 1 ./manifests/backend-chart NAME: backend-helm LAST DEPLOYED: Tue Feb 16 17:32:46 2021 NAMESPACE: project1 STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: http://backend-helm:8080 Check Helm Chart in Developer Console Topology view Helm Chart details "},"imagestreams.html":{"url":"imagestreams.html","title":"Image Streams","keywords":"","body":"Image Stream Image Stream Automatic trigger deployment Image stream is an abstraction to container image in image registry. Image stream itself does not container any image it just a referece to actual image. You can configure builds and deployments to watch an image stream for notifications when new images are added and react by performing a build or deployment, respectively. Automatic trigger deployment import image with schedule update ( Default is every 15 minutes) oc import-image backend --scheduled --confirm --all --from quay.io/voravitl/backend oc get istag Setup image lookup for backend imagestream oc set image-lookup backend oc set image-lookup --list With image lookup is enabled. Imagestream name can be used in deployment spec: containers: - name: backend image: backend:v1 Check for latest update interval imagestream oc get istag backend:v1 Output NAME IMAGE REFERENCE UPDATED backend:v1 quay.io/voravitl/backend@sha256:19ef0afb88a1ce5d6a4422c7ab8395eb05b672fc27d5d387d9fcd8e15a44c5d7 30 seconds ago Deploy application oc apply -f backend.yaml Set trigger oc set triggers deployment/backend --from-image backend:v1 -c backend Trigger will set following annotation to deployment for container name backend metadata: name: backend annotations: image.openshift.io/triggers: '[{\"from\":{\"kind\":\"ImageStreamTag\",\"name\":\"backend:v1\"},\"fieldPath\":\"spec.template.spec.containers[?(@.name==\\\"backend\\\")].image\"}]' When image on image registry "},"openshift-route.html":{"url":"openshift-route.html","title":"OpenShift Route","keywords":"","body":"Deployment Strategy with OpenShift Route Deployment Strategy with OpenShift Route Application Deployment Blue/Green Deployment Canary Deployment Restrict TLS to v1.2 Test TLS/SSL mTLS Route Sharding Access Log Verify Access Log Sidecar Kibana Application Deployment Deploy 2 version of frontend app. Each deployment and service use label app and version for select each version. Initial Route will routing all traffic to v1. Deploy frontend v1 and v2 and create route frontend.yaml oc apply -f manifests/frontend.yaml -n project1 Blue/Green Deployment Test Route FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') while [ 1 ]; do curl -k $FRONTEND_URL/version echo sleep 1 done Use another terminal to patch route to frontend v2 oc patch route frontend -p '{\"spec\":{\"to\":{\"name\":\"frontend-v2\"}}}' -n project1 Check output from cURL that response is from frontend-v2 Set route back to v1 oc patch route frontend -p '{\"spec\":{\"to\":{\"name\":\"frontend-v1\"}}}' -n project1 Check output from cURL that response is from frontend-v1 Canary Deployment Apply route for Canary deployment to v1 and v2 with 80% and 20% ratio route-with-alternate-backend.yaml oc apply -f manifests/route-with-alternate-backend.yaml -n project1 Call frontend for 10 times. You will get 8 responses from v1 and 2 responses from v2 FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') COUNT=0 while [ $COUNT -lt 10 ]; do curl -k $FRONTEND_URL/version echo sleep .2 COUNT=$(expr $COUNT + 1) done Update weight to 60% and 40% oc patch route frontend -p '{\"spec\":{\"to\":{\"weight\":60}}}' -n project1 oc patch route frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/alternateBackends/0/weight\",\"value\":40}]' -n project1 Re-run previous bash script to loop frontend. This times you will get 6 responses from v1 and 4 responses from v2 Restrict TLS to v1.2 Check default ingresscontroller by run command or use OpenShift Web Admin Console oc edit ingresscontroller default -n openshift-ingress-operator Use Web Admin Console to search for ingressscontroller and select default Minimum TLS version can be specified by attribute minTLSVersion Also test with custom profile, edit tlsProfile: and click Save spec: replicas: 2 tlsSecurityProfile: type: Custom custom: ciphers: - ECDHE-ECDSA-AES128-GCM-SHA256 - ECDHE-RSA-AES128-GCM-SHA256 minTLSVersion: VersionTLS12 Test TLS/SSL To test TLS/SSL encryption enabled on OpenShift ingresscontroller, use https://testssl.sh/ testssl.ssh tool to run report for Ingress VIP support of TLS/SSL ciphers and protocols Run the test docker run --rm -ti drwetter/testssl.sh https://frontend-project1.apps.ocp01.example.com Sample results Default TLS profile Google Website mTLS WIP Route Sharding Access Log Router's access log can be enabled to syslog or sidecar container of router's pods. This can be done by add spec.logging.acess.destination.type to IngressController in openshift-ingress-operator namespace with syslog or Container respectively. Following set default IngressController with access log in container. oc patch IngressController default -n openshift-ingress-operator \\ -p '{\"spec\":{\"logging\":{\"access\":{\"destination\":{\"type\":\"Container\"}}}}}' --type=merge oc patch IngressController default -n openshift-ingress-operator \\ -p '{\"spec\":{\"logging\":{\"access\":{\"httpLogFormat\":\"%ci:%cp [%t] %ft %b/%s %B %bq %HM %HU %HV\"}}}}' --type=merge oc get pods -n openshift-ingress Check output that existing router pods are terminated and new router pods contains 2 containers NAME READY STATUS RESTARTS AGE router-default-64bb598c79-78lks 1/1 Running 0 18m router-default-64bb598c79-w4mgl 1/1 Terminating 0 18m router-default-66d57c45c8-2lsvq 2/2 Running 0 15s router-default-66d57c45c8-hh4n5 2/2 Running 0 15s Verify Access Log Sidecar View log from container logs ROUTER_POD=$(oc get pods -n openshift-ingress -o 'custom-columns=Name:.metadata.name' --no-headers | head -n 1) oc log -f $ROUTER_POD -n openshift-ingress -c logs Sample out acesslog .226:51394 [09/Mar/2022:02:49:20.812] fe_sni~ be_edge_http:project1:frontend/pod:frontend-v2-868959894b-mlld2:frontend:http:10.X.X.X:8080 532 0 GET / HTTP/1.1 Kibana Login to Kibana and filter for namespace openshift-logging and container name logs Sample of message "},"hpa.html":{"url":"hpa.html","title":"Horizontal Pod Autoscaler","keywords":"","body":"Horizontal Pod Autoscaler (HPA) Horizontal Pod Autoscaler (HPA) CPU Memory Custom Metrics Autoscaler Configure User Workload Monitoring Install Operator Create SacledObject Test CPU Deploy frontend app (if you still not deploy it yet)oc new-project project1 oc apply -f manifests/backend-v1.yaml -n project1 oc apply -f manifests/backend-service.yaml -n project1 oc apply -f manifests/backend-route.yaml -n project1 oc set env deployment backend-v1 -n project1 APP_BACKEND=https://mockbin.org/status/200 oc wait --for=condition=ready --timeout=60s pod -l app=backend BACKEND_URL=http://$(oc get route backend -n project1 -o jsonpath='{.spec.host}') curl -v -k $BACKEND_URL Review CPU HPA for deployment backend Scale out when average CPU utilization is greater than 20% of CPU limit Maximum pods is 5 Scale down to min replicas if utilization is lower than threshold for 60 sec minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 20 behavior: scaleDown: policies: - type: Pods value: 1 periodSeconds: 60 selectPolicy: Min stabilizationWindowSeconds: 70 scaleUp: policies: - type: Pods value: 5 periodSeconds: 70 selectPolicy: Max stabilizationWindowSeconds: 0 Create CPU HPA for deployment backend oc apply -f manifests/backend-cpu-hpa.yaml -n project1 Check HPA statuswatch oc get horizontalpodautoscaler/backend-cpu -n project1 Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE backend-cpu Deployment/backend /20% 1 5 0 2m4s Load test with K6 50 threads Duration 3 minutes Ramp up 30 sec Ramp down 30 sec URL=http://backend.project1:8080 oc run load-test -n project1 -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Output Check pods CPU in developer console Stop load test and wait 1 minutes to scale down to 1 replica. Remark: k6 will run as pod name load-test for 4 minutes if you want to force stop before 4 minutes just delete load-test pod oc delete pod load-test -n project1 Memory Deploy Memory Leak App resources: requests: cpu: \"0.1\" memory: 100Mi limits: cpu: \"0.2\" memory: 200Mi env: - name: APP_DISHSIZE value: \"10485760\" Deploy Leak App oc apply -f manifests/leak.yaml -n project1 oc expose deployment leak -n project1 oc expose svc leak -n project1 Check for leak app memory limit Review Memory HPA for deployment leak app Scale out when average memory utilization is greater than 100Mi Maximum pods is 3 Scale down to min replicas if utilization is lower than threshold for 60 sec Create Memory HPA for deployment frontend v1 oc apply -f manifests/leak-memory-hpa.yaml -n project1 Check HPA status watch oc get horizontalpodautoscaler/leak-memory -n project1 Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE leak-memory Deployment/leak 81829888/100Mi 1 3 1 5h37m Test Leak App to consume another 30 MB curl -v http://$(oc get route leak -n project1 -o jsonpath='{.spec.host}')/eat/3 watch oc get po -l app=leak -n project1 Output Check that another leak pod is created NAME READY STATUS RESTARTS AGE leak-74d885844d-s4vjz 1/1 Running 0 26s leak-74d885844d-vrx6s 1/1 Running 0 2m45s Check HPA status again NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE leak-memory Deployment/leak 127311872/100Mi 1 3 2 97s Check memory utilization from Developer console Custom Metrics Autoscaler Configure User Workload Monitoring Enable user workload monitoring Create Service Monitoring to monitor backend service oc apply -f manifests/backend-service-monitor.yaml -n project1 Remove HPA by CPU and run K6 with 10 threads oc delete hpa/backend-cpu -n project1 URL=http://backend.project1:8080 oc run load-test -n project1 -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Check for backends's concurrent requests/sec metric Open Developer Console -> Observe -> Metrics -> Custom metrics Input following PromQL to check for number of requests/min rate(http_server_requests_seconds_count{method=\"GET\",uri=\"root\",outcome=\"SUCCESS\"}[1m]) Output Install Operator Install Custom Metrics Autoscaler Operator Verify oc get csv -n openshift-keda oc get po -n openshift-keda Output NAME DISPLAY VERSION REPLACES PHASE custom-metrics-autoscaler.v2.10.1-253 Custom Metrics Autoscaler 2.10.1-253 Succeeded NAME READY STATUS RESTARTS AGE custom-metrics-autoscaler-operator-9c9c7c4cb-26l28 1/1 Running 0 10m Create SacledObject Create Service Account oc create serviceaccount thanos -n project1 Create TriggerAuthentication and assign cluster role to query thanos TOKEN=$(oc describe serviceaccount thanos -n project1|grep Token|awk -F':' '{print $2}'|awk '$1=$1') cat manifests/cma-trigger-authentication.yaml|sed 's/TOKEN/'$TOKEN'/'|sed 's/PROJECT/project1/' | oc create -f - oc adm policy add-cluster-role-to-user cluster-monitoring-view -z thanos -n project1 Create ScaledObject to scale backend pod by conncurrent request. oc apply -f manifests/backend-scaled-object.yaml -n project1 oc get scaledobject/backend -o yaml -n project1 | grep -A4 \" conditions:\" Number of replicas and triggers configuration triggers: - authenticationRef: kind: TriggerAuthentication name: keda-trigger-auth-prometheus metadata: metricName: http_server_requests_seconds_count namespace: project1 query: rate(http_server_requests_seconds_count{method=\"GET\",uri=\"root\",outcome=\"SUCCESS\"}[1m]) serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092 threshold: \"15\" authModes: \"bearer\" Output scaledobject.keda.sh/backend created conditions: - message: ScaledObject is defined correctly and is ready for scaling reason: ScaledObjectReady status: \"True\" type: Ready Test Run Load Test with K6 URL=http://$(oc get route backend -n project1 -o jsonpath='{.spec.host}') oc run load-test -n project1 -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Check that ScaledObject is active NAME SCALETARGETKIND SCALETARGETNAME MIN MAX TRIGGERS AUTHENTICATION READY ACTIVE FALLBACK AGE backend apps/v1.Deployment backend-v1 1 5 prometheus keda-trigger-auth-prometheus True True False 77m Check that backend is scaled out NAME READY STATUS RESTARTS AGE backend-v1-c448b55d9-5t2j6 1/1 Running 0 13s backend-v1-c448b55d9-5v9gm 1/1 Running 0 129m Check concurent request. Open Developer Console and use custom query sum by (pod)(rate(http_server_requests_seconds_count{method=\"GET\",uri=\"root\",outcome=\"SUCCESS\"}[1m])) "},"health.html":{"url":"health.html","title":"Health Check","keywords":"","body":"Pod Health Check Pod Health Check Prerequisite Readiness, Livenss and Startup Probe Configure and Test Probes Command Line Developer Console Prerequisite Deploy frontend app (if you still not deploy it yet) oc apply -f manifests/frontend.yaml -n project1 oc delete deployment frontend-v2 -n project1 Readiness, Livenss and Startup Probe Kubernetes provide 3 types of probe to check pod's health Readiness, check that pod is ready for process request or not. If success, service will allow traffic to this pod. Liveness, check that pod is dead or alive. Pod will be restarted if liveness probe is failed. Startup, for dealing with long startup time pod. Set probe with same as liveness with a duration to failureThreshold * periodSecond. Liveness probe will takeover Startup probe after startup is success. Frontend application provides health check with following URI URI Description /health/live Livenness probe URL /health/ready Readiness probe URL For demo purpose we can set readiness and liveness by following URI URI Description /stop Set liveness to false /start Set liveness to true /not_ready Set readiness to false /ready Set readiness to true Test frontend app readiness Connect to podPOD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc -n project1 rsh $POD Test frontend appcurl http://localhost:8080/ Test readiness probecurl http://localhost:8080/health/ready Set pod to not ready state curl http://localhost:8080/not_ready Test frontend readiness probe and test app again. You will get 503 Service Unavailable response code.curl -v http://localhost:8080/health/ready curl -v http://localhost:8080/ Set pod to ready statecurl http://localhost:8080/ready Configure and Test Probes Command Line Test frontend app liveness Connect to podPOD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc -n project1 rsh $POD Test frontend appcurl http://localhost:8080/ Test liveness probecurl http://localhost:8080/health/live Set pod to not ready state curl http://localhost:8080/stop Test frontend liveness probe and test app again. You will get 503 Service Unavailable response code.curl -v http://localhost:8080/health/live curl -v http://localhost:8080/ Set pod to ready statecurl http://localhost:8080/start Configure Readiness, Liveness and Startup probe oc rollout pause deployment/frontend-v1 -n project1 oc set probe deployment/frontend-v1 --readiness --get-url=http://:8080/health/ready --initial-delay-seconds=8 --failure-threshold=1 --period-seconds=3 --timeout-seconds=5 -n project1 oc set probe deployment/frontend-v1 --liveness --get-url=http://:8080/health/live --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=10 --timeout-seconds=5 -n project1 oc set probe deployment/frontend-v1 --startup --get-url=http://:8080/health/live --initial-delay-seconds=5 --period-seconds=10 -n project1 oc rollout resume deployment/frontend-v1 -n project1 watch oc get pods -n project1 Check pod status. New pod is created and previous pod is terminated. NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-rhzwt 0/1 ContainerCreating 0 3s frontend-v1-c5d4648f9-fkc84 1/1 Running 0 49s Scale frontend-v1 to 3 pods oc scale deployment/frontend-v1 --replicas=3 Test Liveness Probe Test live probe by set one frontend pod to return 503 for liveness probe POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/stop printf \"\\n%s is dead\\n\" $POD Check pod's events oc describe pod $POD -n project1 Sample output Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m57s default-scheduler Successfully assigned project1/frontend-v1-5d8c4ccc8c-j9b6d to ip-10-0-148-247.ap-southeast-1.compute.internal Normal AddedInterface 3m55s multus Add eth0 [10.131.0.112/23] Normal Pulling 3m54s kubelet Pulling image \"quay.io/voravitl/frontend-js:v1\" Normal Pulled 3m51s kubelet Successfully pulled image \"quay.io/voravitl/frontend-js:v1\" in 2.988138314s Normal Created 3m51s kubelet Created container frontend Normal Started 3m51s kubelet Started container frontend Warning Unhealthy 24s kubelet Liveness probe failed: HTTP probe failed with statuscode: 503 Normal Killing 24s kubelet Container frontend failed liveness probe, will be restarted Check that pod is restarted oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-j9b6d 1/1 Running 1 5m28s frontend-v1-5d8c4ccc8c-lx4xb 1/1 Running 0 4m3s frontend-v1-5d8c4ccc8c-qvxwp 1/1 Running 0 4m3s Test Readiness Probe Test live probe by set one frontend pod to return 503 for liveness probe POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/not_ready printf \"\\n%s is not ready\\n\" $POD Check pod status oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-j9b6d 0/1 Running 1 5m28s frontend-v1-5d8c4ccc8c-lx4xb 1/1 Running 0 4m3s frontend-v1-5d8c4ccc8c-qvxwp 1/1 Running 0 4m3s Check pod's events oc describe pod/$POD -n project1 Sample Output Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m57s default-scheduler Successfully assigned project1/frontend-v1-5d8c4ccc8c-j9b6d to ip-10-0-148-247.ap-southeast-1.compute.internal Normal AddedInterface 3m55s multus Add eth0 [10.131.0.112/23] Normal Pulling 3m54s kubelet Pulling image \"quay.io/voravitl/frontend-js:v1\" Normal Pulled 3m51s kubelet Successfully pulled image \"quay.io/voravitl/frontend-js:v1\" in 2.988138314s Normal Created 3m51s kubelet Created container frontend Normal Started 3m51s kubelet Started container frontend Warning Unhealthy 24s kubelet Liveness probe failed: HTTP probe failed with statuscode: 503 Normal Killing 24s kubelet Container frontend failed liveness probe, will be restarted Check that pod is removed from service oc describe svc/frontend-v1 -n project1 Test readiness probe. Notice that all responses will not come from not ready pod. while [ 1 ]; do curl -k https://$(oc get route/frontend -n project1 -o jsonpath='{.spec.host}') printf \"\\n\" sleep 10 done Use another terminal to set not ready pod back to ready. Notice that response now including all 3 pods. POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/ready printf \"\\n%s is ready\\n\" $POD Check developer console Developer Console Remove probes from previous stepsoc set probe deployment/frontend-v1 --remove --readiness --liveness --startup -n project1 watch oc get pods -n project1 Login to OpenShift Web Admin Console and change to Developer Console Select topology then select frontend-v1 deployment, then select \"Add Health Checks\" Add health checks "},"kustomize.html":{"url":"kustomize.html","title":"Kustomize","keywords":"","body":"Kustomize Todo Applicaiton Sample todo applicaition Quarkus Native PostgreSQL Datababase ├── base │ ├── kustomization.yaml │ ├── namespace.yaml │ ├── service-monitor.yaml │ ├── todo-db.yaml │ └── todo.yaml └── overlays └── dev ├── kustomization.yaml └── todo.yaml Create Todo Application with overlays dev oc apply -k manifests/todo-kustomize/overlays/dev Frontend/Backend Applicaiton Sample Frontend/Backend applicaition . ├── base │ ├── backend-service.yaml │ ├── backend.yaml │ ├── demo-rolebinding.yaml │ ├── frontend-service.yaml │ ├── frontend.yaml │ ├── kustomization.yaml │ ├── namespace.yaml │ └── route.yaml └── overlays ├── dev │ ├── backend.yaml │ ├── frontend.yaml │ └── kustomization.yaml └── prod ├── backend.yaml ├── frontend.yaml └── kustomization.yaml Create Todo Application with overlays dev oc apply -k manifests/apps-kustomize/overlays/dev "},"application-metrics.html":{"url":"application-metrics.html","title":"User Workload Monitoring","keywords":"","body":"User Workload Monitoring User Workload Monitoring Prerequisites Service Monitoring K6 Custom Grafana Dashboard Custom Alert Prerequisites Setup User Workload Monitoring DEFAULT_STORAGE_CLASS=$(oc get sc -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}') cat manifests/cluster-monitoring-config.yaml | sed 's/storageClassName:.*/storageClassName: '$DEFAULT_STORAGE_CLASS'/' | oc apply -f - sleep 10 oc -n openshift-user-workload-monitoring wait --for condition=ready \\ --timeout=180s pod -l app.kubernetes.io/name=prometheus oc -n openshift-user-workload-monitoring wait --for condition=ready \\ --timeout=180s pod -l app.kubernetes.io/name=thanos-ruler oc get pvc -n openshift-monitoring Output configmap/cluster-monitoring-config created pod/prometheus-user-workload-0 condition met pod/prometheus-user-workload-1 condition met pod/thanos-ruler-user-workload-0 condition met pod/thanos-ruler-user-workload-1 condition met NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE prometheus-k8s-db-prometheus-k8s-0 Bound pvc-c1513ebb-6fcf-4136-b3a2-41a7a3afd83b 50Gi RWO ocs-external-storagecluster-ceph-rbd 21s prometheus-k8s-db-prometheus-k8s-1 Bound pvc-f9f9d7b1-3303-41ed-9549-43fab596cebe 50Gi RWO ocs-external-storagecluster-ceph-rbd 21s Verify monitoring stack oc get pod -n openshift-user-workload-monitoring Sample output NAME READY STATUS RESTARTS AGE prometheus-operator-787b6f6d75-lrgjg 2/2 Running 0 70s prometheus-user-workload-0 6/6 Running 0 63s prometheus-user-workload-1 6/6 Running 0 63s thanos-ruler-user-workload-0 4/4 Running 0 62s thanos-ruler-user-workload-1 4/4 Running 0 62s CPU and Memory used by User Workload Monitoring Overall resouces consumed by user workload monitoring CPU Usage Memory Usage CPU Quota Memory Quota Service Monitoring Deploy frontend/backend app with custom metrics oc create -f manifests/frontend-v1-and-backend-v1-JVM.yaml -n project1 Call frontend app with curl curl https://$(oc get route frontend -o jsonpath='{.spec.host}' -n project1) Output Frontend version: v1 => [Backend: http://backend:8080, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-5587465b8d-9kvsc, Status:200, Message: Hello, World] Check for backend's metrics Check JVM heap size oc exec -n project1 $(oc get pods -l app=backend \\ --no-headers -o custom-columns='Name:.metadata.name' \\ -n project1 | head -n 1 ) \\ -- curl -s http://localhost:8080/q/metrics | grep heap Sample output jvm_memory_used_bytes{area=\"heap\",id=\"Tenured Gen\"} 1.1301808E7 jvm_memory_used_bytes{area=\"nonheap\",id=\"CodeHeap 'profiled nmethods'\"} 5261952.0 jvm_memory_used_bytes{area=\"heap\",id=\"Eden Space\"} 2506768.0 jvm_memory_used_bytes{area=\"nonheap\",id=\"Metaspace\"} 3.3255E7 jvm_memory_used_bytes{area=\"nonheap\",id=\"CodeHeap 'non-nmethods'\"} 1389824.0 jvm_memory_used_bytes{area=\"heap\",id=\"Survivor Space\"} 524288.0 jvm_memory_used_bytes{area=\"nonheap\",id=\"Compressed Class Space\"} 4096808.0 jvm_memory_used_bytes{area=\"nonheap\",id=\"CodeHeap 'non-profiled nmethods'\"} 1120640.0 ... Check for backend application related metrics curl https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') oc exec -n project1 $(oc get pods -l app=backend \\ --no-headers -o custom-columns='Name:.metadata.name' \\ -n project1 | head -n 1 ) \\ -- curl -s http://localhost:8080/q/metrics | grep http_server_requests_seconds Check that http_server_requests_seconds_count with method GET and root URI value is 1 with return status 200 # HELP http_server_requests_seconds http_server_requests_seconds_count{method=\"GET\",outcome=\"SUCCESS\",status=\"200\",uri=\"root\"} 2.0 http_server_requests_seconds_sum{method=\"GET\",outcome=\"SUCCESS\",status=\"200\",uri=\"root\"} 3.769968656 # TYPE http_server_requests_seconds_max gauge # HELP http_server_requests_seconds_max http_server_requests_seconds_max{method=\"GET\",outcome=\"SUCCESS\",status=\"200\",uri=\"root\"} 3.486753482 Create Service Monitoring to monitor backend service apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: backend-monitor spec: endpoints: - interval: 30s port: http path: /q/metrics # Get metrics from URI /q/metrics scheme: http selector: matchLabels: app: backend # select only label app = backend Create service monitor oc apply -f manifests/backend-service-monitor.yaml -n project1 Remark: Role monitor-edit is required for create ServiceMonitor and PodMonitor resources. Following example is granting role montior-edit to user1 for project1 oc adm policy add-role-to-user monitoring-edit user1 -n project1 Developer Console monitoring metrics Select application metrics Application metrics Scale backend-v1 and frontend-v1 to 5 pod and run load test tool oc scale deployment backend-v1 --replicas=5 -n project1 oc scale deployment frontend-v1 --replicas=5 -n project1 Use K6 oc run load-test-frontend -n project1 \\ -i --image=loadimpact/k6 \\ --rm=true --restart=Never -- run - Use Siege oc create -f manifests/tools.yaml -n project1 TOOL=$(oc get po -n project1 -l app=network-tools --no-headers -o custom-columns='Name:.metadata.name') Run siege command oc exec -n project1 $TOOL -- siege -c 20 -t 4m http://frontend:8080 PromQL for query request/sec for GET method to root URI of backend service rate(http_server_requests_seconds_count{method=\"GET\",uri=\"root\"}[1m]) Sample stacked graph K6 This is optional steps. If you have k6 on your manchine k6 run manifests/load-test-k6.js \\ --out json=output.json \\ --summary-export=summary-export.json \\ -e URL=https://$(oc get route frontend -o jsonpath='{.spec.host}' -n project1) \\ -e THREADS=10 \\ -e RAMPUP=30s \\ -e DURATION=3m \\ -e RAMPDOWN=30s \\ -e K6_NO_CONNECTION_REUSE=true Remark: output are stored in output.json and summary-export.json Run K6 with dashboard (You need to install K6 dashbaord first) k6 run manifests/load-test-k6.js \\ --out dashboard \\ -e URL=https://$(oc get route frontend -o jsonpath='{.spec.host}' -n project1) \\ -e THREADS=10 \\ -e RAMPUP=30s \\ -e DURATION=3m \\ -e RAMPDOWN=30s \\ -e K6_NO_CONNECTION_REUSE=true View Dashboard Custom Grafana Dashboard Install Grafana Operator to project application-monitor or using CLI oc create -f manifests/grafana-sub.yaml Verify oc get csv -n openshift-operators Output NAME DISPLAY VERSION REPLACES PHASE grafana-operator.v5.6.2 Grafana Operator 5.6.2 grafana-operator.v5.6.1 Succeeded Use Grafana Operator by Red Hat to deploy Grafana and configure datasource to Thanos Querier Remark: Grafana Operator is Community Edition - not supported by Red Hat Create project oc new-project application-monitor --display-name=\"App Dashboard\" --description=\"Grafana Dashboard for Application Monitoring\" Create Grafana instance oc create -f manifests/grafana.yaml -n application-monitor oc create route edge grafana --service=grafana-service --port=3000 -n application-monitor watch -d oc get pods -n application-monitor Sample Output NAME READY STATUS RESTARTS AGE grafana-deployment-96d5f5954-5hml7 1/1 Running 0 14s Add cluster role cluster-monitoring-view to Grafana ServiceAccount oc adm policy add-cluster-role-to-user cluster-monitoring-view \\ -z grafana-sa -n application-monitor Create Grafana DataSource with serviceaccount grafana-serviceaccount's token and connect to thanos-querier TOKEN=$(oc create token grafana-sa --duration=4294967296s -n application-monitor) cat manifests/grafana-datasource.yaml|sed 's/Bearer .*/Bearer '\"$TOKEN\"\"'\"'/'|oc apply -n application-monitor -f - Create Grafana Dashboard oc apply -f manifests/grafana-dashboard.yaml -n application-monitor Login to Grafana Dashboard with following URL echo \"Grafana URL => https://$(oc get route grafana -o jsonpath='{.spec.host}' -n application-monitor)\" or use link provided by Developer Console Login with user admin and password openshift Generate workload bash script to loop request to frontend application. FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') while [ 1 ]; do curl $FRONTEND_URL printf \"\\n\" sleep .2 done k6 load test tool with 10 threads for 5 minutes oc run load-test-frontend -n project1 \\ -i --image=loadimpact/k6 \\ --rm=true --restart=Never -- run - Grafana Dashboard Custom Alert Check PrometheusRule backend-app-alert is consists with 2 following alerts: ConcurrentBackend severity warning when total concurrent reqeusts is greater than 40 requests/sec HighLatency servierity critical when percentile 90th of response time is greater than 5 sec Create backend-app-alert oc apply -f manifests/backend-custom-alert.yaml Remark: Role monitoring-rules-view is required for view PrometheusRule resource and role monitoring-rules-edit is required to create, modify, and deleting PrometheusRule Following example is granting role monitoring-rules-view and monitoring-rules-edit to user1 for project1 oc adm policy add-role-to-user monitoring-rules-view user1 -n project1 oc adm policy add-role-to-user monitoring-rules-edit user1 -n project1 Test ConcurrentBackend alert with 25 concurrent requests oc run load-test-frontend -n project1 \\ -i --image=loadimpact/k6 \\ --rm=true --restart=Never -- run - Check for alert in Developer Console by select Menu Monitoring then select Alerts Check for alert in Developer Console Console overview status sum(increase(http_server_requests_seconds_count{outcome=\"SUCCESS\"}[5m]))/sum(increase(http_server_requests_seconds_count[5m]))*100 "},"otel-and-tempo.html":{"url":"otel-and-tempo.html","title":"OpenTelemetry with Tempo","keywords":"","body":"OpenTelemetry and Tempo Tracing with OpenTelemtry and Tempo "},"ci-cd-with-jenkins.html":{"url":"ci-cd-with-jenkins.html","title":"CI/CD with Jenkins","keywords":"","body":"CI/CD with Jenkins CI/CD with Jenkins Overall Solution Build and Deploy to Development Environment Deploy Staging Environment Deploy UAT Environment Deploy Production Environment Setup Projects Jenkins, SonarQube and Nexus Overall Solution Jenkins pipelines to demonstrate CI/CD process to build Quarkus application from source code to container image with version control by tag name and deploy application to Development, Staging, UAT and blue/green deployment to Production environment. Remark: Source code of Quarkus and Jenkins Build and Deploy to Development Environment Build fast-jar Quarkus application Pull dependencies from Nexus Run unit test with JUnit and code quality with SonarQube Push container image to Nexus or internal registry Create service, route and deploymentconfig in dev project Deploy Staging Environment Select version to deploy to stage project Tag container image with version-DDMMYYYY-round Tear down and deploy application to stage project Deploy UAT Environment Select version to deploy to uat project. Only images with tag version-DDMMYYYY-round will be avaiable in list to deploy Tear down and deploy application to uat project Deploy Production Environment Select version to deploy to prod project. Only images with tag version-DDMMYYYY-round will be avaiable in list to deploy Create deploymentconfig and service for blue and green version Create route Select version to deploy and scale down previous version Setup Projects Create 4 projects ci-cd, dev, stage, uat and prod CI_CD=ci-cd DEV=dev STAGE=stage UAT=uat PROD=prod oc new-project $DEV --display-name=\"Development Environment\" oc new-project $STAGE --display-name=\"Staging Environment\" oc new-project $UAT --display-name=\"User Acceptance Test Environment\" oc new-project $PROD --display-name=\"Production Environment\" oc new-project $CI_CD --display-name=\"CI/CD Tools\" Allow jenkins service account to managed dev, stage, uat and prod oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${DEV} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${STAGE} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${UAT} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${PROD} Allow dev, stage, uat and prod to pull image from ci-cd project (in case use internal image registry instead of Nexus) oc policy add-role-to-group system:image-puller system:serviceaccounts:${DEV} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${STAGE} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${UAT} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${PROD} -n ${CI_CD} Remark: You can use bash script setup_projects.sh for all above steps. Jenkins, SonarQube and Nexus Setup Run bash scripts to setup Jenkins, SonarQube and Nexus cd bin ./setup_nexus.sh ./setup_jenkins.sh ./setup_sonar.sh Sample output Check Developer Console Login to Nexus with user admin with password from the 1st line nexus_password.txt. You can change password to whatever you want Check for Nexus's repositories Jenkins will use user and password stored in secret nexus-credential Check for nexus-credential oc describe secret/nexus-credential -n ci-cd Sample output Name: nexus-credential Namespace: ci-cd Labels: Annotations: Type: Opaque Data ==== password: 48 bytes username: 8 bytes Jenkins Pipelines Create pipelines oc apply -f manifests/backend-build-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-uat-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-prod-pipeline.yaml -n ci-cd Control pipeline to use internal registry or Nexus by pipeline's parameter USE_INTERNAL_REGISTRY env: - name: DEV_PROJECT value: dev - name: CICD_PROJECT value: ci-cd - name: BACKEND_URL value: https://httpbin.org/status/200 - name: NEXUS_SVC value: http://nexus.ci-cd.svc.cluster.local:8081 - name: NEXUS_REGISTRY_SVC value: nexus-registry.ci-cd.svc.cluster.local:5000 - name: NEXUS_REGISTRY value: nexus-registry-ci-cd.apps.cluster-a987.a987.example.opentlc.com - name: SONARQUBE_SVC value: http://sonarqube:9000 - name: NEXUS_SECRET value: nexus-credential - name: USE_INTERNAL_REGISTRY value: \"false\" Jenkins Remote API Create token Check for jenkins's user ID Configure pipeline Trigger builds Test USERID=opentlc-mgr-admin-edit-view TOKEN=117d9459d809be344f1823cbc1248fba09 JENKINS_URL=https://jenkins-ci-cd.apps.cluster-1516.1516.example.opentlc.com curl -X POST -L -v --user $USERID:$TOKEN \"$JENKINS_URL/job/ci-cd/job/ci-cd-backend-build-pipeline/buildWithParameters?token=jira&NEXUS_REGISTRY_SVC=nexus-registry.ci-cd.svc.cluster.local:5000&NEXUS_REGISTRY=nexus-registry-ci-cd.apps.cluster-a987.a987.example.opentlc.com\" Checkpoints Maven build in pipeline pull dependencies from nexus Code snippets environment { mvnCmd = \"mvn -s ./nexus_settings.xml \" ... ... } ... script { sh \"${mvnCmd} -Dquarkus.package.type=fast-jar -Dinternal.repo.username=${nexusUser} -Dinternal.repo.password=${nexusPassword} -DskipTests=true clean package\" } Nexus's repository SonarQube code quality checking Code snippets script { sh \"${mvnCmd} sonar:sonar -Dinternal.repo.username=${nexusUser} -Dinternal.repo.password=${nexusPassword} -Dsonar.host.url=${env.SONARQUBE_SVC} -Dsonar.projectName=${imageName}-${devTag} -Dsonar.projectVersion=${devTag}\" } Scan result Container images is built and pushed to Nexus Code snippets openshift.withCluster() { openshift.withProject(env.CICD_PROJECT) { openshift.newBuild( \"--name=${imageName}\", \"--to=${nexus_url}/${imageName}:latest\", \"--to-docker=true\", \"--push-secret=nexus-registry\", \"--strategy=docker\", \"--binary=true\" ) } } Nexus image registry Application is deployed with label version and tag "},"ci-cd.html":{"url":"ci-cd.html","title":"CI/CD with Azure DevOps","keywords":"","body":"CI/CD with Azure DevOps CI/CD Prerequisites Azure DevOps Deploy Back App Deploy Front App Prepare Harbor On Kubernetes/OpenShift Prepare Azure DevOps Service Connection Azure pipelines Prerequisites Openshift 4.6 Cluster Oepnshift User with Admin Permission Azure DevOps Project Harbor Container Registry Postman / K6 for Test Azure DevOps Azure Devops Project https://dev.azure.com/user/project Azure Repo user: chatapazar PAT: xxx demo.Front Repository: https://user@dev.azure.com/user/demo.Front/_git/demo.Front demo.Back Repository : https://user@dev.azure.com/user/demo.Front/_git/demo.Back Azure Artifact create new feed name: my-very-private-feed leave all default Deploy Back App deploy source code from azure repo with openshift s2i login, new project call 'test' oc login oc new-project test prepare secret for azure repo oc create secret generic azure-repo --from-literal=username=chatapazar --from-literal=password=xxx --type=kubernetes.io/basic-auth oc secrets link builder azure-repo deploy back app back.yaml oc create -f back.yaml oc expose svc/back Deploy Front App set current project to test oc project test create secret for harbor oc create secret docker-registry myharbor --docker-username=chatapazar --docker-server=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com --docker-password=xxx oc secrets link default myharbor --for=pull --namespace=test For private Container Registry and Self Sign Cert, if use CA go to create imagestream get ca.crt with openssl openssl s_client -connect ocr.apps.cluster-b3e9.b3e9.example.opentlc.com:443 -showcerts /dev/null|openssl x509 -outform PEM > ca.crt or get cert with firefox (ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/v2 --> select both PEM & PEM chain) create configmap and add trust ca to openshift (both PEM & PEM chain oc create configmap harbor-registry --from-file=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com=ca1.crt -n openshift-config oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"harbor-registry\"}}}' --type=merge oc create configmap registry-config --from-file=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com=ca.crt -n openshift-config oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge create imagestream oc import-image test/front-blue:latest --from=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 --confirm oc import-image test/front-green:latest --from=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 --confirm update imagestream, if you need change version of image in openshift oc tag ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 test/front-blue:latest oc tag ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 test/front-green:latest deploy front-blue, front-green and expose route to front-blue front-blue.yaml, front-green.yaml oc create -f front-blue.yaml oc create -f front-green.yaml oc patch dc front-green -p \"{\\\"spec\\\":{\\\"replicas\\\":0}}\" -n test oc expose service front-blue -l name=front --name=front Environment of front app, can change in front-blue.yaml and front-green.yaml ITERATION_COUNT=1000 BACKEND_URL=http://back:8080/api/values/back ASPNETCORE_URLS=http://*:8080 Canary Deployment create imagestream for canary oc import-image test/front-main:latest --from=ocr.apps.cluster-852b.852b.example.opentlc.com/test/testdemoapp.front:20210105.5 --confirm oc import-image test/front-sub:latest --from=ocr.apps.cluster-852b.852b.example.opentlc.com/test/testdemoapp.front:20210105.5 --confirm create front-main dc oc project test oc create -f front-main.yaml create front-sub dc oc create -f front-sub.yaml create route canary oc create -f canary.yaml test canary manual run release canary in azure devops curl http://canary-test.apps.cluster-852b.852b.example.opentlc.com/api/values/information Prepare Harbor On Kubernetes/OpenShift create new project 'harbor' oc new-project harbor oc adm policy add-scc-to-group anyuid system:authenticated install harbor with helm: https://computingforgeeks.com/install-harbor-image-registry-on-kubernetes-openshift-with-helm-chart/ helm install harbor harbor/harbor \\ --set persistence.persistentVolumeClaim.registry.size=10Gi \\ --set persistence.persistentVolumeClaim.chartmuseum.size=5Gi \\ --set persistence.persistentVolumeClaim.database.size=5Gi \\ --set externalURL=https://ocr.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set expose.ingress.hosts.core=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set expose.ingress.hosts.notary=notary.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set harborAdminPassword=H@rb0rAdm \\ -n harbor change externalURL to https://ocr.{openshift-clustername} change expose.ingress.hosts.core to ocr.{openshift-clustername} change expose.ingress.hosts.notary to notary.{openshift-clustername} create project test create user for access harbor Prepare Azure DevOps Service Connection Service Connection: openshift select new service connection, select type Openshift Authentication method: Token Based Authentication Server URL: such as https://api.cluster-b3e9.b3e9.example.opentlc.com:6443 accept untrusted SSL: checked api token: such as sha256~fF0TCW0az6FMJ6232dJAxdhX4lqZo-bkYdbfFKwv_Zw service connection name: openshift grant access permission to all pipelines: checked Service Connection: harbor select new service connection, select type docker registry registry type: Others Docker Registry: such as https://ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/ Docker ID: harbor user Docker Password: harbor password service connection name: harbor grant access permission to all pipelines: checked Service Connection: fortify select new service connection, select type fortify authen mode: basic authen api url: https://api.trial.fortify.com portal url: https://trial.fortify.com username: chatapazar@gmail.com PAT: xxx Tenant ID: xxx connection name: fortify Azure pipelines Pipelines: sample-pipeline.yml, sample-pipeline-redhat-image.yml current step in ci or pipeline install .net sdk 2.2 for test project (app use 2.1, test use 2.2 ???) restore package/library with azure artifacts build unit test --> publish to Azure DevOps code coverage with cobertura --> publish to Azure DevOps publish Option: scan code with fortify (use fortify on demand, don't have fortify scs license file) Option: login registry.redhat.io for pull ubi8/dotnet-21-runtime --> sample-pipeline-redhat-image.yml build image install trivy, scan image with trivy, publish resutl to Azure DevOps (test) harbor login, with self sign of harbor, need copy ca.crt to docker (such as /etc/docker/certs.d/ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/ca.crt ) in Azure DevOps agent and manual login, recommended use CA in Prod push image to harbor Releases: blue-green.json, canary.json trigger from ci/pipeline or manual stage 1: switch to new version setup oc command check current deployment and destination deployment switch from blue to green or green to blue switch route to new version stage 2: scale down previous version can add approval for confirm setup oc command scale down previous version Test with postman script of Test Canary Deployment change in front route with yaml spec: host: front-test.apps.cluster-852b.852b.example.opentlc.com to: kind: Service name: front-blue weight: 90 alternateBackends: - kind: Service name: front-green weight: 10 port: targetPort: 8080-tcp wildcardPolicy: None or oc pathc oc patch route frontend -p '{\"spec\":{\"to\":{\"weight\":60}}}' -n project1 oc patch route frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/alternateBackends/0/weight\",\"value\":40}]' -n project1 Use Openshift Internal Registry oc create imagestream demofront -n test oc create imagestream demoapp -n test oc login with plugin docker login -u opentlc-mgr -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/test/demofront:xxx docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/test/demoapp:xxx oc tag test/demofront:xxx test/demofront:latest oc tag test/demoapp:xxx test/demoapp:latest create imagestream login openshift login docker to openshift internal registry with token push image tag image stream Step Demo preset architecture prod vs demo preset ocp console / usage / login / user management preset harbor / usage / project / user management / scan manual/auto / set cve block pull / set cve whitelist / set auto scan on push present pipeline / release scan code with fortify scan image in pipeline , change show image from red hat , run with red hat image blue green / deploy canary --> example --> see again in service mesh , network deploy section detail of openshift route deployment streategy openshift internal registry add permission to user for internal registryoc adm policy add-role-to-user system:registry -n oc adm policy add-role-to-user system:image-builder -n ## or for cluster-wide access... oc adm policy add-cluster-role-to-user system:registry oc adm policy add-cluster-role-to-user system:image-builder exampleoc login (with user1) oc new-project user1 oc login (with user2) oc new-project user2 oc login (with admin) oc adm policy add-role-to-user system:registry user1 -n user1 oc adm policy add-role-to-user system:image-builder user2 -n user2 test rbac internal registryoc login (with user1) docker login -u user1 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull hello-world docker tag hello-world:latest default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view imagestream in openshift project user1 docker rmi -f $(docker images -a -q) docker images docker logout default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com oc login (with user2) docker login -u user2 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view result error docker logout default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com oc login (with user1) docker login -u user1 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view result success prune image https://docs.openshift.com/container-platform/4.4/applications/pruning-objects.html#pruning-images_pruning-objects "},"eap-on-ocp.html":{"url":"eap-on-ocp.html","title":"EAP on OpenShift","keywords":"","body":"JBoss EAP on OpenShift JBoss EAP on OpenShift Binary Build and Deployment Configure Standalone Cluster Binary Build and Deployment Clone verify cluster app Pacakge application (WAR) and copy at your local directory mvn clean package mkdir -p deployments cp target/verify-cluster.war deployments Create build config with EAP image stream and use local WAR file Initial environment variables APP_NAME=verify-cluster PROJECT=$(oc project -q) IMAGE_STREAM=jboss-eap73-openshift:7.3 REPLICAS=2 VERSION=1.0.0 BINARY_PATH=deployments/verify-cluster.war Build EAP container image with deployed application Create build config oc new-build --binary=true \\ --name=${APP_NAME} -l app=${APP_NAME} --image-stream=${IMAGE_STREAM} Start build oc start-build ${APP_NAME} \\ --from-file=${BINARY_PATH} \\ --follow Tag image with version oc tag ${APP_NAME}:latest ${APP_NAME}:${VERSION} Check build config oc get buildconfig/verify-cluster output NAME TYPE FROM LATEST verify-cluster Source Binary 1 Create Deoloyment Start build and deploy with oc new-app oc new-app ${APP_NAME}:${VERSION} \\ --labels=app=${APP_NAME},deploymentconfig=${APP_NAME},version=${VERSION},app.kubernetes.io/name=jboss Check pods oc get pods -l app=$APP_NAME Output NAME READY STATUS RESTARTS AGE verify-cluster-565fbb6f86-jt5xl 1/1 Running 0 4m42s Check on Development Console Pods Resources Create route oc expose svc/${APP_NAME} echo \"URL: http://$(oc get route ${APP_NAME} -n ${PROJECT} -o jsonpath='{.spec.host}')/verify-cluster\" Test access verify-cluster and press Increment. Notice pod name and number of hits will be increased Configure Standalone Cluster Service Account default needs view role to run KUBE_PING oc adm policy add-cluster-role-to-user \\ view system:serviceaccount:$PROJECT:default -n $PROJECT Pause rollout oc rollout pause deployment ${APP_NAME} Configure JGROUP oc set env deployment ${APP_NAME} \\ JGROUPS_PING_PROTOCOL=kubernetes.KUBE_PING \\ KUBERNETES_NAMESPACE=${PROJECT} \\ KUBERNETES_LABELS=app=${APP_NAME},version=${VERSION} oc set env deployment ${APP_NAME} DISABLE_EMBEDDED_JMS_BROKER=true Scale replicas oc scale deployment ${APP_NAME} --replicas=${REPLICAS} Resume rollout and check pods oc rollout resume deployment ${APP_NAME} oc get pods -l app=$APP_NAME,version=$VERSION Output NAME READY STATUS RESTARTS AGE verify-cluster-75584cb799-rcf5t 1/1 Terminating 0 52s verify-cluster-789f5dd9d-qmg9s 1/1 Running 0 34s verify-cluster-789f5dd9d-s4q8z 1/1 Running 0 38s Check log oc logs $(oc get pods -l app=$APP_NAME,version=$VERSION | tail -n 1 | awk '{print $1}') Test that sessions is replicated Access verify-cluster app Increment counter Delete pod that response your request oc delete pods Refresh page again and check that counter still continue. binary_build.sh can be used to automated all steps. --list oc set env dc/datagrid-app MYCACHE_CACHE_START=LAZY ``` ## EAP 7.3 - Update Imagestreams - boss-eap73-openshift - jboss-eap73-runtime-openshift - jboss-eap73-openjdk11-openshift - jboss-eap73-openjdk11-runtime-openshift ```bash for resource in \\ eap73-amq-persistent-s2i.json \\ eap73-amq-s2i.json \\ eap73-basic-s2i.json \\ eap73-https-s2i.json \\ eap73-image-stream.json \\ eap73-sso-s2i.json \\ eap73-starter-s2i.json \\ eap73-third-party-db-s2i.json \\ eap73-tx-recovery-s2i.json \\ eap73-openjdk11-amq-persistent-s2i.json \\ eap73-openjdk11-amq-s2i.json \\ eap73-openjdk11-basic-s2i.json \\ eap73-openjdk11-https-s2i.json \\ eap73-openjdk11-image-stream.json \\ eap73-openjdk11-sso-s2i.json \\ eap73-openjdk11-starter-s2i.json \\ eap73-openjdk11-third-party-db-s2i.json \\ eap73-openjdk11-tx-recovery-s2i.json do oc replace -n openshift --force -f \\ https://raw.githubusercontent.com/jboss-container-images/jboss-eap-7-openshift-image/eap73/templates/${resource} done ``` ## EAP 7.2 ```bash for resource in \\ eap72-amq-persistent-s2i.json \\ eap72-amq-s2i.json \\ eap72-basic-s2i.json \\ eap72-https-s2i.json \\ eap72-image-stream.json \\ eap72-sso-s2i.json \\ eap72-starter-s2i.json \\ eap72-third-party-db-s2i.json \\ eap72-tx-recovery-s2i.json do oc replace --force -f \\ https://raw.githubusercontent.com/jboss-container-images/jboss-eap-7-openshift-image/eap72/templates/${resource} -n openshift ``` ## Setup JDG on EAP You need to start EAP with **stanalone-ha.xml** - JBOSS CLI ```bash ./jboss-cli.sh --user=admin --password=admin \\ --controller=http-remoting://127.0.0.1:9990 --connect ``` - Add Socket Binding to RHDG server ```bash /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=remote-rhdg-server1:add(host=127.0.0.1,port=11222) ``` - Add remote cache ```bash batch /subsystem=infinispan/remote-cache-container=rhdg:add(default-remote-cluster=data-grid-cluster) /subsystem=infinispan/remote-cache-container=rhdg/remote-cluster=data-grid-cluster:add(socket-bindings=[remote-rhdg-server1]) run-batch ``` - Enable remote cache statistics ```bash /subsystem=infinispan/remote-cache-container=rhdg:write-attribute(name=statistics-enabled, value=true) /subsystem=infinispan/remote-cache-container=rhdg:read-attribute(name=active-connections) ``` - Check for statistics ```bash /subsystem=infinispan/remote-cache-container=rhdg/remote-cache=verify-cluster.war:read-resource(include-runtime=true, recursive=true) /subsystem=infinispan/remote-cache-container=rhdg/remote-cache=verify-cluster.war:reset-statistics() ``` - Create cache container name **demo** for externalized HTTP to for web ```bash batch /subsystem=infinispan/cache-container=web/invalidation-cache=demo:add() /subsystem=infinispan/cache-container=web/invalidation-cache=demo/store=hotrod:add(remote-cache-container=rhdg,fetch-state=false,purge=false,passivation=false,shared=true) /subsystem=infinispan/cache-container=web/invalidation-cache=demo/component=transaction:add(mode=BATCH) /subsystem=infinispan/cache-container=web/invalidation-cache=demo/component=locking:add(isolation=REPEATABLE_READ) /subsystem=infinispan/cache-container=web:write-attribute(name=default-cache,value=demo) run-batch ``` - edit label ```properties app.openshift.io/runtime=jboss ``` -->"},"gitops.html":{"url":"gitops.html","title":"OpenShift GitOps","keywords":"","body":"OpenShift GitOps OpenShift GitOps GitOps Operator ArgoCD CLI Add Cluster ArcoCD Applications Frontend/Backend App Todo App Phase and Sync-Wave ArgoCD Web Console GitOps Operator Install OpenShift GitOps Operator Wait for few minutes. Check ArgoCD's pods in openshift-gitops namespace oc get pods -n openshift-gitops Output NAME READY STATUS RESTARTS AGE cluster-d469b8c87-hj4td 1/1 Running 0 68s kam-6976788946-fdhfd 1/1 Running 0 68s openshift-gitops-application-controller-0 1/1 Running 0 66s openshift-gitops-applicationset-controller-66db7bd58c-mfp4v 1/1 Running 0 66s openshift-gitops-dex-server-8bd64f9f7-c5lt4 1/1 Running 0 65s openshift-gitops-redis-7867d74fb4-6mnwk 1/1 Running 0 66s openshift-gitops-repo-server-55959654b4-t6jrh 1/1 Running 0 66s openshift-gitops-server-6776b46d54-czc89 1/1 Running 0 66s Access ArgoCD Console. Select Cluster Argo CD from top menu bar By CLI ARGOCD=$(oc get route/openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}') echo https://$ARGOCD Notice that ArgoCD route is passtrough. Extract password from secret PASSWORD=$(oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-) 2>/dev/null echo $PASSWORD Install argocd cli. For OSX use brew brew install argocd ArgoCD CLI login to argocdargocd login $ARGOCD --insecure \\ --username admin \\ --password $PASSWORD Output'admin:login' logged in successfully Context 'openshift-gitops-server-openshift-gitops.apps.cluster-0e2b.0e2b.sandbox563.opentlc.com' updated Use oc or kubectl CLI to login to target cluster and rename context Remark: In this demo, Target cluster is the same cluster as ArgoCD cluster. If target cluster is another cluster then use oc command to login to target cluster first. oc config rename-context $(oc config current-context) dev-cluster Output Context \"default/api-cluster-0e2b-0e2b-sandbox563-opentlc-com:6443/opentlc-mgr\" renamed to \"dev-cluster\". Add Cluster Use argocd CLI to add current cluster to be managed by ArgoCD argocd cluster add dev-cluster Output INFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\" INFO[0001] ClusterRole \"argocd-manager-role\" updated INFO[0002] ClusterRoleBinding \"argocd-manager-role-binding\" updated Cluster 'https://api.cluster-0e2b.0e2b.sandbox563.opentlc.com:6443' added ArcoCD Applications Frontend/Backend App Create application demo-dev-cluster oc apply -f manifests/gitops/applications/demo-dev-cluster.yaml Output application.argoproj.io/demo-dev-cluster created Check application demo-dev-cluster status oc get application -n openshift-gitops Output NAME SYNC STATUS HEALTH STATUS demo-dev-cluster Synced Healthy demo-dev-cluster use kustomize and configured to manifests/apps-kustomize/overlyas/dev manifests/apps-kustomize ├── base │ ├── backend-service.yaml │ ├── backend.yaml │ ├── demo-rolebinding.yaml │ ├── frontend-service.yaml │ ├── frontend.yaml │ ├── kustomization.yaml │ ├── namespace.yaml │ └── route.yaml └── overlays ├── dev │ ├── backend.yaml │ ├── frontend.yaml │ └── kustomization.yaml └── prod ├── backend.yaml ├── frontend.yaml └── kustomization.yaml Todo App Order of operation sync by ArgoCD can be managed by Phase and Wave Phase and Sync-Wave Phase Sync-Wave Object PreSync -100 Secret PreSync 100 pvc for todo-db PreSync 100 todo-db deployment PreSync 200 todo-db service Sync 100 todo deployment Sync 200 todo service Sync 300 todo route PostSync 100 todo service monitor todo-dev-cluster is ordered by todo-db, todo app and service montioring. Create application todo-dev-cluster oc apply -f manifests/gitops/applications/todo-dev-cluster.yaml Check for Hook and Wave in Todo Database todo-db.yaml Secret sync-wave -1 PVC and Deployment sync-wave 1 annotations: argocd.argoproj.io/hook: PreSync argocd.argoproj.io/sync-wave: \"1\" Todo App todo.yaml annotations: argocd.argoproj.io/hook: Sync argocd.argoproj.io/sync-wave: \"1\" ArgoCD Web Console Walkthrough ArgoCD console Open ArgoCD URL Application status Overall Reference to git commit Application topology Node topology Pod's log "},"openshift-service-mesh.html":{"url":"openshift-service-mesh.html","title":"OpenShift Service Mesh","keywords":"","body":"OpenShift Service Mesh OpenShift Service Mesh Overview Setup Control Plane and sidecar Traffic Management Destination Rule, Virtual Service and Gateway Kiali CLI/YAML Test Istio Gateway A/B Deployment with Weight-Routing Conditional Routing by URI Canary Deployment using HTTP headers Traffic Mirroring (Dark Launch) Observability Traffic Analysis Distributed Tracing JDBC Tracing with OpenTracing Envoy Access Log Service Resilience Circuit Breaker Secure with mTLS Within Service Mesh Pod Liveness and Readiness Istio Gateway with mTLS JWT Token Red Hat Single Sign-On RequestAuthentication and Authorization Policy Service Level Objective (SLO) Control Plane with High Availability OpenShift Service Mesh 1.x OpenShift Service Mesh 2.x Rate Limit OpenShift Service Mesh 2.0.x or Istio 1.6.x Overview Sample application Setup Control Plane and sidecar Install following Operators from OperatorHub Red Hat OpenShift distributed tracing platform (Jaeger for OpenShift Service Mesh earlier than version 2.1) Kiali OpenShift Service Mesh ElasticSearch (Optional) Install with CLI oc apply -f manifests/ossm-sub.yaml sleep 10 oc wait --for condition=established --timeout=180s \\ crd/servicemeshcontrolplanes.maistra.io \\ crd/kialis.kiali.io \\ crd/jaegers.jaegertracing.io oc get csv Output customresourcedefinition.apiextensions.k8s.io/servicemeshcontrolplanes.maistra.io condition met customresourcedefinition.apiextensions.k8s.io/kialis.kiali.io condition met customresourcedefinition.apiextensions.k8s.io/jaegers.jaegertracing.io condition met NAME DISPLAY VERSION REPLACES PHASE jaeger-operator.v1.38.0-2 Red Hat OpenShift distributed tracing platform 1.38.0-2 jaeger-operator.v1.36.0-2 Succeeded kiali-operator.v1.57.3 Kiali Operator 1.57.3 kiali-operator.v1.48.3 Succeeded servicemeshoperator.v2.3.0 Red Hat OpenShift Service Mesh 2.3.0-0 servicemeshoperator.v2.2.3 Succeeded o Create control plane by create ServiceMeshControlPlane CRD CLI oc new-project istio-system oc create -f manifests/smcp.yaml -n istio-system OpenShift Administrator Console Switch to Adminstration and navigate to: Operators -> Installed Operators then select Red Hat OpenShift Service Mesh->Service Mesh Control Plane Select Create ServiceMeshControlPlane - Navigate to Proxy and input following values to enable access log at Envoy (sidecar) ![](images/smcp-envoy-access-log.png) - Set outbound traffic policy ![](images/smcp-outbound-allow-any.png) - Verify YAML ![](images/dev-console-smcp-yaml.png) - Create Check for control plane creating status or use CLI watch oc get smcp/basic -n istio-system Output NAME READY STATUS PROFILES VERSION AGE basic 9/9 ComponentsReady [\"default\"] 2.3.0 80s Join project1 into control plane Create data plane project oc new-project project1 Review ServiceMeshMemberRoll CRD apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - project1 Apply ServiceMeshMemberRoll oc create -f manifests/smmr.yaml -n istio-system Check for ServiceMeshMemberRoll status oc describe smmr/default -n istio-system | grep -A2 Spec: Deploy sidecar to frontend app in project1 oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc patch deployment/frontend-v2 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 Check for sidecar in frontend-v1 and frontend-v2 pods oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-577b98f48c-6j5zg 2/2 Running 0 15s frontend-v1-c5d4648f9-7jfk2 1/1 Terminating 0 13m frontend-v2-5cd968bc59-cwsd8 2/2 Running 0 14s frontend-v2-5d4dbdbc9-k6787 1/1 Terminating 0 13m Check developer console Traffic Management Destination Rule, Virtual Service and Gateway Kiali Open Kiali Console Navigate to Services and select frontend, Actions-> Request Routing Click Route To, Set weight of frontend-v1 to 100% then Click Add Rule Click Show Advanced Options Check cluster domain DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') echo $DOMAIN Virtual Service Hosts frontend.apps.$DOMAIN Gateway Traffic Policy Create CLI/YAML Create Destination Rule for frontend v1 and frontend v2 Review Destination Rule CRD apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: frontend spec: host: frontend.project1.svc.cluster.local trafficPolicy: loadBalancer: simple: ROUND_ROBIN connectionPool: tcp: maxConnections: 20 http: http1MaxPendingRequests: 5 outlierDetection: consecutiveGatewayErrors: 2 consecutive5xxErrors: 2 subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 Create destination rule oc apply -f manifests/frontend-destination-rule.yaml -n project1 Create Virtual Service for frontend app Review Virtual Service CRD, Replace DOMAIN with cluster's sub-domain. apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.DOMAIN gateways: - project1/frontend-gateway trafficPolicy: loadBalancer: simple: ROUND_ROBIN tls: mode: DISABLE #ISTIO_MUTUAL http: - route: - destination: port: number: 8080 host: frontend.project1.svc.cluster.local Replace DOMAIN with cluster DOMAIN and create virtual service or run following command DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - Create Gateway for frontend app Check for cluster's sub-domain DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') echo $DOMAIN Review Gateway CRD, Replaced DOMAIN with cluster's sub-domain apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: frontend-gateway spec: servers: - port: number: 80 protocol: HTTP name: http hosts: - frontend.apps.DOMAIN selector: istio: ingressgateway Replace DOMAIN with your clsuter sub-domain and Create gateway oc apply -f manifests/frontend-gateway.yaml -n project1 or use following bash command DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-gateway.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - Check that route is automatically created oc get route -n istio-system | grep frontend-gateway Sample outout project1-frontend-gateway-5f5077c573bd9294 frontend.apps.cluster-27bb.27bb.sandbox664.opentlc.com istio-ingressgateway http None Review Route, Replace DOMAIN with cluster's DOMAIN apiVersion: v1 kind: Route metadata: name: frontend spec: host: frontend.apps.DOMAIN port: targetPort: http2 to: kind: Service name: istio-ingressgateway weight: 100 wildcardPolicy: None Replace DOMAIN with cluster DOMAIN then create Route oc apply -f manifests/frontend-route-istio.yaml -n istio-system or use following bash command bash DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-route-istio.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - --> Test Istio Gateway Test with cURL FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl http://$FRONTEND_ISTIO_ROUTE A/B Deployment with Weight-Routing Set weight routing between 2 services with virtual service Remark: if you use above Kiali steps then you already set 100% traffic to frontend-v1 Check for virtual service with weight routing, Replace DOMAIN with cluster's DOMAIN ... http: - route: - destination: port: number: 8080 host: frontend.project1.svc.cluster.local subset: v1 weight: 100 - destination: port: number: 8080 host: frontend.project1.svc.cluster.local subset: v2 weight: 0 Apply virtual service for Blue/Green deployment with route all traffic to v1 DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-weight-routing.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - CLI Test with cURL to verify that all requests are routed to v1 Blue/Green deployment by route all requests to v2 oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":0},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":100}]}}]' -n project1 Test with cURL to verify that all requests are routed to v2 Adjust traffic 30% to v2 oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":70},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":30}]}}]' -n project1 Test A/B deployment Run 100 requests FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') COUNT=0 rm -f result.txt while [ $COUNT -lt 100 ]; do OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE/version) printf \"%s\\n\" $OUTPUT >> result.txt printf \"%s\\n\" $OUTPUT sleep .2 COUNT=$(expr $COUNT + 1) done Check result for comparing percentage of requests to v1 and v2 printf \"Version 1: %s\\n\" $(cat result.txt | grep \"v1\" | wc -l) printf \"Version 2: %s\\n\" $(cat result.txt | grep \"v2\" | wc -l) rm -f result.txt Conditional Routing by URI Set conditional routing between 2 services with virtual service Check for virtual service by URI, Replace DOMAIN with cluster's DOMAIN. Condition with regular expression Route to v1 if request URI start with \"/ver\" and end with \"1\" apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.SUBDOMAIN gateways: - project1/frontend-gateway http: - match: - uri: regex: /ver(.*)1 # Rewrite URI back to / because frontend app not have /ver(*)1 rewrite: uri: \"/\" route: - destination: host: frontend port: number: 8080 subset: v1 - route: - destination: host: frontend port: number: 8080 subset: v2 Apply virtual service oc apply -f manifests/frontend-virtual-service-with-uri.yaml -n project1 or use following bash command DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-uri.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - Test with URI /version1 and /ver1 FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl -w\"\\n\\n\" $FRONTEND_ISTIO_ROUTE/version1 curl -w\"\\n\\n\" $FRONTEND_ISTIO_ROUTE/vers1 curl -w\"\\n\\n\" $FRONTEND_ISTIO_ROUTE/ver1 Test with URI / FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl -w\"\\n\\n\" $FRONTEND_ISTIO_ROUTE/ Canary Deployment using HTTP headers Canary Deployment by checking User-Agent header with Virtual Service, Replace DOMAIN with cluster's sub-domain. If HTTP header User-Agent contains text Firewall, request will be routed to frontend v2 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.DOMAIN gateways: - project1/frontend-gateway http: - match: - headers: user-agent: regex: (.*)Firefox(.*) route: - destination: host: frontend port: number: 8080 subset: v2 - route: - destination: host: frontend port: number: 8080 subset: v1 Apply Virtual Service oc apply -f manifests/frontend-virtual-service-with-header.yaml -n project1 or use following bash command DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-header.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - Test with cURL with HTTP header User-Agent contains Firefox FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl -H \"User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" $FRONTEND_ISTIO_ROUTE Traffic Mirroring (Dark Launch) Deploy backend application oc apply -f manifests/backend.yaml -n project1 oc apply -f manifests/backend-destination-rule.yaml -n project1 oc apply -f manifests/backend-virtual-service-v1-v2-50-50.yaml -n project1 oc get pods -n project1 Configure frontend to request to backend oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc set env deployment/frontend-v2 BACKEND_URL=http://backend:8080/ -n project1 Optional: Draw connetion from frontend to backend in Developer Console oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc annotate deployment frontend-v2 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 Deploy audit app and mirror every requests that frontend call backend to audit app oc apply -f manifests/audit-app.yaml -n project1 oc get pods -n project1 Update backend virtual service to mirror requests to audit app. oc apply -f manifests/backend-virtual-service-mirror.yaml -n project1 Use cURL to call frontend and check audit's pod log by CLI (with another terminal) or Web Console cURL frontend FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl $FRONTEND_ISTIO_ROUTE View audit log oc logs -f $(oc get pods --no-headers | grep audit|head -n 1|awk '{print $1}') -c backend -n project1 Kiali Graph Observability Traffic Analysis Check Kiali Console login to OpenShift Developer Console, select project istio-system and open Kiali console Login to Kiali Console and select Graph Namespace: select checkbox \"project1\" Display: select checkbox \"Request distribution\" and \"Traffic animation\" Run following command DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-weight-routing.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":70},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":30}]}}]' -n project1 FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') while [ 1 ]; do OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE) printf \"%s\\n\" $OUTPUT sleep .2 done Check Kiali Console Traffic analysis for frontend app. Select Application->frontend->inbound traffic and outbound traffic Distributed Tracing Distributed tracing with Jaeger. Select tab Tracing Overall tracing for frontend app Login to Jaeger by select \"View in Tracing\" Drill down to tracing information Simulate error on backend app set weight to 50/50 oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":50},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":50}]}}]' -n project1 set backend pod to return 504 oc exec -c backend -n project1 $(oc get pods -n project1 -l app=backend --no-headers -o=custom-columns=\"NAME:.metadata.name\"|head -n 1) -- curl -w \"\\n\\n\" -s http://localhost:8080/stop Request to frontend app curl -s -w \"\\n\" $FRONTEND_ISTIO_ROUTE curl -s -w \"\\n\" $FRONTEND_ISTIO_ROUTE Query Jaeger with tag http.status_code=504. Check detail trace to verify that envoy retry request to backend service Drill down into detail of transaction set backend pod to return 200 oc exec -c backend -n project1 $(oc get pods -n project1 -l app=backend --no-headers -o=custom-columns=\"NAME:.metadata.name\"|head -n 1) -- curl -w \"\\n\\n\" -s http://localhost:8080/start JDBC Tracing with OpenTracing Deploy todo application with Kustomize oc create -k manifests/todo-kustomize/overlays/dev watch oc get pods -n todo Todo Application Add namespace todo to ServiceMeshMemberRoll oc patch smmr default -n istio-system --type='json' -p='[{\"op\":\"add\",\"path\":\"/spec/members/-\",\"value\":\"todo\"}]' Add istio sidecar to deployment todo oc patch deployment todo -n todo -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' Change todo container image to quay.io/voravitl/todo:trace and set JDBC URL to use tracing driver oc patch deployment todo -n todo --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"quay.io/voravitl/todo:trace\"}]' oc set env deployment todo -n todo quarkus.datasource.jdbc.url=jdbc:tracing:postgresql://todo-db/todo oc set env deployment todo -n todo quarkus.jaeger.endpoint=http://jaeger-collector.istio-system:14268/api/traces Create, update and delete tasks in todo application then Check Kiali console. Select by query type Transaction with create todo item Transaction with query todo items Transaction with deleting todo item Check for traceid in todo pod's log oc logs -f \\ $(oc get pods -l app=todo -n todo --no-headers -o=custom-columns=\"NAME:.metadata.name\"|head -n 1) \\ -n todo -c todo output 09:54:27 INFO x-b3-traceid=510dc04de55eb770167efec4a8cd622a, , parentId=167efec4a8cd622a, x-b3-spanid=5ad367c2c286947e, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) update id:9 09:54:27 INFO x-b3-traceid=447bbafa3ed323909180faf410d181f5, , parentId=9180faf410d181f5, x-b3-spanid=a7a9d4b287917f5f, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) update id:10 09:54:32 INFO x-b3-traceid=32c779ecb20641fa3823e3b6d448c41e, , parentId=3823e3b6d448c41e, x-b3-spanid=bf61443edfc58082, sampled=true [io.qu.sa.TodoResource] (executor-thread-0) getAll Envoy Access Log Envoy access log already enabled with ServiceMeshControlPlane CRD proxy: accessLogging: envoyService: enabled: false file: encoding: TEXT name: /dev/stdout Check access log oc logs -f \\ $(oc get pods -n project1 --no-headers -o=custom-columns=\"NAME:.metadata.name\" -l app=frontend|head -n 1) \\ -c istio-proxy -n project1 Sample output 03:17:46 INFO x-b3-traceid=7256eae02a1f11166d5add572165bfa0, , parentId=6d5add572165bfa0, x-b3-spanid=41e2c3434fbb022a, sampled=true [io.qu.sa.TodoResource] (executor-thread-2) getAll Search by x-b3-traceid View trace Service Resilience Configure our application to contains only frontend-v1 and backend-v1 and scale backend to 3 pods. oc delete all --all -n project1 oc delete gateway --all -n project1 oc delete dr,vs --all -n project1 oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc apply -f manifests/backend.yaml -n project1 oc delete deployment/frontend-v2 -n project1 oc delete deployment/backend-v2 -n project1 oc delete route frontend -n project1 oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc scale deployment backend-v1 --replicas=3 -n project1 oc apply -f manifests/backend-destination-rule-v1-only.yaml -n project1 oc apply -f manifests/backend-virtual-service.yaml -n project1 oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1 DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - cat manifests/frontend-gateway.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - watch oc get pods -n project1 Test with cURL FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl http://$FRONTEND_ISTIO_ROUTE Sample output - Check for field Host that is backend pod that processed for this request Frontend version: 1.0.0 => [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Hello, Quarkus] Loop 6 times. Result from backend will be round robin. Create bash function function loop_frontend(){ FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') COUNT=0 MAX=$1 while [ $COUNT -lt $MAX ]; do curl -s http://$FRONTEND_ISTIO_ROUTE | awk -F',' '{print $5 \"=>\" $2}' COUNT=$(expr $COUNT + 1 ) done } Run function with input paramter 6 loop_frontend 6 Sample output Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Check that pods run on which node oc get pods -o=custom-columns=\"NAME:.metadata.name,AZ:.metadata.labels['topology\\.kubernetes\\.io/zone']\" -n project1 Sample output NAME AZ backend-v1-7779cb476b-6wbsp us-east-2a backend-v1-7779cb476b-bgk22 us-east-2a backend-v1-7779cb476b-q2hz9 us-east-2b frontend-v1-d6dc6768-vbzcc us-east-2a Envoy has Localcity Load Balancing feature and this feature is enabled by default. A locality defines geographic location by region, zone and subzone. Envoy will try to send request to pod within defined geographic if avilable In this case is within same AZ Notice that response came frome 2 pods in AZ us-east-2a same AZ with frontend By default, Envoy will automatically retry if it get response with code 503 Force one backend pod to return 503 by command line. oc exec -n project1 -c backend -- curl -s http://localhost:8080/not_ready Sample output Backend version:v1, Response:200, Host:backend-v1-7779cb476b-bgk22, Status:200, Message: Readiness: false by Web Console Verify response from that pod. oc exec -n project1 -c backend -- curl -sv http://localhost:8080/ Sample Output Test with cURL again. You will get only status 200 loop_frontend 10 Sample Output Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:200 Check tracing in Jaeger by query with http.response_code=503 Drill down to check that envoy retry request to backend after it got 503 response. Set backend pod to return 200 oc exec -n project1 -c backend -- curl -s http://localhost:8080/ready Circuit Breaker Update destination rule with circuit breaker oc apply -f manifests/backend-destination-rule-circuit-breaker.yaml -n project1 Review Circuit Breaker configuration in deatination rule If found error 1 times (consecutiveErrors) then eject that pod from pool for 15 mintues (baseEjectionTime) Maximum number of pod that can be ejected is 100% (maxEjectionPercent) Check this every 15 min (interval) outlierDetection: baseEjectionTime: 15m consecutiveErrors: 1 interval: 15m maxEjectionPercent: 100 Set one backend pod to return 504 and verify that pod return 504 oc exec -n project1 -c backend -- curl -s http://localhost:8080/stop Sample output Backend version:v1, Response:200, Host:backend-v1-7779cb476b-bgk22, Status:200, Message: Liveness: false Verify that backend pod return 504 oc exec -n project1 -c backend -- curl -s http://localhost:8080/ Sample output Backend version:v1, Response:504, Host:backend-v1-7779cb476b-bgk22, Status:504, Message: Application liveness is set to fasle Test again with cURL. You will get 504 just one times. loop_frontend 15 Sample output Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-q2hz9=> Status:504 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Host:backend-v1-7779cb476b-6wbsp=> Status:200 Check Kiali Console. Remark that there is lightning icon at backend service. This is represent for circuit breaker. Set backend pod to normal status oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -sv http://localhost:8080/start Secure with mTLS Within Service Mesh Enable data plane mTLS by edit ServiceMeshControlPlane with following configuration Deploy another pod without sidecar and try to connect to anther services that is part of mesh Pod Liveness and Readiness Enable Liveness nad Readiness on backend-v1 oc set probe deployment backend-v1 \\ --readiness --get-url=http://:8080/q/health/ready \\ --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=5 -n project1 oc set probe deployment backend-v1 \\ --liveness --get-url=http://:8080/q/health/live \\ --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=5 -n project1 Check for pod status watch oc get pods -l app=backend,version=v1 -n project1 Example of output NAME READY STATUS RESTARTS AGE backend-v1-5846f59c84-p6tn5 1/2 CrashLoopBackOff 4 68s Remark: Liveness and Readiness probe fail because kubelet cannot connect to port 8080 anymore. Rewrite HTTP probe by annotation to deployment oc patch deployment/backend-v1 \\ -n project1 \\ -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/rewriteAppHTTPProbers\":\"true\"}}}}}' Remove Liveness and Readiness probe oc set probe deployment backend-v1 --remove --readiness --liveness -n project1 Istio Gateway with mTLS Create certificates and private key mkdir -p certs DOMAIN=$(oc whoami --show-console | awk -F'apps.' '{print $2}') CN=frontend.apps.$DOMAIN echo \"Create Root CA and Private Key\" openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj '/O=example Inc./CN=example.com' \\ -keyout certs/example.com.key -out certs/example.com.crt echo \"Create Certificate and Private Key for $CN\" openssl req -out certs/frontend.csr -newkey rsa:2048 -nodes -keyout certs/frontend.key -subj \"/CN=${CN}/O=Great Department\" openssl x509 -req -days 365 -CA certs/example.com.crt -CAkey certs/example.com.key -set_serial 0 -in certs/frontend.csr -out certs/frontend.crt Create secret to store private key and certificate oc create secret generic frontend-credential \\ --from-file=tls.key=certs/frontend.key \\ --from-file=tls.crt=certs/frontend.crt \\ -n istio-system Update Gateway cat manifests/gateway-tls.yaml|sed s/DOMAIN/$DOMAIN/g|oc apply -n project1 -f - Verify updated gateway configuration oc get gateway frontend-gateway -n project1 -o yaml Example of output spec: selector: istio: ingressgateway servers: - hosts: - frontend.apps.cluster-27bb.27bb.sandbox664.opentlc.com port: name: https number: 443 protocol: HTTPS tls: credentialName: frontend-credential mode: SIMPLE port is changed to 443 with protocol HTTPS TLS mode is SIMPLE and use private key and certificate from secret name frontend-credential in control plane namespace SIMPLE mode is for TLS. For mutual TLS use MUTUAL Check that route created by Istio Gateway is updated to passthrough mode oc get route \\ $(oc get route -n istio-system --no-headers -o=custom-columns=\"NAME:.metadata.name\" | grep frontend) \\ -n istio-system -o jsonpath='{.spec.tls.termination}' Example of output passthrough Test with cURL export GATEWAY_URL=$(oc get route $(oc get route -n istio-system | grep frontend | awk '{print $1}') -n istio-system -o yaml -o jsonpath='{.spec.host}') curl -kv https://$GATEWAY_URL Example of output * SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=frontend-istio-user1.apps.; O=Great Department * start date: Sep 1 12:10:22 2021 GMT * expire date: Sep 1 12:10:22 2022 GMT * issuer: O=example Inc.; CN=example.com * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. * Using HTTP2, server supports multi-use * Connection state changed (HTTP/2 confirmed) Create client certificate CN=great-partner.apps.acme.com echo \"Create Root CA and Private Key\" openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj '/O=Acme Inc./CN=acme.com' \\ -keyout certs/acme.com.key -out certs/acme.com.crt echo \"Create Certificate and Private Key for $CN\" openssl req -out certs/great-partner.csr -newkey rsa:2048 -nodes -keyout certs/great-partner.key -subj \"/CN=${CN}/O=Great Department\" openssl x509 -req -days 365 -CA certs/acme.com.crt -CAkey certs/acme.com.key -set_serial 0 -in certs/great-partner.csr -out certs/great-partner.crt Update frontend-credential secret oc create secret generic frontend-credential \\ --from-file=tls.key=certs/frontend.key \\ --from-file=tls.crt=certs/frontend.crt \\ --from-file=ca.crt=certs/acme.com.crt \\ -n istio-system --dry-run=client -o yaml \\ | oc replace -n istio-system secret frontend-credential -f - Update frontend gateway TLS mode to MUTUAL oc patch gateway frontend-gateway -n project1 \\ --type='json' \\ -p='[{\"op\":\"replace\",\"path\":\"/spec/servers/0/tls/mode\",\"value\":\"MUTUAL\"}]' Test cURL without client certificate curl -k https://$GATEWAY_URL You will get following error curl: (35) error:1401E410:SSL routines:CONNECT_CR_FINISHED:sslv3 alert handshake failure cURL with Acme Inc certificate curl -kv --cacert certs/acme.com.crt \\ --cert certs/great-partner.crt \\ --key certs/great-partner.key \\ https://$GATEWAY_URL Example of output * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: certs/acme.com.crt CApath: none ... * Server certificate: * subject: CN=frontend-istio-user1.apps.; O=Great Department * start date: Sep 1 12:10:22 2021 GMT * expire date: Sep 1 12:10:22 2022 GMT * issuer: O=example Inc.; CN=example.com * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway ... Frontend version: 1.0.0 => [Backend: http://backend:8080, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-xp65r, Status:200, Message: Hello, Quarkus] Generate another certificate and private key (Pirate Inc) that frontend gateway not trust CN=bad-partner.apps.pirate.com echo \"Create Root CA and Private Key\" openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj '/O=Pirate Inc./CN=pirate.com' \\ -keyout certs/pirate.com.key -out certs/pirate.com.crt echo \"Create Certificate and Private Key for $CN\" openssl req -out certs/bad-partner.csr -newkey rsa:2048 -nodes -keyout certs/bad-partner.key -subj \"/CN=${CN}/O=Bad Department\" openssl x509 -req -days 365 -CA certs/pirate.com.crt -CAkey certs/pirate.com.key -set_serial 0 -in certs/bad-partner.csr -out certs/bad-partner.crt cURL with Pirate Inc certificate curl -k --cacert certs/pirate.com.crt \\ --cert certs/bad-partner.crt \\ --key certs/bad-partner.key \\ https://$GATEWAY_URL You will get error alert unknown ca curl: (35) error:1401E418:SSL routines:CONNECT_CR_FINISHED:tlsv1 alert unknown ca Update frontend gateway to trust Pirate Inc by update frontend-credential secret cat certs/acme.com.crt > certs/trusted.crt cat certs/pirate.com.crt >> certs/trusted.crt oc create secret generic frontend-credential \\ --from-file=tls.key=certs/frontend.key \\ --from-file=tls.crt=certs/frontend.crt \\ --from-file=ca.crt=certs/trusted.crt \\ -n istio-system --dry-run=client -o yaml \\ | oc replace -n istio-system secret frontend-credential -f - Test with Pirate Inc certificate curl -k --cacert certs/pirate.com.crt \\ --cert certs/bad-partner.crt \\ --key certs/bad-partner.key \\ https://$GATEWAY_URL Recheck that Acme Inc can acess frontend app curl -kv --cacert certs/acme.com.crt \\ --cert certs/great-partner.crt \\ --key certs/great-partner.key \\ https://$GATEWAY_URL Update frontend gateway TLS mode to SIMPLE oc patch gateway frontend-gateway -n project1 \\ --type='json' \\ -p='[{\"op\":\"replace\",\"path\":\"/spec/servers/0/tls/mode\",\"value\":\"SIMPLE\"}]' JWT Token Red Hat Single Sign-On Setup Red Hat Single Sign-On (Keycloak) Create namespace oc new-project sso --description=\"Red Hat Single Sign-On\" --display-name=\"Red Hat Single Sign-On\" Install Red Hat Single Sign-On Operator Install to namespace sso Create Keycloak instance oc create -f manifests/keycloak.yaml -n sso watch oc get pods -n sso Sample outout NAME READY STATUS RESTARTS AGE keycloak-0 0/1 PodInitializing 0 19s keycloak-postgresql-76c57d6f5b-jgnx6 0/1 ContainerCreating 0 19s rhsso-operator-748cdf5c96-6nwtq 1/1 Running 0 82s Cosmetic topology view oc label deployment/keycloak-postgresql app.kubernetes.io/name=postgresql -n sso oc annotate statefulset/keycloak 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"keycloak-postgresql\"}]' -n sso Extract admin password KEYCLOAK_ADMIN_PASSWORD=$(oc extract secret/credential-demo -n sso --to=- --keys=ADMIN_PASSWORD 2>/dev/null) Create Realm and Keycloak Client with Client Credential oc create -f manifests/keycloak-realm.yaml -n sso oc create -f manifests/keycloak-client.yaml -n sso Client secret Test login with client secret KEYCLOAK=$(oc get route/keycloak -n sso -o jsonpath='{.spec.host}') CLIENT_SECRET=e31fe61b-2cc1-41da-9644-d72bdb8339d5 TOKEN=$(curl -s --location --request \\ POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode client_id=frontend-app \\ --data-urlencode client_secret=$CLIENT_SECRET \\ --data-urlencode scope=email \\ --data-urlencode grant_type=client_credentials | jq .access_token | sed s/\\\"//g) RequestAuthentication and Authorization Policy Create RequestAuthentication and AuthorizationPolicy oc apply -f manitests/frontend-jwt -n project1 Test without JWT token. You will get HTTP resonse code 403 curl -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com Test with invalid JWT token. You will get HTTP resonse code 401 Test with valid JWT token TOKEN=$(curl -s --location --request \\ POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode client_id=frontend-app \\ --data-urlencode client_secret=$CLIENT_SECRET \\ --data-urlencode scope=email \\ --data-urlencode grant_type=client_credentials | jq .access_token | sed s/\\\"//g) curl --header 'Authorization: Bearer '$TOKEN -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com Test with expired JWT token (token life is 5 minutes). You will get HTTP response code 401 with following message Jwt is expired* Closing connection 0 Service Level Objective (SLO) We can use Service Level Indicator (SLI) and Service Level Objective (SLO) to determine and measure availability of services. For RESTful Web Service we can use HTTP response code to measure for SLI Deploy applicaition oc delete all --all -n project1 oc delete gateway --all -n project1 oc delete dr,vs --all -n project1 oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc apply -f manifests/backend.yaml -n project1 oc scale deployment/frontend-v1 --replicas=5 -n project1 oc scale deployment/backend-v1 --replicas=10 -n project1 oc scale deployment/backend-v2 --replicas=10 -n project1 oc delete deployment/frontend-v2 -n project1 oc delete route frontend -n project1 oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc apply -f manifests/backend-destination-rule.yaml -n project1 oc apply -f manifests/backend-virtual-service-v1-v2-50-50.yaml -n project1 oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1 DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - cat manifests/frontend-gateway.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - oc patch virtualservice backend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"backend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":100},{\"destination\":{\"host\":\"backend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":0}]}}]' -n project1 watch oc get pods -n project1 Generate load Create namespace oc new-project load-test Run K6 with 15 threads for 10 minutes to simulate workload FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') oc run load-test -n load-test -i --rm \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Prometheus in Service Mesh's control plane contains information about HTTP responses then we can use following PromQL to check for the sucessfull request and total request of backend service Use OpenShift Developer Console, select project istio-system and open Prometheus console Success Rate Successful request for last 5 minutes sum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) Total requests for last 5 minutes sum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m])) Latency 99th Percentile of response time in sec of frontend service histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"frontend\",response_code!~\"5*\"}[5m])) by (le))/1000 SLO for success rate can be calculated by following PromQL and compare this to your desired service level e.g. 99.9% sum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) / sum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m]))*100 Login to Grafana Dashbaord in control plane and import SLO Dashbaord Backend Application service %availability sum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5.*\"}[5m])) / sum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m])) *100 Frontend 99th percentile response time in second histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"frontend\",response_code!~\"5*\"}[5m])) by (le))/1000 Backend 99th percentile response time in second histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) by (le))/1000 Run following bash script to force 5 backend-v1 pod to return 504 then set those pods to return to 200 OK. for pod in $(oc get pods -l app=backend,version=v1 --no-headers -o=custom-columns=\"NAME:.metadata.name\" -n project1|head -n 5|sort) do oc exec -n project1 -c backend $pod -- curl -s http://localhost:8080/stop -w \"\\n\" sleep 1 done sleep 10 for pod in $(oc get pods -l app=backend,version=v1 --no-headers -o=custom-columns=\"NAME:.metadata.name\" -n project1|head -n 5|sort) do oc exec -n project1 -c backend $pod -- curl -s http://localhost:8080/start -w \"\\n\" done Run following bash script to set traffic to backend-v2 and check both frontend and backend response time increasing. oc patch virtualservice backend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"backend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":30},{\"destination\":{\"host\":\"backend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":70}]}}]' -n project1 Control Plane with High Availability OpenShift Service Mesh 1.x ServiceMeshControlPlane with high availability configuration Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway Set request and limit Set autoscaling to true Set number of min and max replicas with target CPU utilization to trigger HPA ingress: enabled: true ingress: false runtime: container: resources: requests: cpu: 10m memory: 128Mi limits: cpu: 2000m memory: 2048Mi For others components Set number of replicas to 2 deployment: autoScaling: enabled: false replicas: 2 Set pod anti-affinity to prevent scheduler to place pods to the same node Remark: namespaces in podAntiAffinity is needed to support multiples control planes in the same OpenShift cluster. Change this to match name of control plane's namespace pod: tolerations: - key: node.kubernetes.io/unreachable operator: Exists effect: NoExecute tolerationSeconds: 60 affinity: podAntiAffinity: requiredDuringScheduling: - key: istio topologyKey: kubernetes.io/hostname operator: In values: - galley namespaces: istio-system Check that pods of each deployment run on different nodes oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase Output NAME NODE PHASE grafana-7bdb4fb848-847c8 ip-10-0-160-48.us-east-2.compute.internal Running istio-citadel-6668b5b947-njgbb ip-10-0-160-48.us-east-2.compute.internal Running istio-citadel-6668b5b947-nk9dz ip-10-0-137-21.us-east-2.compute.internal Running istio-galley-6dc7f9c496-hkm57 ip-10-0-137-21.us-east-2.compute.internal Running istio-galley-6dc7f9c496-qcw9q ip-10-0-160-48.us-east-2.compute.internal Running istio-ingressgateway-6bcd484457-25tq7 ip-10-0-137-21.us-east-2.compute.internal Running istio-ingressgateway-6bcd484457-nvfb9 ip-10-0-160-48.us-east-2.compute.internal Running istio-pilot-74d5db759c-m9jxm ip-10-0-137-21.us-east-2.compute.internal Running istio-pilot-74d5db759c-rcdxj ip-10-0-160-48.us-east-2.compute.internal Running istio-policy-58ff56d7dc-26wsq ip-10-0-137-21.us-east-2.compute.internal Running istio-policy-58ff56d7dc-62gwl ip-10-0-160-48.us-east-2.compute.internal Running istio-sidecar-injector-ffc58c87-4t5gc ip-10-0-137-21.us-east-2.compute.internal Running istio-sidecar-injector-ffc58c87-rjz7l ip-10-0-160-48.us-east-2.compute.internal Running istio-telemetry-646d7cf56c-fz72g ip-10-0-137-21.us-east-2.compute.internal Running istio-telemetry-646d7cf56c-lctxg ip-10-0-160-48.us-east-2.compute.internal Running jaeger-7b866d475f-nhrp5 ip-10-0-160-48.us-east-2.compute.internal Running kiali-75dc58b5f6-bwk7q ip-10-0-137-21.us-east-2.compute.internal Running prometheus-85db9d786b-vzskf ip-10-0-160-48.us-east-2.compute.internal Running prometheus-85db9d786b-wgrwz ip-10-0-137-21.us-east-2.compute.internal Running Verify HPA for ingress gateway oc get hpa -n istio-system Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-ingressgateway Deployment/istio-ingressgateway 0%/85% 2 4 2 10m OpenShift Service Mesh 2.x ServiceMeshControlPlane with high availability configuration Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway Set request and limit Set autoscaling to true Set number of min and max replicas with target CPU utilization to trigger HPA ingress: enabled: true runtime: container: resources: requests: cpu: 500m memory: 300Mi limits: cpu: 2 memory: 1Gi deployment: autoScaling: enabled: true maxReplicas: 4 minReplicas: 2 targetCPUUtilizationPercentage: 85 For others components Set number of replicas to 2 pilot: deployment: replicas: 2 Check that pods of each deployment run on different nodes oc get pods -n istio-system Output grafana-78f656547-gkm92 2/2 Running 0 54s istio-ingressgateway-667749f4bd-pfl2l 1/1 Running 0 54s istio-ingressgateway-667749f4bd-sfwx4 1/1 Running 0 39s istiod-basic-install-6994d86579-4n8jf 1/1 Running 0 77s istiod-basic-install-6994d86579-b5bgv 1/1 Running 0 77s jaeger-85d4744d8b-krqfl 2/2 Running 0 54s kiali-784df775f8-xccsw 1/1 Running 0 28s prometheus-79ff59d59f-6j99k 3/3 Running 0 65s prometheus-79ff59d59f-msrpb 3/3 Running 0 65s Verify HPA for ingress gateway oc get hpa -n istio-system Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-ingressgateway Deployment/istio-ingressgateway 0%/85% 2 4 2 10m Check that pods of each deployment run on different nodes oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase Output NAME NODE PHASE grafana-99f7c499f-jnd9k ip-10-0-166-202.us-east-2.compute.internal Running istio-ingressgateway-5fc94885b5-hjhqw ip-10-0-166-202.us-east-2.compute.internal Running istio-ingressgateway-5fc94885b5-hxn9r ip-10-0-151-28.us-east-2.compute.internal Running istiod-basic-install-58c9bc5bf8-4wbhq ip-10-0-151-28.us-east-2.compute.internal Running jaeger-596448d54d-gwn97 ip-10-0-166-202.us-east-2.compute.internal Running kiali-85c677967c-k7767 ip-10-0-166-202.us-east-2.compute.internal Running prometheus-565c997f45-plqqb ip-10-0-151-28.us-east-2.compute.internal Running prometheus-565c997f45-s7q2t ip-10-0-166-202.us-east-2.compute.internal Running Rate Limit OpenShift Service Mesh 2.0.x or Istio 1.6.x Support Rate Limiting feature in OSSM 2.1.x, OSSM 2.0.x was built on upstream istio 1.6 and we tested Rating Limiting case by Enable SMCP Policy Mixer Plugins: (change smcp/basic to another your control plane) oc patch -n istio-system smcp/basic --type merge -p '{\"spec\":{\"policy\":{\"type\": \"Mixer\", \"mixer\":{\"enableChecks\":true}}}}' wait until operator create istio-policy pod complete. Create Sample Microservice for Test this feature oc delete all --all -n project1 oc delete gateway --all -n project1 oc delete dr,vs --all -n project1 oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc apply -f manifests/backend.yaml -n project1 oc delete deployment/frontend-v2 -n project1 oc delete deployment/backend-v2 -n project1 oc delete route frontend -n project1 oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc scale deployment backend-v1 --replicas=3 -n project1 oc apply -f manifests/backend-destination-rule-v1-only.yaml -n project1 oc apply -f manifests/backend-virtual-service.yaml -n project1 oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1 DOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - cat manifests/frontend-gateway.yaml | sed 's/DOMAIN/'$DOMAIN'/'|oc apply -n project1 -f - watch oc get pods -n project1 after finish Test with cURL FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep frontend-gateway |awk '{print $2}') curl http://$FRONTEND_ISTIO_ROUTE Sample output - Check for field Host that is backend pod that processed for this request Frontend version: 1.0.0 => [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Hello, Quarkus] Follow upstream istio 1.6 Rate Limiting documentation for test rate limit : https://istio.io/v1.6/docs/tasks/policy-enforcement/rate-limiting/ or use this example (memquota example) create rate limit configuration (QuotaSpecBinding, QuotaSpec, instance, rule, handler), see detail in rate-limit-memquota.yaml oc apply -f manifests/mesh/rate-limit-memquota.yaml set handler for manage receive only 1 req/min apiVersion: config.istio.io/v1alpha2 kind: handler metadata: name: quotahandler namespace: istio-system spec: compiledAdapter: memquota params: quotas: - name: requestcountquota.instance.istio-system maxAmount: 1 validDuration: 60s set service for rate limit apiVersion: config.istio.io/v1alpha2 kind: QuotaSpecBinding metadata: name: request-count namespace: istio-system spec: quotaSpecs: - name: request-count namespace: istio-system services: - name: frontend namespace: project1 # default #- service: '*' # Uncomment this to bind *all* services to request-count Test with cURL call first time curl -v http://$FRONTEND_ISTIO_ROUTE example result * Trying 3.131.170.108... * TCP_NODELAY set * Connected to frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com (3.131.170.108) port 80 (#0) > GET / HTTP/1.1 > Host: frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com > User-Agent: curl/7.64.1 > Accept: */* > [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-79668c5b99-tkqjn, Status:200, Message: Hello, Quarkus]* Closing connection 0 call it again for check rate limit, openshift service mesh will return HTTP Status 429 Too Many Requests curl -v http://$FRONTEND_ISTIO_ROUTE example result * Trying 3.131.170.108... * TCP_NODELAY set * Connected to frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com (3.131.170.108) port 80 (#0) > GET / HTTP/1.1 > Host: frontend.apps.cluster-deff.deff.sandbox1488.opentlc.com > User-Agent: curl/7.64.1 > Accept: */* > retest again after 1 minute, service will back response with HTTP Status 200 "}}