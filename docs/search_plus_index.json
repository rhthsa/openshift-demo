{"./":{"url":"./","title":"Introduction","keywords":"","body":"OpenShift Demo Gitbook Chick here for Gitbook Table of Contents Infrastructure OpenShift Authentication Providers with AD OpenShift External Authentication Provider LDAP group sync Group Policy Access and Projects collaboration OpenShift MachineSet and Infrastructure Nodes MachineSet on VMware Infrastructure Node and Moving Cluster Services to Infra Nodes OpenShift Platform Monitoring and Alert Monitoring Stack AlertRules and Alert Receiver OpenShift Cluster Logging OpenShift Networking OpenShift Network Policies Based SDN Egress IPs Network Access Logs OpenShift state backup with etcd snapshot Pod Taint and Toleration Custom Roles and Service Account Custom Alert Compliance Multi-cluster Management with Advanced Cluster Management (RHACM) Application Manageement Cost saving with hibernating OpenShiftContainer Applications Application Build & Deployment Command Line with oc Command Line with odo Helm Image Streams OpenShift Route Blue/Green Deployment Canary Deployment Configure TLS version Horizontal Pod Autoscaler HPA by CPU HPA by Memory Health Check Readiness Probe Liveness Probe Startup Probe OpenShift Service Mesh Install and configure control plane Sidecar injection Blue/Green Deployment Canary Deployment A/B Testing Deployment Routing by URI with regular expression Traffic Analysis Traffic Mirroring Tracing Circuit Breaker Secure with mTLS JWT Token (with RHSSO) Service Level Objective (SLO) Control Plane with High Availability User Workload Monitoring Setup User Workload Monitoring Monitor Custom Metrics Custom Grafana Dashboard Custom Alert Build Container with OC command Build Container with OpenShift DO (odo) CI/CD with Jenkins Build Quarkus App Pull artifacts from Nexus Unit Test Code Quality Push container image to Nexus or internal registry Blue/Green Deployment CI/CD with Azure DevOps Azure DevOps Deploy Back App Deploy Front App Prepare Harbor On Kubernetes/OpenShift Prepare Azure DevOps Service Connection Azure pipelines EAP on OpenShift OpenShift GitOps Additional Solutions Managed Multi-Cluster Application Metrics with Prometheus & Thanos "},"infrastructure-authentication-providers.html":{"url":"infrastructure-authentication-providers.html","title":"OpenShift Authentication Providers with AD","keywords":"","body":"Authentication Providers with AD Authentication Providers with AD Prerequisites OpenShift RBAC with AD Background: LDAP Structure Examine the OAuth configuration Syncing LDAP Groups to OpenShift Groups Change Group Policy Examine cluster-admin policy Examine cluster-reader policy Create Projects for Collaboration Map Groups to Projects Examine Group Access Prometheus Prerequisites Microsoft AD (with LDAP protocol) Users and Groups Assigned Roles OpenShift RBAC with AD Configuring External Authentication Providers OpenShift supports a number of different authentication providers, and you can find the complete list in the understanding identity provider configuration. One of the most commonly used authentication providers is LDAP, whether provided by Microsoft Active Directory or by other sources. OpenShift can perform user authentication against an LDAP server, and can also configure group membership and certain RBAC attributes based on LDAP group membership. Background: LDAP Structure In this environment we are providing LDAP with the following user groups: ocp-user: Users with OpenShift access Any users who should be able to log-in to OpenShift must be members of this group All of the below mentioned users are in this group ocp-normal-dev: Normal OpenShift users Regular users of OpenShift without special permissions Contains: normaluser1, teamuser1, teamuser2 ocp-fancy-dev: Fancy OpenShift users Users of OpenShift that are granted some special privileges Contains: fancyuser1, fancyuser2 ocp-teamed-app: Teamed app users A group of users that will have access to the same OpenShift Project Contains: teamuser1, teamuser2 Examine the OAuth configuration Since this is a pure, vanilla OpenShift 4 installation, it has the default OAuth resource. You can examine that OAuth configuration with the following: oc get oauth cluster -o yaml You will see something like: apiVersion: config.openshift.io/v1 kind: OAuth metadata: annotations: release.openshift.io/create-only: \"true\" creationTimestamp: \"2020-03-17T18:12:52Z\" generation: 1 name: cluster resourceVersion: \"1563\" selfLink: /apis/config.openshift.io/v1/oauths/cluster uid: ebb0582d-b0e4-4c40-a33f-12459593f8e2 spec: {} There are a few things to note here. Firstly, there's basically nothing here! How does the kubeadmin user work, then? The OpenShift OAuth system knows to look for a kubeadmin Secret in the kube-system Namespace. You can examine it with the following: oc get secret -n kube-system kubeadmin -o yaml You will see something like: apiVersion: v1 data: kubeadmin: JDJhJDEwJDdQNHZtbXMxdmpDa3FsNlJMLjJBcC5BSWdBazB6d09IWUdXZEdrRXBERGRwWXNmVVcxanpX kind: Secret metadata: creationTimestamp: \"2019-04-29T17:30:51Z\" name: kubeadmin namespace: kube-system resourceVersion: \"2065\" selfLink: /api/v1/namespaces/kube-system/secrets/kubeadmin uid: 892945dc-6aa4-11e9-9959-02774c6d6b2e type: Opaque That Secret contains the encoded hash of the kubeadmin password. This account will continue to work even after we configure a new OAuth. If you want to disable it, you would need to delete the secret. In a real-world environment, you will likely want to integrate with your existing identity management solution. For this lab we are configuring LDAP as our identityProvider. Here's an example of the OAuth configuration. Look for the element in identityProviders with type: LDAP like the following: apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldap challenge: false login: true mappingMethod: claim type: LDAP ldap: attributes: id: - distinguishedName email: - userPrincipalName name: - givenName preferredUsername: - sAMAccountName bindDN: \"cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com\" bindPassword: name: ldapuser-secret insecure: true url: \"ldap://ad1.dcloud.demo.com:389/cn=Users,dc=dcloud,dc=demo,dc=com?sAMAccountName?sub?(memberOf=cn=ocp-user,cn=Users,dc=dcloud,dc=demo,dc=com)\" tokenConfig: accessTokenMaxAgeSeconds: 86400 Some notable fields under identityProviders:: name: The unique ID of the identity provider. It is possible to have multiple authentication providers in an OpenShift environment, and OpenShift is able to distinguish between them. mappingMethod: claim: This section has to do with how usernames are assigned within an OpenShift cluster when multiple providers are configured. See the Identity provider parameters section for more information. attributes: This section defines the LDAP fields to iterate over and assign to the fields in the OpenShift user's \"account\". If any attributes are not found / not populated when searching through the list, the entire authentication fails. In this case we are creating an identity that is associated with the AD distinguishedName, an email address from the LDAP userPrincipalName, a name from the LDAP givenName, and a username from the AD sAMAccountName. bindDN: When searching LDAP, bind to the server as this user. bindPassword: Reference to the Secret that has the password to use when binding for searching. url: Identifies the LDAP server and the search to perform. For more information on the specific details of LDAP authentication in OpenShift you can refer to the Configuring an LDAP identity provider documentation. To setup the LDAP identity provider we must: Create a Secret with the bind password. Update the cluster OAuth object with the LDAP identity provider. As the kubeadmin user apply the OAuth configuration with oc. oc create secret generic ldapuser-secret --from-literal=bindPassword=b1ndP^ssword -n openshift-config cat Syncing LDAP Groups to OpenShift Groups In OpenShift, groups can be used to manage users and control permissions for multiple users at once. There is a section in the documentation on how to sync groups with LDAP. Syncing groups involves running a program called groupsync when logged into OpenShift as a user with cluster-admin privileges, and using a configuration file that tells OpenShift what to do with the users it finds in the various groups. We have provided a groupsync configuration file for you: View configuration file kind: LDAPSyncConfig apiVersion: v1 url: ldap://ad1.dcloud.demo.com:389 insecure: true bindDN: cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com bindPassword: b1ndP^ssword rfc2307: groupsQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (cn=ocp-*) scope: sub pageSize: 0 groupUIDAttribute: distinguishedName groupNameAttributes: - cn groupMembershipAttributes: - member usersQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (objectclass=user) scope: sub pageSize: 0 userUIDAttribute: distinguishedName userNameAttributes: - sAMAccountName Without going into too much detail (you can look at the documentation), the groupsync config file does the following: searches LDAP using the specified bind user and password queries for any LDAP groups whocp name begins with ocp- creates OpenShift groups with a name from the cn of the LDAP group finds the members of the LDAP group and then puts them into the created OpenShift group uses the dn and uid as the UID and name attributes, respectively, in OpenShift Execute the groupsync: cat groupsync.yaml kind: LDAPSyncConfig apiVersion: v1 url: ldap://ad1.dcloud.demo.com:389 insecure: true bindDN: cn=ldapuser,cn=Users,dc=dcloud,dc=demo,dc=com bindPassword: b1ndP^ssword rfc2307: groupsQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (cn=ocp-*) scope: sub pageSize: 0 groupUIDAttribute: distinguishedName groupNameAttributes: - cn groupMembershipAttributes: - member usersQuery: baseDN: cn=Users,dc=dcloud,dc=demo,dc=com derefAliases: never filter: (objectclass=user) scope: sub pageSize: 0 userUIDAttribute: distinguishedName userNameAttributes: - sAMAccountName EOF oc adm groups sync --sync-config=./groupsync.yaml --confirm You will see output like the following: group/ocp-fancy-dev group/ocp-user group/ocp-normal-dev group/ocp-teamed-app What you are seeing is the Group objects that have been created by the groupsync command. If you are curious about the --confirm flag, check the output of the help with oc adm groups sync -h. If you want to see the Groups that were created, execute the following: oc get groups You will see output like the following: NAME USERS ocp-admin ldapuser ocp-fancy-dev fancyuser1, fancyuser2 ocp-normal-dev normaluser1, teamuser1, teamuser2 ocp-teamed-app teamuser1, teamuser2 ocp-user fancyuser1, fancyuser2, normaluser1, teamuser1, teamuser2 Take a look at a specific group in YAML: oc get group ocp-fancy-dev -o yaml The YAML looks like: apiVersion: user.openshift.io/v1 kind: Group metadata: annotations: openshift.io/ldap.sync-time: 2020-03-11T10:57:03-0400 openshift.io/ldap.uid: cn=ocp-fancy-dev,ou=Users,o=5e615ba46b812e7da02e93b5,dc=jumpcloud,dc=com openshift.io/ldap.url: ldap.jumpcloud.com:636 creationTimestamp: \"2020-03-11T14:57:03Z\" labels: openshift.io/ldap.host: ldap.jumpcloud.com name: ocp-fancy-dev resourceVersion: \"48481\" selfLink: /apis/user.openshift.io/v1/groups/ocp-fancy-dev uid: 630a9d2b-b577-46bd-8294-6b26e7f9a6e1 users: - fancyuser1 - fancyuser2 OpenShift has automatically associated some LDAP metadata with the Group, and has listed the users who are in the group. What happens if you list the Users? oc get user You will get: No resources found. Why would there be no Users found? They are clearly listed in the Group definition. Users are not actually created until the first time they try to log in. What you are seeing in the Group definition is simply a placeholder telling OpenShift that, if it encounters a User with that specific ID, that it should be associated with the Group. Change Group Policy We will grant a cluster role cluster-admin to ldap group ocp-admin Change the policy for the ocp-admin Group: oc adm policy add-cluster-role-to-group cluster-admin ocp-admin In your environment, there is a special group of super developers called ocp-fancy-dev who should have special cluster-reader privileges. This is a role that allows a user to view administrative-level information about the cluster. For example, they can see the list of all Projects in the cluster. Change the policy for the ocp-fancy-dev Group: oc adm policy add-cluster-role-to-group cluster-reader ocp-fancy-dev Note: If you are interested in the different roles that come with OpenShift, you can learn more about them in the role-based access control (RBAC) documentation. Examine cluster-admin policy login as a ldapuser oc login -u ldapuser -p b1ndP^ssword Then, try to list Projects: oc get projects You will see a full list of projects. Examine cluster-reader policy Go ahead and login as a regular user: oc login -u normaluser1 -p openshift Then, try to list Projects: oc get projects You will see: No resources found. Now, login as a member of ocp-fancy-dev: oc login -u fancyuser1 -p openshift And then perform the same oc get projects and you will now see the list of all of the projects in the cluster: NAME DISPLAY NAME STATUS app-management * default kube-public kube-system labguide openshift openshift-apiserver ... You should now be starting to understand how RBAC in OpenShift Container Platform can work. Create Projects for Collaboration Make sure you login as the cluster administrator: oc login -u ldapuser Then, create several Projects for people to collaborate: oc adm new-project app-dev --display-name=\"Application Development\" oc adm new-project app-test --display-name=\"Application Testing\" oc adm new-project app-prod --display-name=\"Application Production\" You have now created several Projects that represent a typical Software Development Lifecycle setup. Next, you will configure Groups to grant collaborative access to these projects. Note: Creating projects with oc adm new-project does not use the project request process or the project request template. These projects will not have quotas or limitranges applied by default. A cluster administrator can \"impersonate\" other users, so there are several options if you wanted these projects to get quotas/limit ranges: . use --as to specify impersonating a regular user with oc new-project . use oc process and provide values for the project request template, piping into create (eg: oc process ... | oc create -f -). This will create all of the objects in the project request template, which would include the quota and limit range. . manually create/define the quota and limit ranges after creating the projects. For these exercises it is not important to have quotas or limit ranges on these projects. Map Groups to Projects As you saw earlier, there are several roles within OpenShift that are preconfigured. When it comes to Projects, you similarly can grant view, edit, or administrative access. Let's give our ocp-teamed-app users access to edit the development and testing projects: oc adm policy add-role-to-group edit ocp-teamed-app -n app-dev oc adm policy add-role-to-group edit ocp-teamed-app -n app-test And then give them access to view production: oc adm policy add-role-to-group view ocp-teamed-app -n app-prod Now, give the ocp-fancy-dev group edit access to the production project: oc adm policy add-role-to-group edit ocp-fancy-dev -n app-prod Examine Group Access Log in as normaluser1 and see what Projects you can see: oc login -u normaluser1 -p openshift oc get projects You should get: No resources found. Then, try teamuser1 from the ocp-teamed-app group: oc login -u teamuser1 -p openshift oc get projects You should get: NAME DISPLAY NAME STATUS app-dev Application Development Active app-prod Application Production Active app-test Application Testing Active You did not grant the team users edit access to the production project. Go ahead and try to create something in the production project as teamuser1: oc project app-prod oc new-app docker.io/siamaksade/mapit You will see that it will not work: error: can't lookup images: imagestreamimports.image.openshift.io is forbidden: User \"teamuser1\" cannot create resource \"imagestreamimports\" in API group \"image.openshift.io\" in the namespace \"app-prod\" error: local file access failed with: stat docker.io/siamaksade/mapit: no such file or directory error: unable to locate any images in image streams, templates loaded in accessible projects, template files, local docker images with name \"docker.io/siamaksade/mapit\" Argument 'docker.io/siamaksade/mapit' was classified as an image, image~source, or loaded template reference. The 'oc new-app' command will match arguments to the following types: 1. Images tagged into image streams in the current project or the 'openshift' project - if you don't specify a tag, we'll add ':latest' 2. Images in the Docker Hub, on remote registries, or on the local Docker engine 3. Templates in the current project or the 'openshift' project 4. Git repository URLs or local paths that point to Git repositories --allow-missing-images can be used to point to an image that does not exist yet. See 'oc new-app -h' for examples. This failure is exactly what we wanted to see. Prometheus Now that you have a user with cluster-reader privileges (one that can see many administrative aspects of the cluster), we can revisit Prometheus and attempt to log-in to it. Login as a the user with cluster-reader privileges: oc login -u fancyuser1 -p openshift Find the prometheus Route with the following command: oc get route prometheus-k8s -n openshift-monitoring You will see something like the following: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD prometheus-k8s prometheus-k8s-openshift-monitoring.{{ ROUTE_SUBDOMAIN }} prometheus-k8s web reencrypt/Redirect None Warning: Before continuing, make sure to go to the OpenShift web console and log out by using the dropdown menu at the upper right where it says kube:admin. Otherwise Prometheus will try to use your kubeadmin user to pass through authentication. While it will work, it doesn't demonstrate the cluster-reader role. The installer configured a Route for Prometheus by default. Go ahead and control+click the Prometheus link to open it in your browser. You'll be greeted with a login screen. Click the Log in with OpenShift button, then select the ldap auth mechanism, and use the fancyuser1 user that you gave cluster-reader privileges to earlier. More specifically, the ocp-fancy-dev group has cluster-reader permissions, and fancyuser1 is a member. Remember that the password for all of these users is openshift. You will probably get a certificate error because of the self-signed certificate. Make sure to accept it. After logging in, the first time you will be presented with an auth proxy permissions acknowledgement. There is actually an OAuth proxy that sits in the flow between you and the Prometheus container. This proxy is used to validate your AuthenticatioN (AuthN) as well as authorize (AuthZ) what is allowed to happen. Here you are explicitly authorizing the permissions from your fancyuser1 account to be used as part of accessing Prometheus. Hit Allow selected permissions. At this point you are viewing Prometheus. There are no alerts configured. If you look at Status and then Targets you can see some interesting information about the current state of the cluster. "},"infrastructure-infra-nodes.html":{"url":"infrastructure-infra-nodes.html","title":"OpenShift MachineSet and Infrastructure Nodes","keywords":"","body":"OpenShift Infrastructure Nodes OpenShift Infrastructure Nodes Prerequisites OpenShift Infrastructure Nodes OpenShift MachineSet Defining a Custom MachineSet Logging Machine Config MachineConfig overview Checking Machine Config Pool status Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI Resources enough to provision 3 infra nodes e.g. 4vCPU, 16GB RAM per node OpenShift Infrastructure Nodes The OpenShift subscription model allows customers to run various core infrastructure components at no additional charge. In other words, a node that is only running core OpenShift infrastructure components is not counted in terms of the total number of subscriptions required to cover the environment. OpenShift components that fall into the infrastructure categorization include: kubernetes and OpenShift control plane services (\"masters\") router container image registry cluster metrics collection (\"monitoring\") cluster aggregated logging service brokers Any node running a container/pod/component not described above is considered a worker and must be covered by a subscription. OpenShift MachineSet In the case of an infrastructure node, we want to create additional Machines that have specific Kubernetes labels. Then, we can configure the various infrastructure components to run specifically on nodes with those labels. To accomplish this, you will create additional MachineSets. In order to understand how MachineSets work, run the following. This will allow you to follow along with some of the following discussion. oc get machineset -n openshift-machine-api -o yaml $(oc get machineset -n openshift-machine-api | grep worker | cut -d' ' -f 1) Sample Output apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: creationTimestamp: \"2020-12-28T05:02:31Z\" generation: 3 labels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 name: ocp01-7k4c4-worker namespace: openshift-machine-api resourceVersion: \"799241\" selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/ocp01-7k4c4-worker uid: 52ad683d-99b6-423b-b045-4279f241640e spec: replicas: 1 selector: matchLabels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 machine.openshift.io/cluster-api-machineset: ocp01-7k4c4-worker template: metadata: labels: machine.openshift.io/cluster-api-cluster: ocp01-7k4c4 machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker machine.openshift.io/cluster-api-machineset: ocp01-7k4c4-worker spec: metadata: {} providerSpec: value: apiVersion: vsphereprovider.openshift.io/v1beta1 credentialsSecret: name: vsphere-cloud-credentials diskGiB: 120 kind: VSphereMachineProviderSpec memoryMiB: 8192 metadata: creationTimestamp: null network: devices: - networkName: VM Network numCPUs: 2 numCoresPerSocket: 2 snapshot: \"\" template: ocp01-7k4c4-rhcos userDataSecret: name: worker-user-data workspace: datacenter: dCloud-DC datastore: NFS_Datastore folder: /dCloud-DC/vm/ocp01-7k4c4 resourcePool: /dCloud-DC/host/dCloud-Cluster/Resources server: vc1.dcloud.cisco.com status: availableReplicas: 1 fullyLabeledReplicas: 1 observedGeneration: 3 readyReplicas: 1 replicas: 1 Important information in MachineSet Metadata The metadata on the MachineSet itself includes information like the name of the MachineSet and various labels. Selector The MachineSet defines how to create Machines, and the Selector tells the operator which machines are associated with the set Template Metadata The template is the part of the MachineSet that templates out the Machine. The template itself can have metadata associated, and we need to make sure that things match here when we make changes: Template Spec The template needs to specify how the Machine/Node should be created. You will notice that the spec and, more specifically, the providerSpec contains all of the important AWS data to help get the Machine created correctly and bootstrapped. In our case, we want to ensure that the resulting node inherits one or more specific labels. As you’ve seen in the examples above, labels go in metadata sections: Defining a Custom MachineSet Now that you’ve analyzed an existing MachineSet it’s time to go over the rules for creating one, at least for a simple change like we’re making: Don’t change anything in the providerSpec Don’t change any instances of machine.openshift.io/cluster-api-cluster: Give your MachineSet a unique name Make sure any instances of machine.openshift.io/cluster-api-machineset match the name Add labels you want on the nodes to .spec.template.spec.metadata.labels Even though you’re changing MachineSet name references, be sure not to change the subnet. This sounds complicated, but we have a template that will do the hard work for you: export CLUSTERID=$(oc get machineset -n openshift-machine-api | grep worker | cut -d' ' -f 1 | sed 's/-worker//g') cat oc get machineset -n openshift-machine-api You should see the new infra set listed with a name similar to the following: ocp01-7k4c4-infra 1 1 13s We don’t yet have any ready or available machines in the set because the instances are still coming up and bootstrapping. You can check oc get machine -n openshift-machine-api to see when the instance finally starts running. Then, you can use oc get node to see when the actual node is joined and ready. Note: It can take several minutes for a Machine to be prepared and added as a Node. oc get nodes NAME STATUS ROLES AGE VERSION ocp01-7k4c4-infra-tz8w4 Ready infra,worker 18m v1.19.0+7070803 ocp01-7k4c4-master-0 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-master-1 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-master-2 Ready master 2d v1.19.0+7070803 ocp01-7k4c4-worker-wbq9b Ready worker 56m v1.19.0+7070803 ocp01-7k4c4-worker-zw9w8 Ready worker 2d v1.19.0+7070803 If you’re having trouble figuring out which node is the new one, take a look at the AGE column. It will be the youngest! Also, in the ROLES column you will notice that the new node has both a worker and an infra role. For the HA, we will need 3 infra nodes. export $INFRAMS=$(oc get machineset -n openshift-machine-api | grep infra | cut -d' ' -f 1) oc scale machineset $INFRAMS -n openshift-machine-api --replicas=3 Binding infrastructure node workloads using taints and tolerations If you have an infra node that has the infra and worker roles assigned, you must configure the node so that user workloads are not assigned to it. Use the following command to add a taint to the infra node to prevent scheduling user workloads on it: for node in $(oc get nodes | grep infra | cut -d' ' -f 1 ); do oc adm taint nodes $node node-role.kubernetes.io/infra:NoSchedule done Check the Labels We can ask what its labels are by using command: oc get nodes --show-labels -l node-role.kubernetes.io/infra= Output NAME STATUS ROLES AGE VERSION LABELS ocp01-7k4c4-infra-tz8w4 Ready infra,worker 10m v1.19.0+7070803 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ocp01-7k4c4-infra-tz8w4,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos Moving Infrastructure Components Now that you have infra nodes, it’s time to move various infrastructure components onto them. Router The OpenShift router is managed by an Operator called openshift-ingress-operator. Its Pod lives in the openshift-ingress-operator project: Registry The registry uses a similar CRD mechanism to configure how the operator deploys the actual registry pods. That CRD is configs.imageregistry.operator.openshift.io. You will edit the cluster CR object in order to add the nodeSelector oc patch configs.imageregistry.operator.openshift.io/cluster -p '{\"spec\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\": \"NoSchedule\",\"key\": \"node-role.kubernetes.io/infra\",\"operator\": \"Exists\"}]}}' --type=merge Monitoring The Cluster Monitoring operator is responsible for deploying and managing the state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is installed by default during the initial cluster installation. Its operator uses a ConfigMap in the openshift-monitoring project to set various tunables and settings for the behavior of the monitoring stack. The following ConfigMap definition will configure the monitoring solution to be redeployed onto infrastructure nodes. apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \"\" grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" Logging OpenShift’s log aggregation solution is not installed by default. There is a dedicated lab exercise that goes through the configuration and deployment of logging. Machine Config There are times when you need to make changes to the operating systems running on OpenShift Container Platform nodes. This can include changing settings for network time service, adding kernel arguments, or configuring journaling in a specific way. MachineConfig overview The Machine Config Operator (MCO) manages updates to systemd, CRI-O and Kubelet, the kernel, Network Manager and other system features. It also offers a MachineConfig CRD that can write configuration files onto the host. Understanding what MCO does and how it interacts with other components is critical to making advanced, system-level changes to an OpenShift Container Platform cluster. Here are some things you should know about MCO, MachineConfigs, and how they are used: A MachineConfig can make a specific change to a file or service on the operating system of each system representing a pool of OpenShift Container Platform nodes. MCO is only supported for writing to files in /etc and /var directories, although there are symbolic links to some directories that can be writeable by being symbolically linked to one of those areas. The /opt directory is an example. Ignition is the configuration format used in MachineConfigs. See the Ignition Configuration Specification v3.1.0 for details. Checking Machine Config Pool status To see the status of the Machine Config Operator, its sub-components, and the resources it manages, use the following oc commands: Procedure To see the number of MCO-managed nodes available on your cluster for each pool, type: oc get machineconfigpool result NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE master rendered-master-dd… True False False 3 3 3 0 4h42m worker rendered-worker-fde… True False False 3 3 3 0 4h42m In the previous output, there are three master and three worker nodes. All machines are updated and none are currently updating. Because all nodes are Updated and Ready and none are Degraded, you can ell that there are no issues. To see each existing machineconfig, type: oc get machineconfigs result NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 00-worker 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 01-master-container-runtime 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m 01-master-kubelet 2c9371fbb673b97a6fe8b1c52… 3.1.0 5h18m ... rendered-master-dde... 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m rendered-worker-fde... 2c9371fbb673b97a6fe8b1c52... 3.1.0 5h18m Note that the machineconfigs listed as rendered are not meant to be changed or deleted. Expect them to be hidden at some point in the future. Check the status of worker (or change to master) to see the status of that pool of nodes: `` oc describe mcp worker ... Degraded Machine Count: 0 Machine Count: 3 Observed Generation: 2 Ready Machine Count: 3 Unavailable Machine Count: 0 Updated Machine Count: 3 Events: You can view the contents of a particular machineconfig (in this case, 01-master-kubelet). The trimmed output from the following oc describe command shows that this machineconfig contains both configuration files (cloud.conf and kubelet.conf) and a systemd service (Kubernetes Kubelet): oc describe machineconfigs 01-master-kubelet result Name: 01-master-kubelet ... Spec: Config: Ignition: Version: 3.1.0 Storage: Files: Contents: Source: data:, Mode: 420 Overwrite: true Path: /etc/kubernetes/cloud.conf Contents: Source: data:,kind%3A%20KubeletConfiguration%0AapiVersion%3A%20kubelet.config.k8s.io%2Fv1beta1%0Aauthentication%3A%0A%20%20x509%3A%0A%20%20%20%20clientCAFile%3A%20%2Fetc%2Fkubernetes%2Fkubelet-ca.crt%0A%20%20anonymous... Mode: 420 Overwrite: true Path: /etc/kubernetes/kubelet.conf Systemd: Units: Contents: [Unit] Description=Kubernetes Kubelet Wants=rpc-statd.service network-online.target crio.service After=network-online.target crio.service ExecStart=/usr/bin/hyperkube \\ kubelet \\ --config=/etc/kubernetes/kubelet.conf \\ ...\\ Using MachineConfigs to configure nodes Tasks in this section let you create MachineConfig objects to modify files, systemd unit files, and other operating system features running on OpenShift Container Platform nodes. For more ideas on working with MachineConfigs, see content related to changing MTU network settings, adding or updating SSH authorized keys, , replacing DNS nameservers, verifying image signatures, enabling SCTP, and configuring iSCSI initiatornames for OpenShift Container Platform. MachineConfigs OpenShift Container Platform version 4.6 supports Ignition specification version 3.1. All new MachineConfigs you create going forward should be based on Ignition specification version 3.1. If you are upgrading your OpenShift Container Platform cluster, any existing Ignition specification version 2.x MachineConfigs will be translated automatically to specification version 3.1. Configuring chrony time service You can set the time server and related settings used by the chrony time service (chronyd) by modifying the contents of the chrony.conf file and passing those contents to your nodes as a machine config. Procedure Create the contents of the chrony.conf file and encode it as base64. Specify any valid, reachable time source. For example: cat chrony.conf pool 198.18.128.1 iburst driftfile /var/lib/chrony/drift rtcsync makestep 1.0 3 logdir /var/log/chrony EOF Create the MachineConfig object file, replacing the base64 string with the one you just created yourself. This example adds the file to master nodes. You can change it to worker or make an additional MachineConfig object for the worker role: cat ./99_masters-chrony-configuration.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: master name: 99-master-etc-chrony-conf spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 3.1.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat chrony.conf | base64 -w 0) group: name: root mode: 420 overwrite: true path: /etc/chrony.conf user: name: root osImageURL: \"\" EOF cat ./99_workers-chrony-configuration.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: worker name: 99-worker-etc-chrony-conf spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 3.1.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat chrony.conf | base64 -w 0) group: name: root mode: 420 overwrite: true path: /etc/chrony.conf user: name: root osImageURL: \"\" EOF Make a backup copy of the configuration file. Apply the configuration in one of two ways: If the cluster is not up yet, generate manifest files, add this file to the openshift directory, and then continue to create the cluster. Example: generate the cluster manifests: ./openshift-install create manifests --dir install-config/ --log-level debug And then copy the 2 files into install_dir/manifests: cp 99_masters-chrony-configuration.yaml install-config/manifests/. cp 99_workers-chrony-configuration.yaml install-config/manifests/. Then, deploy the cluster: ./openshift-install create cluster --dir install-config/ --log-level debug If the cluster is already running, apply the file as follows: oc apply -f ./99_masters-chrony-configuration.yaml oc apply -f ./99_workers-chrony-configuration.yaml "},"infrastructure-monitoring-alerts.html":{"url":"infrastructure-monitoring-alerts.html","title":"OpenShift Platform Monitoring and Alert","keywords":"","body":"Cluster Monitoring and Alerts Cluster Monitoring and Alerts Prerequisites OpenShift Mornitering and Alert Configuring the monitoring stack Managing alerts Sending notifications to external systems Tesing Alerts Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI OpenShift installer Node subnet with DHCP pool DNS NTP OpenShift Mornitering and Alert OpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. OpenShift Container Platform delivers monitoring best practices out of the box. A set of alerts are included by default that immediately notify cluster administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. The OpenShift Container Platform monitoring stack is based on the Prometheus open source project and its wider ecosystem. The monitoring stack includes the following: Default platform monitoring components. A set of platform monitoring components are installed in the openshift-monitoring project by default during an OpenShift Container Platform installation. This provides monitoring for core OpenShift Container Platform components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters. These components are illustrated in the Installed by default section in the following diagram. Components for monitoring user-defined projects. After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the openshift-user-workload-monitoring project. This provides monitoring for user-defined projects. Default monitoring targets In addition to the components of the stack itself, the default monitoring stack monitors: CoreDNS Elasticsearch (if Logging is installed) etcd Fluentd (if Logging is installed) HAProxy Image registry Kubelets Kubernetes apiserver Kubernetes controller manager Kubernetes scheduler Metering (if Metering is installed) OpenShift apiserver OpenShift controller manager Operator Lifecycle Manager (OLM) Configuring the monitoring stack The OpenShift Container Platform 4 installation program provides only a low number of configuration options before installation. Configuring most OpenShift Container Platform framework components, including the cluster monitoring stack, happens post-installation. You can configure the monitoring stack by creating and updating monitoring config maps. Procedure Check whether the cluster-monitoring-config ConfigMap object exists: oc -n openshift-monitoring get configmap cluster-monitoring-config If the ConfigMap object does not exist: Create and apply the following YAML manifest. In this example the file is called cluster-monitoring-config.yaml: cat Managing alerts In OpenShift Container Platform 4.6, the Alerting UI enables you to manage alerts, silences, and alerting rules. Alerting rules. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed. Alerts. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an OpenShift Container Platform cluster. Silences. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue. Understanding alert filters In the Administrator perspective, the Alerts page in the Alerting UI provides details about alerts relating to default OpenShift Container Platform and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown. You can filter by alert state, severity, and source. By default, only Platform alerts that are Firing are displayed. The following describes each alert filtering option: Alert State filters: Firing. The alert is firing because the alert condition is true and the optional for duration has passed. The alert will continue to fire as long as the condition remains true. Pending. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires. Silenced. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions. Severity filters: Critical. The condition that triggered the alert could have a critical impact. The alert requires immediate attention when fired and is typically paged to an individual or to a critical response team. Warning. The alert provides a warning notification about something that might require attention in order to prevent a problem from occurring. Warnings are typically routed to a ticketing system for non-immediate review. Info. The alert is provided for informational purposes only. None. The alert has no defined severity. You can also create custom severity definitions for alerts relating to user-defined projects. Source filters: Platform. Platform-level alerts relate only to default OpenShift Container Platform projects. These projects provide core OpenShift Container Platform functionality. User. User alerts relate to user-defined projects. These alerts are user-created and are customizable. User-defined workload monitoring can be enabled post-installation to provide observability into your own workloads. Sending notifications to external systems In OpenShift Container Platform 4.6, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure OpenShift Container Platform to send alerts to the following receiver types: PagerDuty Webhook Email Slack Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review. **Set up Test Mail Server and Slack Channel Before we configure the AlertManager to send email and slack alert, we will need to run a webmail with smtp and slack channel for testing Deploy the maildev in default namespace and expose webmail oc project default oc new-app --docker-image=maildev/maildev --name='maildev' oc expose service/maildev --port=80 From this deployment you can reach smtp via maildev.default.svc.cluster.local:25 and webmail from route host using command oc get route Create Slack Channel webhooks token, or use prepared token you have Webhook notification with line-notify-gateway You will need to setup Line Notification service to allow Line Notifications in your chat group. After you setup you will get your token to use in AlertManager webhook to line-notify-gateway container, which is provided as an example for receive AlertManager webhook json and send to Line Notification Service. oc -n default new-app --docker-image=nontster/line-notify-gateway oc -n default get svc Configuring alert receivers Procedure In the Administrator perspective, navigate to Administration → Cluster Settings → Global Configuration → Alertmanager. You can apply Alert Receivers configuration example that includes receivers for smtp to webmail, slack and line-notification-gateway. cat - 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 type: Opaque EOF Configuration content in plain-text global: resolve_timeout: 5m smtp_from: ocp@example.com smtp_smarthost: 'maildev.default.svc.cluster.local:25' smtp_hello: localhost smtp_require_tls: false slack_api_url: >- https://hooks.slack.com/services/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 inhibit_rules: - equal: - namespace - alertname source_match: severity: critical target_match_re: severity: warning|info - equal: - namespace - alertname source_match: severity: warning target_match_re: severity: info receivers: - name: Critical email_configs: - to: admin@example.com - name: Default email_configs: - to: admin@example.com - name: slack slack_configs: - channel: '#alert' - name: slack-warning slack_configs: - channel: '#alert' - name: slack-watchdog slack_configs: - channel: '#alert' - name: Warning email_configs: - to: warning@example.com - name: Watchdog email_configs: - to: watchdog@example.com route: group_by: - namespace group_interval: 5m group_wait: 30s receiver: Default repeat_interval: 1h routes: - receiver: Watchdog match: alertname: Watchdog - receiver: Critical match: severity: critical - receiver: Warning match: severity: warning - receiver: slack match: severity: info - receiver: slack-watchdog match: alertname: Watchdog - receiver: slack-warning match: severity: warning - receiver: line-warning match: severity: warning - receiver: line-watchdog match: alertname: Watchdog Tesing Alerts We will demontrate Alerts from Prometheus and AlertManager by using Platform and User Workload application with metrics from cpu, memory, persistent volume resource. Procedures Deploy a sample httpd pod with persistent volume 1G size mount to /httpd-pvc path oc new-project user1 cat rsh to httpd pod to check PV size and use fallocate command to make PV high utilization (>95%) oc rsh $(oc -n user1 get pod | grep httpd | cut -d' ' -f1) check volume utilization df -h result, /httpd-pvc, Use 1% Filesystem Size Used Avail Use% Mounted on overlay 120G 19G 102G 16% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 47M 3.9G 2% /etc/passwd /dev/sdd 976M 2.6M 958M 1% /httpd-pvc /dev/mapper/coreos-luks-root-nocrypt 120G 19G 102G 16% /etc/hosts tmpfs 3.9G 28K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware use fallocate to create a file with 950M size cd /httpd-pvc/ fallocate -l 950M file.tmp re-check /httpd-pvc again, now it is Use 100% df -h result Filesystem Size Used Avail Use% Mounted on overlay 120G 19G 102G 16% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 47M 3.9G 2% /etc/passwd /dev/sdd 976M 953M 7.4M 100% /httpd-pvc /dev/mapper/coreos-luks-root-nocrypt 120G 19G 102G 16% /etc/hosts tmpfs 3.9G 28K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware For the User Workload Monitoring, we will need to add rolebinding to enabled permission to edit/view PrometheusRule, ServiceMonitor and PodMonitor. Let's add monitoring-edit role to user ldapuser oc policy add-role-to-user monitoring-edit ldapuser -n user1 Create Prometheus Rule to alert when PV utilization is >80% in namespace user1 cat Now we can go back to OpenShift Console Developer View, go to user1 project > Monitoring > Alerts We will see our user defined alert rule and alert that trigger You can click on Alert and see the details When alert is Firing, firing alerts will also route to external notifications defined by AlertManager "},"infrastructure-cluster-logging.html":{"url":"infrastructure-cluster-logging.html","title":"OpenShift Cluster Logging","keywords":"","body":"OpenShift Infrastructure Nodes OpenShift Infrastructure Nodes Prerequisites OpenShift Cluster Logging Deploying OpenShift Logging Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI done the OpenShift Infra Nodes provisioning OpenShift Cluster Logging In this lab you will explore the logging aggregation capabilities of OpenShift. An extremely important function of OpenShift is collecting and aggregating logs from the environments and the application pods it is running. OpenShift ships with an elastic log aggregation solution: EFK. (ElasticSearch, Fluentd and Kibana) The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). The collector, Fluentd, is deployed to each node in the OpenShift cluster. It collects all node and container logs and writes them to Elasticsearch (ES). Kibana is the centralized, web UI where users and administrators can create rich visualizations and dashboards with the aggregated data. Administrators can see and search through all logs. Application owners and developers can allow access to logs that belong to their projects. The EFK stack runs on top of OpenShift. Warning This lab requires that you have completed the infra-nodes lab. The logging stack will be installed on the infra nodes that were created in that lab. Deploying OpenShift Logging OpenShift Container Platform cluster logging is designed to be used with the default configuration, which is tuned for small to medium sized OpenShift Container Platform clusters. The installation instructions that follow include a sample Cluster Logging Custom Resource (CR), which you can use to create a cluster logging instance and configure your cluster logging deployment. If you want to use the default cluster logging install, you can use the sample CR directly. If you want to customize your deployment, make changes to the sample CR as needed. The following describes the configurations you can make when installing your cluster logging instance or modify after installtion. See the Configuring sections for more information on working with each component, including modifications you can make outside of the Cluster Logging Custom Resource. Create the openshift-logging namespace OpenShift Logging will be run from within its own namespace openshift-logging. This namespace does not exist by default, and needs to be created before logging may be installed. The namespace is represented in yaml format as: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-logging: \"true\" openshift.io/cluster-monitoring: \"true\" To create the namespace, run the following command: cat Install the Elasticsearch and Cluster Logging Operators in the cluster In order to install and configure the EFK stack into the cluster, additional operators need to be installed. These can be installed from the Operator Hub from within the cluster via the GUI. When using operators in OpenShift, it is important to understand the basics of some of the underlying principles that make up the Operators. CustomResourceDefinion (CRD) and CustomResource (CR) are two Kubernetes objects that we will briefly describe.CRDs are generic pre-defined structures of data. The operator understands how to apply the data that is defined by the CRD. In terms of programming, CRDs can be thought as being similar to a class. CustomResource (CR) is an actual implementations of the CRD, where the structured data has actual values. These values are what the operator will use when configuring it’s service. Again, in programming terms, CRs would be similar to an instantiated object of the class. The general pattern for using Operators is first, install the Operator, which will create the necessary CRDs. After the CRDs have been created, we can create the CR which will tell the operator how to act, what to install, and/or what to configure. For installing openshift-logging, we will follow this pattern. To begin, log-in to the OpenShift Cluster’s GUI. Then follow the following steps: Install the Elasticsearch Operator: In the OpenShift console, click Operators → OperatorHub. Choose Elasticsearch Operator from the list of available Operators, and click Install. On the Create Operator Subscription page, select Update Channel 4.6, leave all other defaults and then click Subscribe. This makes the Operator available to all users and projects that use this OpenShift Container Platform cluster. Install the Cluster Logging Operator: Note The Cluster Logging operator needs to be installed in the openshift-logging namespace. Please ensure that the openshift-logging namespace was created from the previous steps In the OpenShift console, click Operators → OperatorHub. Choose Cluster Logging from the list of available Operators, and click Install. On the Create Operator Subscription page, Under ensure Installation Mode that A specific namespace on the cluster is selected, and choose openshift-logging. In addition, select Update Channel 4.6 and leave all other defaults and then click Subscribe. Verify the operator installations: Switch to the Operators → Installed Operators page. Make sure the openshift-logging project is selected. In the Status column you should see green checks with either InstallSucceeded or Copied and the text Up to date. Note During installation an operator might display a Failed status. If the operator then installs with an InstallSucceeded message, you can safely ignore the Failed message. Create the Loggging CustomResource (CR) instance Now that we have the operators installed, along with the CRDs, we can now kick off the logging install by creating a Logging CR. This will define how we want to install and configure logging. In the OpenShift Console, switch to the the Administration → Custom Resource Definitions page. On the Custom Resource Definitions page, click ClusterLogging. On the Custom Resource Definition Overview page, select View Instances from the Actions menu Note If you see a 404 error, don’t panic. While the operator installation succeeded, the operator itself has not finished installing and the CustomResourceDefinition may not have been created yet. Wait a few moments and then refresh the page. On the Cluster Loggings page, click Create Cluster Logging. In the YAML editor, replace the code with the following: Note: you need to change storageclass that available in your environment apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" elasticsearch: nodeCount: 3 storage: storageClassName: thin size: 100Gi redundancyPolicy: \"SingleRedundancy\" nodeSelector: node-role.kubernetes.io/infra: \"\" resources: request: memory: 4G visualization: type: \"kibana\" kibana: replicas: 1 nodeSelector: node-role.kubernetes.io/infra: \"\" curation: type: \"curator\" curator: schedule: \"30 3 * * *\" nodeSelector: node-role.kubernetes.io/infra: \"\" collection: logs: type: \"fluentd\" fluentd: {} Then click Create. Verify the Loggging install Now that Logging has been created, let’s verify that things are working. Switch to the Workloads → Pods page. Select the openshift-logging project. You should see pods for cluster logging (the operator itself), Elasticsearch, and Fluentd, and Kibana. Alternatively, you can verify from the command line by using the following command: oc get pods -n openshift-logging You should eventually see something like: NAME READY STATUS RESTARTS AGE cluster-logging-operator-cb795f8dc-xkckc 1/1 Running 0 32m elasticsearch-cdm-b3nqzchd-1-5c6797-67kfz 2/2 Running 0 14m elasticsearch-cdm-b3nqzchd-2-6657f4-wtprv 2/2 Running 0 14m elasticsearch-cdm-b3nqzchd-3-588c65-clg7g 2/2 Running 0 14m fluentd-2c7dg 1/1 Running 0 14m fluentd-9z7kk 1/1 Running 0 14m fluentd-br7r2 1/1 Running 0 14m fluentd-fn2sb 1/1 Running 0 14m fluentd-pb2f8 1/1 Running 0 14m fluentd-zqgqx 1/1 Running 0 14m kibana-7fb4fd4cc9-bvt4p 2/2 Running 0 14m The Fluentd Pods are deployed as part of a DaemonSet, which is a mechanism to ensure that specific Pods run on specific Nodes in the cluster at all times: oc get daemonset -n openshift-logging You will see something like: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd 9 9 9 9 9 kubernetes.io/os=linux 94s You should expect 1 fluentd Pod for every Node in your cluster. Remember that Masters are still Nodes and fluentd will run there, too, to slurp the various logs. You will also see the storage for ElasticSearch has been automatically provisioned. If you query the PersistentVolumeClaim objects in this project you will see the new storage. oc get pvc -n openshift-logging You will see something like: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE elasticsearch-elasticsearch-cdm-ggzilasv-1 Bound pvc-f3239564-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m elasticsearch-elasticsearch-cdm-ggzilasv-2 Bound pvc-f324a252-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m elasticsearch-elasticsearch-cdm-ggzilasv-3 Bound pvc-f326aa7d-389c-11ea-bab2-06ca7918708a 100Gi RWO ocs-storagecluster-ceph-rbd 15m Note Much like with the Metrics solution, we defined the appropriate NodeSelector in the Logging configuration (CR) to ensure that the Logging components only landed on the infra nodes. That being said, the DaemonSet ensures FluentD runs on all nodes. Otherwise we would not capture all of the container logs. Accessing Kibana As mentioned before, Kibana is the front end and the way that users and admins may access the OpenShift Logging stack. To reach the Kibana user interface, first determine its public access URL by querying the Route that got set up to expose Kibana’s Service: To find and access the Kibana route: In the OpenShift console, click on the Networking → Routes page. Select the openshift-logging project. Click on the Kibana route. In the Location field, click on the URL presented. Click through and accept the SSL certificates Alternatively, this can be obtained from the command line: oc get route -n openshift-logging You will see something like: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD kibana kibana-openshift-logging.{{ ROUTE_SUBDOMAIN }} kibana reencrypt/Redirect None There is a special authentication proxy that is configured as part of the EFK installation that results in Kibana requiring OpenShift credentials for access. Because you’ve already authenticated to the OpenShift Console as a cluster-admin user, you will see an administrative view of what Kibana has to show you (which you authorized by clicking the button). Queries with Kibana Once the Kibana web interface is up, we are now able to do queries. Kibana offers a the user a powerful interface to query all logs that come from the cluster. By default, Kibana will show all logs that have been received within the the last 15 minutes. This time interval may be changed in the upper right hand corner. The log messages are shown in the middle of the page. All log messages that are received are indexed based on the log message content. Each message will have fields associated that are associated to that log message. To see the fields that make up an individual message, click on the arrow on the side of each message located in the center of the page. This will show the message fields that are contained. First, set the default index pattern to .all. On the left hand side towards the top, in the drop down menu select the .all index pattern. To select fields to show for messages, look on left hand side fore the Available Fields label. Below this are fields that can be selected and shown in the middle of the screen. Find the hostname field below the Available Fields and click add. Notice now, in the message pain, each message’s hostname is displayed. More fields may be added. Click the add button for kubernetes.pod_name and also for message. To create a query for logs, the Add a filter + link right below the search box may be used. This will allow us to build queries using the fields of the messages. For example, if we wanted to see all log messages from the openshift-logging namespace, we can do the following: Click on Add a filter +. In the Fields input box, start typing kubernetes.namespace_name. Notice all of the available fields that we can use to build the query Next, select is. In the Value field, type in openshift-logging Click the \"Save\" button Now, in the center of the screen you will see all of the logs from all the pods in the openshift-logging namespace. Of course, you may add more filters to refine the query. One other neat option that Kibana allows you to do is save queries to use for later. To save a query do the following: click on Save at the top of the screen. Type in the name you would like to save it as. In this case, let’s type in openshift-logging Namespace Once this has been saved, it can be used at a later time by hitting the Open button and selecting this query. Please take time to explore the Kibana page and get experience by adding and doing more queries. This will be helpful when using a production cluster, you will be able to get the exact logs that you are looking for in a single place. "},"infrastructure-networking.html":{"url":"infrastructure-networking.html","title":"OpenShift Networking","keywords":"","body":"Infrastructure Networking Infrastructure Networking Prerequisites OpenShift Network Policy Based SDN Switch Your Project Execute the Creation Script Examine the created infrastructure Test Connectivity should work Restricting Access Test Connectivity #2 should fail Allow Access Test Connectivity #3 should work again Test Connectivity #4 while chaning NetworkPolicy Egress IP address assignment for project egress traffic Configuring automatically assigned egress IP addresses for a namespace Network Logging Container Platform Network Segmentation Multi-Cluster Level Cluster Namespace separation level Additional Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 OpenShift Network Policy Based SDN OpenShift has a software defined network (SDN) inside the platform that is based on Open vSwitch. This SDN is used to provide connectivity between application components inside of the OpenShift environment. It comes with default network ranges pre-configured, although you can make changes to these should they conflict with your existing infrastructure, or for whatever other reason you may have. The OpenShift Network Policy SDN plug-in allows projects to truly isolate their network infrastructure inside OpenShift’s software defined network. While you have seen projects isolate resources through OpenShift’s RBAC, the network policy SDN plugin is able to isolate pods in projects using pod and namespace label selectors. The network policy SDN plugin was introduced in OpenShift 3.7, and more information about it and its configuration can be found in the link:https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html[networking documentation^]. Additionally, other vendors are working with the upstream Kubernetes community to implement their own SDN plugins, and several of these are supported by the vendors for use with OpenShift. These plugin implementations make use of appc/CNI, which is outside the scope of this lab. Switch Your Project Before continuing, make sure you are using a project that actually exists. If the last thing you did in the previous lab was delete a project, this will cause errors in the scripts in this lab. oc project default Execute the Creation Script Only users with project or cluster administration privileges can manipulate Project networks. Then, execute a script that we have prepared for you. It will create two Projects and then deploy a DeploymentConfig with a Pod for you: oc new-project netproj-a oc label namespace netproj-a app=iperf3-client oc new-project netproj-b oc label namespace netproj-b app=iperf3-server cat Examine the created infrastructure Two Projects were created for you, netproj-a and netproj-b. Execute the following command to see the created resources: oc get pods -n netproj-a After a while you will see something like the following: NAME READY STATUS RESTARTS AGE iperf3-clients-7c566cfdc-7dtn5 1/1 Running 0 14m Similarly: oc get pods -n netproj-b After a while you will see something like the following: NAME READY STATUS RESTARTS AGE iperf3-server-deployment-79c44f8b-6bkrn 1/1 Running 0 14m We will run commands inside the pod in the netproj-a Project that will connect to TCP port 5201 of the pod in the netproj-b Project. Test Connectivity (should work) Now that you have some projects and pods, let's test the connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project. To test connectivity between the two pods, run: export client=$(oc get pod -n netproj-a | grep iperf3-clients | cut -d' ' -f1) oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 58320 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 997 Mbits/sec 0 491 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 1 691 KBytes [ 5] 2.00-3.00 sec 119 MBytes 1.00 Gbits/sec 1 1.21 MBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 1 1.21 MBytes [ 5] 4.00-5.00 sec 119 MBytes 999 Mbits/sec 1 1.55 MBytes [ 5] 5.00-6.00 sec 119 MBytes 1.00 Gbits/sec 0 1.71 MBytes [ 5] 6.00-7.00 sec 119 MBytes 1.00 Gbits/sec 0 1.71 MBytes [ 5] 7.00-8.00 sec 119 MBytes 997 Mbits/sec 0 1.71 MBytes [ 5] 8.00-9.00 sec 119 MBytes 1.00 Gbits/sec 1 1.71 MBytes [ 5] 9.00-10.00 sec 119 MBytes 1.00 Gbits/sec 1 1.79 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec 6 sender [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec receiver iperf Done. Note that the last line says worked. This means that the pod in the netproj-a Project was able to connect to the pod in the netproj-b Project. This worked because, by default, with the network policy SDN, all pods in all projects can connect to each other. Restricting Access With the Network Policy based SDN, it's possible to restrict access in a project by creating a NetworkPolicy custom resource (CR). For example, the following restricts all access to all pods in a Project where this NetworkPolicy CR is applied. This is the equivalent of a DenyAll default rule on a firewall: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-default spec: podSelector: null ingress: - from: - podSelector: {} Note that the podSelector is empty, which means that this will apply to all pods in this Project. Also note that the ingress list is empty, which means that there are no allowed ingress rules defined by this NetworkPolicy CR. To restrict access to the pod in the netproj-b Project simply apply the above NetworkPolicy CR with: cat Test Connectivity #2 (should fail) Since the \"block all by default\" NetworkPolicy CR has been applied, connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project should now be blocked. Test by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: iperf3: error - unable to connect to server: Connection timed out command terminated with exit code 1 Note the last line that says FAILED!. This means that the pod in the netproj-a Project was unable to connect to the pod in the netproj-b Project (as expected). Allow Access With the Network Policy based SDN, it's possible to allow access to individual or groups of pods in a project by creating multiple NetworkPolicy CRs. The following allows access to port 5000 on TCP for all pods in the project with the label app: iperf3-server. The pod in the netproj-b project has this label. The ingress section specifically allows this access from all projects that have the label app: iperf3-client. # allow access to TCP port 5201 for pods with the label \"run: ose\" specifically # from projects with the label \"name: netproj-a\". cat Note that the podSelector is where the local project's pods are matched using a specific label selector. All NetworkPolicy CRs in a project are combined to create the allowed ingress access for the pods in the project. In this specific case the \"deny all\" policy is combined with the \"allow TCP 5201\" policy. Test Connectivity #3 (should work again) Since the \"allow access from netproj-a on port 5000\" NetworkPolicy has been applied, connectivity between the pod in the netproj-a Project and the pod in the netproj-b Project should be allowed again. Test by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 10 -b 1G' You will see something like the following: Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 34702 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 999 Mbits/sec 2 274 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 0 300 KBytes [ 5] 2.00-3.00 sec 119 MBytes 1.00 Gbits/sec 0 362 KBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 0 385 KBytes [ 5] 4.00-5.00 sec 119 MBytes 999 Mbits/sec 0 397 KBytes [ 5] 5.00-6.00 sec 119 MBytes 1.00 Gbits/sec 0 403 KBytes [ 5] 6.00-7.00 sec 119 MBytes 999 Mbits/sec 1 412 KBytes [ 5] 7.00-8.00 sec 119 MBytes 1.00 Gbits/sec 0 448 KBytes [ 5] 8.00-9.00 sec 119 MBytes 1.00 Gbits/sec 1 459 KBytes [ 5] 9.00-10.00 sec 119 MBytes 999 Mbits/sec 0 463 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.16 GBytes 1000 Mbits/sec 4 sender [ 5] 0.00-10.03 sec 1.16 GBytes 997 Mbits/sec receiver iperf Done. Note the last line that says worked. This means that the pod in the netproj-a Project was able to connect to the pod in the netproj-b Project (as expected). Test Connectivity #4 while chaning NetworkPolicy To show NetworkPolicy is non-disruptive to the application connections while updating the network policies. We will run a test for 30 seconds and try to update network policies in between the test Verify that UDP 5201 is still closed by running: oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -u -t 10 -b 1G' The UDP connection should be failed iperf3: error - unable to read from stream socket: Resource temporarily unavailable Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 command terminated with exit code 1 Run the test again as TCP 5201 for 30 seconds. oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -t 30 -b 1G' And update network policy while the test is still running eg. also add UDP port 5201 in OpenShift console spec: podSelector: matchLabels: app: iperf3-server ingress: - ports: - protocol: TCP port: 5201 - protocol: UDP port: 5201 from: - namespaceSelector: matchLabels: app: iperf3-client policyTypes: - Ingress TCP test result should be able to complete without connection reset or disconnect. Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 56720 connected to 172.30.101.62 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 119 MBytes 1000 Mbits/sec 1 878 KBytes [ 5] 1.00-2.00 sec 119 MBytes 1.00 Gbits/sec 0 1.04 MBytes [ 5] 2.00-3.00 sec 119 MBytes 999 Mbits/sec 0 1.09 MBytes [ 5] 3.00-4.00 sec 119 MBytes 1.00 Gbits/sec 0 1.54 MBytes [ 5] 4.00-5.00 sec 119 MBytes 1.00 Gbits/sec 0 1.62 MBytes [ 5] 5.00-6.00 sec 119 MBytes 997 Mbits/sec 0 1.97 MBytes [ 5] 6.00-7.00 sec 118 MBytes 989 Mbits/sec 0 1.97 MBytes [ 5] 7.00-8.00 sec 121 MBytes 1.01 Gbits/sec 1 2.07 MBytes [ 5] 8.00-9.00 sec 119 MBytes 999 Mbits/sec 0 2.17 MBytes [ 5] 9.00-10.00 sec 119 MBytes 1.00 Gbits/sec 1 2.40 MBytes [ 5] 10.00-11.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 11.00-12.00 sec 119 MBytes 999 Mbits/sec 1 2.52 MBytes [ 5] 12.00-13.00 sec 119 MBytes 1000 Mbits/sec 1 2.52 MBytes [ 5] 13.00-14.00 sec 119 MBytes 1.00 Gbits/sec 1 2.52 MBytes [ 5] 14.00-15.00 sec 119 MBytes 999 Mbits/sec 0 2.52 MBytes [ 5] 15.00-16.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 16.00-17.00 sec 119 MBytes 1.00 Gbits/sec 0 2.52 MBytes [ 5] 17.00-18.00 sec 119 MBytes 999 Mbits/sec 0 2.64 MBytes [ 5] 18.00-19.00 sec 119 MBytes 1.00 Gbits/sec 0 2.64 MBytes [ 5] 19.00-20.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 20.00-21.00 sec 119 MBytes 999 Mbits/sec 0 2.77 MBytes [ 5] 21.00-22.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 22.00-23.00 sec 119 MBytes 1.00 Gbits/sec 0 2.77 MBytes [ 5] 23.00-24.00 sec 119 MBytes 997 Mbits/sec 0 2.77 MBytes [ 5] 24.00-25.01 sec 119 MBytes 988 Mbits/sec 0 2.77 MBytes [ 5] 25.01-26.00 sec 120 MBytes 1.02 Gbits/sec 0 2.77 MBytes [ 5] 26.00-27.00 sec 119 MBytes 999 Mbits/sec 1 2.77 MBytes [ 5] 27.00-28.00 sec 119 MBytes 1.00 Gbits/sec 1 2.77 MBytes [ 5] 28.00-29.01 sec 119 MBytes 985 Mbits/sec 0 2.77 MBytes [ 5] 29.01-30.00 sec 120 MBytes 1.01 Gbits/sec 1 2.77 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-30.00 sec 3.49 GBytes 1000 Mbits/sec 9 sender [ 5] 0.00-30.02 sec 3.49 GBytes 999 Mbits/sec receiver iperf Done. Re-run UDP test again to confirm UDP 5201 has been allowed. oc exec $client -n netproj-a -- /bin/sh -c 'iperf3 -c iperf3-server.netproj-b.svc.cluster.local -u -t 10 -b 1G' iperf3 UDP test is now working Connecting to host iperf3-server.netproj-b.svc.cluster.local, port 5201 [ 5] local 10.131.0.55 port 39507 connected to 172.30.4.76 port 5201 [ ID] Interval Transfer Bitrate Total Datagrams [ 5] 0.00-1.00 sec 26.0 MBytes 218 Mbits/sec 19520 [ 5] 1.00-2.00 sec 30.2 MBytes 253 Mbits/sec 22663 [ 5] 2.00-3.00 sec 33.4 MBytes 279 Mbits/sec 25072 [ 5] 3.00-4.00 sec 30.0 MBytes 253 Mbits/sec 22486 [ 5] 4.00-5.00 sec 34.6 MBytes 291 Mbits/sec 25986 [ 5] 5.00-6.00 sec 35.3 MBytes 296 Mbits/sec 26455 [ 5] 6.00-7.00 sec 35.7 MBytes 299 Mbits/sec 26766 [ 5] 7.00-8.00 sec 33.6 MBytes 281 Mbits/sec 25183 [ 5] 8.00-9.00 sec 37.6 MBytes 317 Mbits/sec 28229 [ 5] 9.00-10.00 sec 36.7 MBytes 308 Mbits/sec 27537 - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Jitter Lost/Total Datagrams [ 5] 0.00-10.00 sec 333 MBytes 279 Mbits/sec 0.000 ms 0/249897 (0%) sender [ 5] 0.00-10.01 sec 286 MBytes 240 Mbits/sec 0.025 ms 35426/249897 (14%) receiver iperf Done. Egress IP address assignment for project egress traffic As a cluster administrator, you can configure the OpenShift SDN default Container Network Interface (CNI) network provider to assign one or more egress IP addresses to a project. By configuring an egress IP address for a project, all outgoing external connections from the specified project will share the same, fixed source IP address. External resources can recognize traffic from a particular project based on the egress IP address. An egress IP address assigned to a project is different from the egress router, which is used to send traffic to specific destinations. Egress IP addresses are implemented as additional IP addresses on the primary network interface of the node and must be in the same subnet as the node’s primary IP address. High availability of nodes is automatic. If a node that hosts an egress IP address is unreachable and there are nodes that are able to host that egress IP address, then the egress IP address will move to a new node. When the unreachable node comes back online, the egress IP address automatically moves to balance egress IP addresses across nodes. Configuring automatically assigned egress IP addresses for a namespace In OpenShift Container Platform you can enable automatic assignment of an egress IP address for a specific namespace across one or more nodes. Test ping from pod in netproj-a to VM outside OpenShift, the source IP Address is the Node IP Address [root@centos7-tools1 ~]# tcpdump -i ens160 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 08:37:18.756559 IP 198.18.1.18 > tools1.dcloud.cisco.com: ICMP echo request, id 3, seq 1, length 64 08:37:18.756627 IP tools1.dcloud.cisco.com > 198.18.1.18: ICMP echo reply, id 3, seq 1, length 64 08:37:19.757960 IP 198.18.1.18 > tools1.dcloud.cisco.com: ICMP echo request, id 3, seq 2, length 64 08:37:19.758014 IP tools1.dcloud.cisco.com > 198.18.1.18: ICMP echo reply, id 3, seq 2, length 64 Update the NetNamespace object with the egress IP address using the following JSON: oc patch netnamespace netproj-a --type=merge -p \\ '{\"egressIPs\": [\"198.18.1.241\"]}' You can set egressIPs to two or more IP addresses on different nodes to provide high availability. If multiple egress IP addresses are set, pods use the first IP in the list for egress, but if the node hosting that IP address fails, pods switch to using the next IP in the list after a short delay. Manually assign the egress IP to the node hosts. Set the egressIPs parameter on the HostSubnet object on the node host. Using the following JSON, include as many IPs as you want to assign to that node host: for node in $(oc get nodes | grep '\\-worker' | cut -d' ' -f1); do oc patch hostsubnet $node --type=merge -p '{\"egressCIDRs\": [\"198.18.1.0/24\"]}' done In the previous example, all egress traffic for project1 will be routed to the node hosting the specified egress IP, and then connected (using NAT) to that IP address. Test ping from the same POD in netproj-a again, now the source IP Address is now egressIP assigned to netproj-a [root@centos7-tools1 ~]# tcpdump -i ens160 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes 09:08:33.841050 IP 198.18.1.241 > registry0.example.com: ICMP echo request, id 7, seq 130, length 64 09:08:33.841131 IP registry0.example.com > 198.18.1.241: ICMP echo reply, id 7, seq 130, length 64 09:08:34.861757 IP 198.18.1.241 > registry0.example.com: ICMP echo request, id 7, seq 131, length 64 09:08:34.861815 IP registry0.example.com > 198.18.1.241: ICMP echo reply, id 7, seq 131, length 64 Network Logging The network access logging will be done with envoy sidecar proxy, the details will be in OpenShift Service Mesh section. Container Platform Network Segmentation For the network segregation requirement on Container Platform, we can consider multiple choices of strategy for the network segregation levels supported in OpenShift design architectures. The following are the options that we could do to helps network segmentations are enforced at each level Multi-Cluster Level The highest level of separation is to dedicate one cluster per environment per network zone, e.g. 1 Cluster for Private Zone(Internal Only), 1 Cluster for Extranet Zone and 1 Cluster for DMZ Zone (Internet facing). By seprating clusters per network zone, you can ensure that everything is separated from each environemnt. The deployment can be done in a Pipeline to deploy to each cluster in each stage. Pros: Highest level of separation. Resources, RBAC, NetworkPolicy, Storage, etc. are per cluster. You can also dedicate a cluster per enduser team as well Least scope of security breach will be limited within a cluster Cons: Consume more resources for Control Plane, Storage, Networking, Monitoring, Logging, etc. per cluster High maintenance and operations for Multi-Cluster Management, if you don't prepare Multi-Cluster Management tools. Example ArgoCD, Red Hat Advanced Cluster Management Cluster Namespace separation level In the Container Platform, many technology and components are supporting separation of resources within a cluster. The components that can be used for separation are Container Runtime, CGroups, SELinux: Container Host level for Application resource segration on container host Project/Namespace: We can do the resource seprating by RBAC, Resource Quotas Node Seletor: To select node for sepecific resources Router/Ingress Sharding: We can dedicate Router/Ingress Controller to be the ingestion point in each network zone Pros: Resource efficientcy: by using the same cluster to shared control plane and infrastructure components Less management: less number of clusters to manage and upgrade Focused on Environment: by reducing overall resources, you can focused to have an identical, but smaller, OpenShift Cluster for Testing and Development environment. This can help you to have a test bed for testing integration, operation, BCP or upgrade. Cons: More Router/Ingress Infra nodes End user can't get full cluster-admin control when compare with dedicated cluster per team approach, but OpenShift can somehow compensate by having Operators with multi tenants support, such as OpenShift Service Mesh, the user can create their own service mesh control plane Additional Openshift Blog: https://www.openshift.com/blog/network-policies-controlling-cross-project-communication-on-openshift "},"infrastructure-backup-etcd.html":{"url":"infrastructure-backup-etcd.html","title":"OpenShift state backup with etcd snapshot","keywords":"","body":"Infrastructure Basic Infrastructure Basic Prerequisites Backup etcd Backing up etcd data Backup etcd with cron job Restoring to a previous cluster state Prerequisites OpenShift 4.6 on VMware 6.7 U3+ or 7.0 VMware Cloud Native Storage to support CNS CSI OpenShift installer Node subnet with DHCP pool DNS NTP Backup etcd etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects. Back up your cluster’s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. After you have an etcd backup, you can restore to a previous cluster state. Backing up etcd data Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd. Prerequisites You have access to the cluster as a user with the cluster-admin role. You have checked whether the cluster-wide proxy is enabled. Procedure Start a debug session for a master node: oc debug node/ Change your root directory to the host: chroot /host If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. Run the cluster-backup.sh script and pass in the location to save the backup to. /usr/local/bin/cluster-backup.sh /home/core/assets/backup Example script output 1bf371f1b5a483927cd01bb593b0e12cff406eb8d7d0acf4ab079c36a0abd3f7 etcdctl version: 3.3.18 API version: 3.3 found latest kube-apiserver-pod: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-7 found latest kube-controller-manager-pod: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8 found latest kube-scheduler-pod: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6 found latest etcd-pod: /etc/kubernetes/static-pod-resources/etcd-pod-2 Snapshot saved at /home/core/assets/backup/snapshot_2020-03-18_220218.db snapshot db and kube resources are successfully saved to /home/core/assets/backup In this example, two files are created in the /home/core/assets/backup/ directory on the master host: snapshot_.db: This file is the etcd snapshot. statickuberesources.tar.gz: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot. Backup etcd with cron job Procedures: Set your sftp target, username and password sftp_target=198.18.134.150 sftp_user=\"root\" sftp_pass=\"b1ndP^ssword\" Create a backup script cat etcd-backup-on-debug-pod.sh #!/bin/sh echo \"chroot to /host\" chroot /host /bin/sh Create etcd backup namespace cat Create config-map etcd-backup-on-debug-pod.sh from cluster-backup.sh oc create configmap etcd-backup-on-debug-pod --from-file=etcd-backup-on-debug-pod.sh Pickup the first master node name to run etcd backup master_node=$(oc get node -l node-role.kubernetes.io/master= -o=jsonpath='{.items[0].metadata.name}') Create a cronjob to run debug pod and backup script cat - quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:091cd1158444af8382312b71150d26e6550c5d52023b993fec6afd2253d2e425 serviceAccount: default volumes: - name: host hostPath: path: / type: Directory - name: etcd-backup-script configMap: name: etcd-backup-on-debug-pod defaultMode: 0744 EOF Restoring to a previous cluster state To restore the cluster to a previous state, you must have previously backed up etcd data by creating a snapshot. You will use this snapshot to restore the cluster state. You can use a saved etcd backup to restore back to a previous cluster state. You use the etcd backup to restore a single control plane host. Then the etcd cluster Operator handles scaling to the remaining master hosts. Prerequisites Access to the cluster as a user with the cluster-admin role. SSH access to master hosts. A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: snapshot.db and static_kuberesources.tar.gz. Procedure Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on. Establish SSH connectivity to each of the control plane nodes, including the recovery host. The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal. Warning: If you do not complete this step, you will not be able to access the master hosts to complete the restore procedure, and you will be unable to recover your cluster from this state. Copy the etcd backup directory to the recovery control plane host. This procedure assumes that you copied the backup directory containing the etcd snapshot and the resources for the static pods to the /home/core/ directory of your recovery control plane host. Stop the static pods on all other control plane nodes. Note: It is not required to manually stop the pods on the recovery host. The recovery script will stop the pods on the recovery host. Access a control plane host that is not the recovery host. Move the existing etcd pod file out of the kubelet manifest directory:sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp Verify that the etcd pods are stopped. sudo crictl ps | grep etcd The output of this command should be empty. If it is not empty, wait a few minutes and check again. Move the existing Kubernetes API server pod file out of the kubelet manifest directory: sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp Verify that the Kubernetes API server pods are stopped.sudo crictl ps | grep kube-apiserver The output of this command should be empty. If it is not empty, wait a few minutes and check again. Move the etcd data directory to a different location:sudo mv /var/lib/etcd/ /tmp Repeat this step on each of the other master hosts that is not the recovery host. Access the recovery control plane host. If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. You can check whether the proxy is enabled by reviewing the output of oc get proxy cluster -o yaml. The proxy is enabled if the httpProxy, httpsProxy, and noProxy fields have values set. Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory: sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup Example script output ...stopping kube-scheduler-pod.yaml ...stopping kube-controller-manager-pod.yaml ...stopping etcd-pod.yaml ...stopping kube-apiserver-pod.yaml Waiting for container etcd to stop .complete Waiting for container etcdctl to stop .............................complete Waiting for container etcd-metrics to stop complete Waiting for container kube-controller-manager to stop complete Waiting for container kube-apiserver to stop ..........................................................................................complete Waiting for container kube-scheduler to stop complete Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup starting restore-etcd static pod starting kube-apiserver-pod.yaml static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml starting kube-controller-manager-pod.yaml static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml starting kube-scheduler-pod.yaml static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml Restart the kubelet service on all master hosts. From the recovery host, run the following command:sudo systemctl restart kubelet.service Repeat this step on all other master hosts. Verify that the single member control plane has started successfully. From the recovery host, verify that the etcd container is running. sudo crictl ps | grep etcd Example output 3ad41b7908e32 36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009 About a minute ago Running etcd 0 7c05f8af362f0 From the recovery host, verify that the etcd pod is running. oc get pods -n openshift-etcd | grep etcd Example output NAME READY STATUS RESTARTS AGE etcd-ip-10-0-143-125.ec2.internal 1/1 Running 1 2m47s If the status is Pending, or the output lists more than one running etcd pod, wait a few minutes and check again. Force etcd redeployment. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc patch etcd cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge The forceRedeploymentReason value must be unique, which is why a timestamp is appended. When the etcd cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up. Verify all nodes are updated to the latest revision. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition for etcd to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 If the output shows a message such as 2 nodes are at revision 3; 1 nodes are at revision 4, this means that the update is still in progress. Wait a few minutes and try again. After etcd is redeployed, force new rollouts for the control plane. The Kubernetes API server will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer. In a terminal that has access to the cluster as a cluster-admin user, run the following commands. Update the kubeapiserver: oc patch kubeapiserver cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Update the kubecontrollermanager: oc patch kubecontrollermanager cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Update the kubescheduler: oc patch kubescheduler cluster -p='{\"spec\": {\"forceRedeploymentReason\": \"recovery-'\"$( date --rfc-3339=ns )\"'\"}}' --type=merge Verify all nodes are updated to the latest revision. oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type==\"NodeInstallerProgressing\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}' Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update: AllNodesAtLatestRevision 3 nodes are at revision 3 Verify that all master hosts have started and joined the cluster. In a terminal that has access to the cluster as a cluster-admin user, run the following command: oc get pods -n openshift-etcd | grep etcd Example output etcd-ip-10-0-143-125.ec2.internal 2/2 Running 0 9h etcd-ip-10-0-154-194.ec2.internal 2/2 Running 0 9h etcd-ip-10-0-173-171.ec2.internal 2/2 Running 0 9h "},"infrastructure-taint-and-toleration.html":{"url":"infrastructure-taint-and-toleration.html","title":"Pod Taint and Toleration","keywords":"","body":"Taint and Toleration Taint and Toleration Pod Eviction Node UnreachablePod Eviction Pod eviction from node behavior can be configured by adding toleration to pod. This default value is 5 minutes in case of node unreachable or not-ready Node Unreachable Configure pod to evict from node in case node is unreachable. Check pod default toleration Check toleration oc describe pod $(oc get pods | grep Running | tail -n 1 | awk '{print $1}') | \\ grep -A2 -i toleration Output example. node.kubernetes.io/not-ready is 300s (5 minutes) node.kubernetes.io/unreachable is 300s (5 minutes) Tolerations: node.kubernetes.io/memory-pressure:NoSchedule op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Set unreachable to 1 minute by adding toleration for node unreachable to deployment or deployment config. This toleration mean that Pods remain bound to nodes for 60s after unreachable of these problems is detected. Remark: Default value of 5 minutes is reasonable in the assumption for preventing false posivites Example of deployment with unreachable toleration set to 1 minute. template: metadata: labels: app: backend-native deploymentconfig: backend-native spec: containers: - image: image-registry.openshift-image-registry.svc:5000/demo/backend-native@sha256:5b76fdf7113c0db6d7fddea54997dd648a55b2a04383effb82f55cdbb0419dd5 ... ... tolerations: - key: node.kubernetes.io/unreachable operator: Exists effect: NoExecute tolerationSeconds: 60 Check pod toleration node.kubernetes.io/unreachable is chaged to 60s. Pod will be recreate on another node if node is unreachable for 60s Tolerations: node.kubernetes.io/memory-pressure:NoSchedule op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 60s "},"custom-roles.html":{"url":"custom-roles.html","title":"Custom Roles and Service Account","keywords":"","body":"Custom Roles and Service Account Custom Roles and Service Account Service Account Create Service Account Custom Roles Local Role Create cluster role Test Service Account CLI REST API Use Service Account with Deployment Service Account Create custom roles for service account to view,list and watch - configmaps - pods - services - namespaces - endpoints - secrets - *nodes* Remark: nodes need cluster role Create Service Account Create Service Account oc create sa sa-discovery -n demo Output serviceaccount/sa-discovery created Custom Roles Local Role Create role for service account. oc create role app-discovery \\ --verb=get,list,watch \\ --resource=configmaps,pods,services,namespaces,endpoints \\ -n demo oc describe role app-discovery -n demo or create from app-discovery yaml oc create -f manifests/app-discovery-role.yaml -n demo oc describe role app-discovery -n demo oc describe role list-secret -n demo Output role.rbac.authorization.k8s.io/app-discovery created Name: app-discovery Labels: Annotations: PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get list watch] endpoints [] [] [get list watch] namespaces [] [] [get list watch] pods [] [] [get list watch] secrets [] [] [get list watch] services [] [] [get list watch] Assign role to service account oc adm policy add-role-to-user app-discovery \\ system:serviceaccount:demo:sa-discovery --role-namespace=demo -n demo Output role.rbac.authorization.k8s.io/app-discovery added: \"system:serviceaccount:demo:sa-discovery\" Create cluster role Create cluster role to view node oc create clusterrole view-nodes \\ --verb=get,list,watch --resource=nodes or create from view-nodes yaml oc create -f manifests/clusterrole-view-nodes.yaml Output clusterrole.rbac.authorization.k8s.io/view-nodes created Assign role to service account oc adm policy add-cluster-role-to-user \\ view-nodes system:serviceaccount:demo:sa-discovery Output clusterrole.rbac.authorization.k8s.io/view-nodes added: \"system:serviceaccount:demo:sa-discovery\" Test Service Account CLI Test service account sa-discovery with CLI tool Get service account sa-discovery token TOKEN=$(oc sa get-token sa-discovery -n demo) Login with service account token oc login --token=$TOKEN oc whoami Output Using project \"demo\". system:serviceaccount:demo:app-discovery Test list resources clear printf \"List configmaps\\n\" oc get configmaps -n demo echo \"Press any keys to continue...\";read clear printf \"List secrets\\n\" oc get secrets -n demo echo \"Press any keys to continue...\";read clear printf \"List pods\\n\" oc get pods -n demo echo \"Press any keys to continue...\";read clear printf \"List services\\n\" oc get svc -n demo echo \"Press any keys to continue...\";read clear printf \"List nodes\\n\" oc get nodes echo \"Press any keys to continue...\";read clear Test get secret oc describe secrets/$(oc get secrets --no-headers|head -n 1|awk '{print $1}') You will get following error because sa-discovery has only list action Error from server (Forbidden): secrets \"builder-dockercfg-cjfz6\" is forbidden: User \"system:serviceaccount:demo:sa-discovery\" cannot get resource \"secrets\" in API group \"\" in the namespace \"demo\" REST API List pods API=$(oc whoami --show-server) NAMESPACE=demo curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/namespaces/$NAMESPACE/pods Output \"items\": [ { \"metadata\": { \"name\": \"backend-797f8bfdcc-xrzkw\", \"generateName\": \"backend-797f8bfdcc-\", \"namespace\": \"demo\", \"selfLink\": \"/api/v1/namespaces/demo/pods/backend-797f8bfdcc-xrzkw\", \"uid\": \"e6845671-6e46-4b20-aa7b-ced5839341e2\", \"resourceVersion\": \"56509\", \"creationTimestamp\": \"2021-06-10T09:10:10Z\", \"labels\": { \"app\": \"backend\", \"pod-template-hash\": \"797f8bfdcc\", \"version\": \"v1\" }, Get sepcified pod curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/namespaces/$NAMESPACE/pods/ Get node curl -k -H \"Accept: application/json\" -H \"Authorization: Bearer $TOKEN\" $API/api/v1/nodes/$(oc get nodes --no-headers|head -n 1|awk '{print $1}') Use Service Account with Deployment Backend deployment (backend-discovery-sa.yaml) with custom service account spec: replicas: 1 selector: matchLabels: app: backend version: v1 template: metadata: creationTimestamp: null labels: app: backend version: v1 annotations: sidecar.istio.io/inject: \"false\" spec: serviceAccountName: svip-ignite-discovery automountServiceAccountToken: false containers: - name: backend Check service account used by pod oc get pod/ -o jsonpath='{.spec.serviceAccountName}' "},"custom-alert.html":{"url":"custom-alert.html","title":"Custom Alert","keywords":"","body":"Custom Monitoring Custom Monitoring Monitor for Pod Creation Cluster Level User Workload Monitoring Test Alert Monitor for Pod Creation Create custom alerts to monitor for pod creating status with PrometheusRule pod-stuck-alerts.yaml This PrometheusRule will sending alerts if pod status PodStuckContainerCreating for 2 minutes PodStuckImagePullBackOff for 30 seconds PodStuckErrImagePull for 2 minuts PodStuckCrashLoopBackOff for 2 minutes PodStuckCreateContainerError for 2 minutes apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: pod-stuck spec: groups: - name: PodStuck rules: - alert: PodStuckContainerCreating annotations: description: Pod creation failed with ContainerCreating message: Pod in project project stuck at ContainerCreating expr: kube_pod_container_status_waiting_reason{reason=\"ContainerCreating\"} == 1 for: 2m labels: severity: critical - alert: PodStuckImagePullBackOff annotations: description: Pod creation failed with ImagePullBackOff message: Pod in project project stuck at ImagePullBackOff expr: kube_pod_container_status_waiting_reason{reason=\"ImagePullBackOff\"} == 1 for: 2m labels: severity: critical - alert: PodStuckErrImagePull annotations: description: Pod creation failed with ErrImagePull message: Pod in project project stuck at ErrImagePull expr: kube_pod_container_status_waiting_reason{reason=\"ErrImagePull\"} == 1 for: 30s labels: severity: critical - alert: PodStuckCrashLoopBackOff annotations: description: Pod creation failed with ImagePullBackOff message: Pod in project project stuck at CrashLoopBackOff expr: kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\"} == 1 for: 2m labels: severity: critical - alert: PodStuckCreateContainerError annotations: description: Pod creation failed with ErrImagePull message: Pod in project project stuck at CreateContainerError expr: kube_pod_container_status_waiting_reason{reason=\"CreateContainerError\"} == 1 for: 2m labels: severity: critical Cluster Level Create PrometheusRule in namespace openshift-monitoring oc create -f manifests/pod-stuck-alerts.yaml -n openshift-monitoring Check alert rules User Workload Monitoring If user workload monitoring is enabled. Prometheus Rule can be created at project level. oc create -f manifests/pod-stuck-alerts.yaml -n demo Add following label to deploy rules to Thanos Ruler metadata: name: pod-stuck labels: openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus Check for alert rules in Developer Console Test Alert Create following deployments. These deployments intentionally put pods into error state. oc create -f manifests/pod-stuck -n demo Check for result oc get pods -n demo Sample result NAME READY STATUS RESTARTS AGE backend-v1-759c485867-27ql6 0/1 ContainerCreating 0 2m42s backend-v2-5c866fb9bf-2fzj4 0/1 CreateContainerError 0 2m40s backend-v3-cbcd9cddb-pjbvl 0/1 ImagePullBackOff 0 2m39s Check for alerts on Notifications menu Overview For User Workload Monitoring Check for details of an alert "},"compliance-operator.html":{"url":"compliance-operator.html","title":"Compliance","keywords":"","body":"OpenShift Compliance with Compliance Operator OpenShift Compliance with Compliance Operator Prerequisites Compliance Operator CIS Profile Openscap Report Prerequisites OpenShift 4.6 or 4.7 Cluster-admin user access Compliance Operator Install Compliance Operator from OperatorHub [Optional] Verify Compliance Operator Check compliance profile oc get profiles.compliance -n openshift-compliance Output example NAME AGE ocp4-cis 4h52m ocp4-cis-node 4h52m ocp4-e8 4h52m ocp4-moderate 4h52m rhcos4-e8 4h52m rhcos4-moderate 4h52m Check detail of profile oc get -o yaml profiles.compliance ocp4-cis -n openshift-compliance Output example ... rules: - ocp4-accounts-restrict-service-account-tokens - ocp4-accounts-unique-service-account - ocp4-api-server-admission-control-plugin-alwaysadmit - ocp4-api-server-admission-control-plugin-alwayspullimages - ocp4-api-server-admission-control-plugin-namespacelifecycle - ocp4-api-server-admission-control-plugin-noderestriction ... Check details of rule oc get -o yaml rules.compliance ocp4-accounts-unique-service-account -n openshift-compliance Check for default ScanSetting List all ScanSetting oc get scansettings -n openshift-compliance Result NAME AGE default 35m default-auto-apply 35m Check for default ScanSetting oc describe scansettings default -n openshift-compliance Output example, scheduled at 1AM everyday and apply to both master and worker node and use block storage (RWO) for stored result Raw Result Storage: Pv Access Modes: ReadWriteOnce Rotation: 3 Size: 1Gi Roles: worker master Scan Tolerations: Effect: NoSchedule Key: node-role.kubernetes.io/master Operator: Exists Schedule: 0 1 * * * Events: CIS Profile To start scan, create ScanSettingBinding. Scan will be started immediately after save Use Admin Console to create ScanSettingBinding, default is rhcos4-moderate and use default ScanSetting Add opc4-cis profile to profiles list apiVersion: compliance.openshift.io/v1alpha1 profiles: - apiGroup: compliance.openshift.io/v1alpha1 name: rhcos4-moderate kind: Profile - apiGroup: compliance.openshift.io/v1alpha1 name: ocp4-cis kind: Profile settingsRef: apiGroup: compliance.openshift.io/v1alpha1 name: default kind: ScanSetting kind: ScanSettingBinding metadata: name: cis-and-moderate-profile namespace: openshift-compliance or use CLI oc apply -f manifests/cis-and-moderate-profile.yaml oc describe scansettingbinding/cis-and-moderate-profile -n openshift-compliance Check for status Status: Conditions: Last Transition Time: 2021-05-12T08:02:50Z Message: The scan setting binding was successfully processed Reason: Processed Status: True Type: Ready Output Ref: API Group: compliance.openshift.io Kind: ComplianceSuite Name: cis-and-moderate-profile Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuiteCreated 10s scansettingbindingctrl ComplianceSuite openshift-compliance/cis-and-moderate-profile created Check ComplianceScan tab or use CLI oc get compliancescan -n openshift-compliance Output NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT rhcos4-moderate-master RUNNING NOT-AVAILABLE rhcos4-moderate-worker RUNNING NOT-AVAILABLE Check result Count for FAIL oc get compliancecheckresult -n openshift-compliance | grep FAIL | wc -l Script for summary result bin/check-compliance-result.sh ================== RESULT ================== TYPE NUMBER PASS 148 FAIL 395 MANUAL 34 INFO 2 NOT_APPLICABLE 0 ================ SEVERITY ================= high 29 low 27 medium 515 unknown 14 Check for result description for ocp4-cis-api-server-encryption-provider-config oc describe compliancecheckresult/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance Output ... Description: Configure the Encryption Provider etcd is a highly available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_config Instructions: Run the following command: $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type' The output should return aescdc as the encryption type. ... Severity: medium Status: FAILED Events: Fix failed policies with ComplianceRemediation List ComplianceRemediation oc get ComplianceRemediation -n openshift-compliance Output NAME STATE ocp4-cis-api-server-encryption-provider-cipher NotApplied ocp4-cis-api-server-encryption-provider-config NotApplied rhcos4-moderate-master-audit-rules-dac-modification-chmod NotApplied rhcos4-moderate-master-audit-rules-dac-modification-chown NotApplied ... Fix failed ocp4-cis-api-server-encryption-provider-config policy with ComplianceRemidiation oc patch -n openshift-compliance complianceremediation \\ ocp4-cis-api-server-encryption-provider-config -p '{\"spec\":{\"apply\":true}}' --type='merge' Check result oc get ComplianceRemediation/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance Output NAME STATE ocp4-cis-api-server-encryption-provider-config Applied Re-run scan Annotate ComplianceScans to rescan or use script for scan in $(oc get compliancescans -n openshift-compliance -o custom-columns=NAME:.metadata.name --no-headers) do oc annotate compliancescans $scan compliance.openshift.io/rescan= -n openshift-compliance done watch -d oc get compliancescans -n openshift-compliance Result compliancescan.compliance.openshift.io/ocp4-cis annotated compliancescan.compliance.openshift.io/rhcos4-moderate-master annotated compliancescan.compliance.openshift.io/rhcos4-moderate-worker annotated NAME PHASE RESULT ocp4-cis RUNNING NOT-AVAILABLE rhcos4-moderate-master RUNNING NOT-AVAILABLE rhcos4-moderate-worker RUNNING NOT-AVAILABLE Recheck policy ocp4-cis-api-server-encryption-provider-config oc describe compliancecheckresult/ocp4-cis-api-server-encryption-provider-config -n openshift-compliance Output ... Severity: medium Status: PASS Events: Change ScanSettingBinding cis-and-moderate-profile to use ScanSetting default-auto-apply oc patch -n openshift-compliance ScanSettingBinding cis-and-moderate-profile -p '{\"settingsRef\":{\"name\":\"default-auto-apply\"}}' --type='merge' Output scansettingbinding.compliance.openshift.io/cis-and-moderate-profile patched Re-run scan Check compliance scan status watch oc get compliancescans -n openshift-compliance Output NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT rhcos4-moderate-master RUNNING NOT-AVAILABLE rhcos4-moderate-worker RUNNING NOT-AVAILABLE Recheck result ================== RESULT ================== TYPE NUMBER PASS 150 FAIL 89 MANUAL 34 INFO 2 NOT_APPLICABLE 0 INCONSISTENT 304 ================ SEVERITY ================= high 29 low 27 medium 515 unknown 14 Openscap Report Create pod to mount to cis pvoc create -f manifests/cis-extract.yaml oc apply -f manifests/ocp4-worker-extract.yaml Copy report file from pod oc cp cis-extract:/cis-scan-results . Reports ├── 0 │ └── ocp4-cis-api-checks-pod.xml.bzip2 └── 1 └── ocp4-cis-api-checks-pod.xml.bzip2 Install openscap on RHELyum install openscap-scanner openscap-utils scap-security-guide Use openscap utils in RHEL to generate HTML reportoscap xccdf generate report ocp4-cis-api-checks-pod.xml.bzip2 > ocp4-cis.html Sample reports cis and ocp4-worker-moderate HTML version here cis and ocp4-worker-moderate "},"acm-application-management.html":{"url":"acm-application-management.html","title":"Application Manageement","keywords":"","body":"Application Management with RHACM Application Management with RHACM RHACM Environment Demo Application with Kustomize RHACM Configuration ACM Console RHACM Environment RHACM with 2 managed clusters Production Cluster(s) labeled with environment=prod Development Cluster(s) labled with environment=dev Demo Application with Kustomize Frontend/Backend App with 2 overlays for environment dev and prod Number of replicas Backend's environment variables Frontend's environment variables . ├── base │ ├── backend-service.yaml │ ├── backend.yaml │ ├── frontend-service.yaml │ ├── frontend.yaml │ ├── kustomization.yaml │ ├── namespace.yaml │ └── route.yaml └── overlays ├── dev │ ├── backend.yaml │ ├── frontend.yaml │ └── kustomization.yaml └── prod ├── backend.yaml ├── frontend.yaml └── kustomization.yaml RHACM Configuration RHACM application managment configuration Create Namespace Create Channel Create Applicatoion Create subscription for production and development environment Create placement rule for production and development environment Deploy oc apply -f manifests/acm-app-management/01_namespace.yaml oc apply -f manifests/acm-app-management/02_channel.yaml oc apply -f manifests/acm-app-management/03_application_demo_app.yaml oc apply -f manifests/acm-app-management/04_subscription_dev.yaml oc apply -f manifests/acm-app-management/04_subscription_prod.yaml oc apply -f manifests/acm-app-management/05_placement_dev.yaml oc apply -f manifests/acm-app-management/05_placement_prod.yaml or for i in $(ls -1 manifests/acm-app-management) do oc apply -f manifests/acm-app-management/$i done ACM Console Demo App topology Check number of replicas for prod Filter by subscription "},"acm-hibernate.html":{"url":"acm-hibernate.html","title":"Cost saving with hibernating OpenShift","keywords":"","body":"Hibernate OpenShift on Cloud Providers for Cost Saving Hibernate OpenShift on Cloud Providers for Cost Saving Prerequisites Introduction How ACM acheive Hibernate policies for OpenShift Steps 1-2-3 Prerequisites OpenShift 4.6 or 4.7 Cluster-admin user access Red Hat Advanced Cluster Management Cloud Provider Credentials Introduction Original article from Hibernate for cost savings for Advanced Cluster Management Provisioned Clusters with Subscriptions OpenShift 4 has the ability to suspend and resume clusters on Cloud Providers. Red Hat Advanced Cluster Management extends this capability through its Cluster Lifecycle Management (Hive), where you can have a policy to hibernate the clusters to save the cost saving in non-working hours (ex. 16 hours in a day is 16/24 = 66% saving) How ACM acheive Hibernate policies for OpenShift We will walkthrough high-level process how ACM can define the herbernate policy for the cluster on Clouds Clone the cluster-hibernate repository to your environment Create the Running/Hibernate manifest files and put them in your Channel Create the Subscription to local hub cluster with your desire time windows Sample picture Steps 1-2-3 Steps: a b c d Code Block uname -a --- sample: yaml spec: key1: value1 item_list: - name: item1 property: prop1 - name: item2 property: prop2 Sample inline yaml to oc apply cat "},"build-with-oc.html":{"url":"build-with-oc.html","title":"Application Build & Deployment with oc","keywords":"","body":"Build Container Image with OC CLI ______ ______ ______ __ __ / __ \\ / | / || | | | | | | | | ,----' | ,----'| | | | | | | | | | | | | | | | | `--' | | `----. | `----.| `----.| | \\______/ \\______| \\______||_______||__| Build Container Image with OC CLI Configure OpenShift with external registry (optional) Source build All-in-One Build and Deploy Binary build with Dockerfile Configure OpenShift with external registry (optional) Create docker secret to access external registry With user and password NEXUS_REGISTRY=external_registry.example.com oc create secret docker-registry nexus-registry --docker-server=$NEXUS_REGISTRY \\ --docker-username=$CICD_NEXUS_USER \\ --docker-password=$CICD_NEXUS_PASSWORD \\ --docker-email=unused \\ From dockercfg file apiVersion: v1 kind: Secret metadata: name: nexus-registry type: kubernetes.io/dockercfg data: .dockercfg: | \"\" Link secret for builder oc secrets link default nexus-registry --for=pull Link secret for pull image oc secrets link builder nexus-registry For insecure registry Edit image.config.openshift.io/cluster oc edit image.config.openshift.io/cluster Add insecure registry to spec spec: registrySources: insecureRegistries: - nexus-registry.ci-cd.svc.cluster.local - nexus-registry.example.com Source build All-in-One Use source-to-image from git this will create image stream build config deployment service oc new-app https://gitlab.com/ocp-demo/frontend-js \\ --name=frontend Check build log oc logs bc/frontend --follow Build and Deploy Create build config oc new-build --name=frontend-v1 -l app=frontend-v1 \\ https://gitlab.com/ocp-demo/frontend-js Create deployment and service oc new-app frontend-v1 Binary build with Dockerfile Clone sample Backend Quarkus git clone https://gitlab.com/ocp-demo/backend_quarkus Create application binary cd code mvn clean package -DskipTests=true Create Build Config Push to OpenShift's internal image registry APP_NAME=backend oc new-build --binary --name=$APP_NAME -l app=$APP_NAME Push to OpenShift's external image registry APP_NAME=backend EXTERNAL_REGISTRY=nexus-registry.example.com EXTERNAL_REGISTRY_SECRET=nexus-registry TAG=latest oc new-build --binary --to-docker=true \\ --to=$EXTERNAL_REGISTRY/$APP_NAME:$TAG \\ --push-secret=$EXTERNAL_REGISTRY_SECRET \\ --name=$APP_NAME \\ -l app=$APP_NAME Change build strategy to DockerStrategy oc patch bc/$APP_NAME \\ -p \"{\\\"spec\\\":{\\\"strategy\\\":{\\\"dockerStrategy\\\":{\\\"dockerfilePath\\\":\\\"src/main/docker/Dockerfile.jvm\\\"}}}}\" Build container image oc start-build $APP_NAME --from-dir=. --follow Create Application from internal image registry oc new-app --image-stream=${APP_NAME} \\ --labels=app.openshift.io/runtime=quarkus,app.openshift.io/runtime-version=11,app.kubernetes.io/part-of=Demo Pause rollout deployment oc expose svc $APP_NAME Create liveness and readiness probe oc set probe deployment/$APP_NAME --readiness \\ --get-url=http://:8080/q/health/ready \\ --initial-delay-seconds=8 \\ --failure-threshold=1 --period-seconds=10 oc set probe deployment/$APP_NAME --liveness \\ --get-url=http://:8080/q/health/live \\ --initial-delay-seconds=5 -\\ -failure-threshold=3 --period-seconds=10 Set request and limit oc set resources deployment $APP_NAME --requests=\"cpu=50m,memory=100Mi\" oc set resources deployment $APP_NAME --limits=\"cpu=150m,memory=150Mi\" Create configmap oc create configmap $APP_NAME --from-file=config/application.properties oc set volume deployment/{APP_NAME --add --name=$APP_NAME-config \\ --mount-path=/deployments/config/application.properties \\ --sub-path=application.properties \\ --configmap-name=$APP_NAME Set HPA oc autoscale deployment $APP_NAME --min 2 --max 4 --cpu-percent=60 Resume rollout deployment oc rollout resume deployment $APP_NAME Create route Expose service oc expose svc $APP_NAME Create route with edge TLS oc create route edge $APP_NAME --service=$APP_NAME --port=8080 "},"build-with-odo.html":{"url":"build-with-odo.html","title":"Application Build & Deployment with odo","keywords":"","body":"Build Container Image with OpenShift DO ___ ____ _ _ __ _ ____ ___ / _ \\ _ __ ___ _ __ / ___|| |__ (_)/ _| |_ | _ \\ / _ \\ | | | | '_ \\ / _ \\ '_ \\\\___ \\| '_ \\| | |_| __| | | | | | | | | |_| | |_) | __/ | | |___) | | | | | _| |_ | |_| | |_| | \\___/| .__/ \\___|_| |_|____/|_| |_|_|_| \\__| |____/ \\___/ |_| Build Container Image with OpenShift DO ODO Catalog Sample Java ODO Catalog list odo catalog odo catalog list components Catalog Odo Devfile Components: NAME DESCRIPTION REGISTRY java-maven Upstream Maven and OpenJDK 11 DefaultDevfileRegistry java-openliberty Open Liberty microservice in Java DefaultDevfileRegistry java-quarkus Upstream Quarkus with Java+GraalVM DefaultDevfileRegistry java-springboot Spring Boot® using Java DefaultDevfileRegistry java-vertx Upstream Vert.x using Java DefaultDevfileRegistry java-wildfly Upstream WildFly DefaultDevfileRegistry java-wildfly-bootable-jar Java stack with WildFly in bootable Jar mode, OpenJDK 11 and... DefaultDevfileRegistry nodejs Stack with NodeJS 12 DefaultDevfileRegistry python Python Stack with Python 3.7 DefaultDevfileRegistry python-django Python3.7 with Django DefaultDevfileRegistry Odo S2I Components: NAME PROJECT TAGS SUPPORTED java openshift latest,openjdk-11-el7,openjdk-11-ubi8,openjdk-8-el7 YES nodejs openshift 12-ubi8,14-ubi8,latest YES dotnet openshift 2.1-el7,2.1-ubi8,3.1-el7,3.1-ubi8 NO golang openshift 1.13.4-ubi7,1.14.7-ubi8,latest NO httpd openshift 2.4-el7,2.4-el8,latest NO java openshift openjdk-8-ubi8 NO nginx openshift 1.14-el8,1.16-el7,1.16-el8,1.18-ubi7,1.18-ubi8,latest NO nodejs openshift 10-ubi7,10-ubi8,12-ubi7,14-ubi7 NO perl openshift 5.26-el7,5.26-ubi8,5.30-el7,5.30-ubi8,latest NO php openshift 7.2-ubi8,7.3-ubi7,7.3-ubi8,7.4-ubi8,latest NO python openshift 2.7-ubi7,2.7-ubi8,3.6-ubi8,3.8-ubi7,3.8-ubi8,latest NO ruby openshift 2.5-ubi7,2.5-ubi8,2.6-ubi7,2.6-ubi8,2.7-ubi7,2.7-ubi8,latest NO Sample Java Create project odo project create odo-demo Create Application From binary git clone https://gitlab.com/ocp-demo/backend_quarkus && cd backend_quarkus cd code mvn clean package -DskipTests=true -Dquarkus.package.uber-jar=true odo create java backend --s2i --binary target/*.jar Sample output Validation ✓ Validating component [75ms] Please use `odo push` command to create the component with source deployed From source code odo create nodejs frontend --s2i --git https://gitlab.com/ocp-demo/frontend-js Check for odo configuration kind: LocalConfig apiversion: odo.dev/v1alpha1 ComponentSettings: Type: nodejs SourceLocation: https://gitlab.com/ocp-demo/frontend-js SourceType: git Ports: - 8080/TCP Application: app Project: demo Name: frontend Deploy odo push Sample outout Validation ✓ Checking component [125ms] Configuration changes ✓ Initializing component ✓ Creating component [458ms] Applying URL changes ✓ URLs are synced with the cluster, no changes are required. Pushing to component backend of type binary ✓ Checking files for pushing [19ms] ✓ Waiting for component to start [2m] ✓ Syncing files to the component [2s] ✓ Building component [2s] Expose service ( create route) odo url create --port 8080 Sample outout ✓ URL backend-8080 created for component: backend To apply the URL configuration changes, please use `odo push` Remark: you need to run odo push to propagate change to OpenShift ✓ Checking component [150ms] Configuration changes ✓ Retrieving component data [213ms] ✓ Applying configuration [184ms] Applying URL changes ✓ URL backend-8080: http://backend-8080-app-backend-quarkus.apps.cluster-69f4.69f4.sandbox957.opentlc.com/ created Pushing to component backend of type binary ✓ Checking file changes for pushing [10ms] ✓ Waiting for component to start [41ms] ✓ Syncing files to the component [2s] ✓ Building component [3s] "},"helm.html":{"url":"helm.html","title":"Application Deployment with Helm","keywords":"","body":"Deploy Backend app with Helm Chart Deploy backend app using helm chart => backend-chart Test with dry run oc project project1 helm install --dry-run test ./manifests/backend-chart Install chart helm install backend-helm --namespace=project1 ./manifests/backend-chart Sample Output 1 ./manifests/backend-chart NAME: backend-helm LAST DEPLOYED: Tue Feb 16 17:32:46 2021 NAMESPACE: project1 STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: http://backend-helm:8080 Check Helm Chart in Developer Console Topology view Helm Chart details "},"imagestreams.html":{"url":"imagestreams.html","title":"Image Streams","keywords":"","body":"Image Stream Image Stream Automatic trigger deployment Image stream is an abstraction to container image in image registry. Image stream itself does not container any image it just a referece to actual image. You can configure builds and deployments to watch an image stream for notifications when new images are added and react by performing a build or deployment, respectively. Automatic trigger deployment import image with schedule update ( Default is every 15 minutes) oc import-image backend --scheduled --confirm --all --from quay.io/voravitl/backend oc get istag Setup image lookup for backend imagestream oc set image-lookup backend oc set image-lookup --list With image lookup is enabled. Imagestream name can be used in deployment spec: containers: - name: backend image: backend:v1 Check for latest update interval imagestream oc get istag backend:v1 Output NAME IMAGE REFERENCE UPDATED backend:v1 quay.io/voravitl/backend@sha256:19ef0afb88a1ce5d6a4422c7ab8395eb05b672fc27d5d387d9fcd8e15a44c5d7 30 seconds ago Deploy application oc apply -f backend.yaml Set trigger oc set triggers deployment/backend --from-image backend:v1 -c backend Trigger will set following annotation to deployment for container name backend metadata: name: backend annotations: image.openshift.io/triggers: '[{\"from\":{\"kind\":\"ImageStreamTag\",\"name\":\"backend:v1\"},\"fieldPath\":\"spec.template.spec.containers[?(@.name==\\\"backend\\\")].image\"}]' When image on image registry "},"openshift-route.html":{"url":"openshift-route.html","title":"OpenShift Route","keywords":"","body":"Deployment Strategy with OpenShift Route Deployment Strategy with OpenShift Route Application Deployment Blue/Green Deployment Canary Deployment Restrict TLS to v1.2 Test TLS/SSL Application Deployment Deploy 2 version of frontend app. Each deployment and service use label app and version for select each version. Initial Route will routing all traffic to v1. Deploy frontend v1 and v2 and create route frontend.yaml oc apply -f manifests/frontend.yaml -n project1 Blue/Green Deployment Test Route FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') while [ 1 ]; do curl -k $FRONTEND_URL/version echo sleep 1 done Use another terminal to patch route to frontend v2 oc patch route frontend -p '{\"spec\":{\"to\":{\"name\":\"frontend-v2\"}}}' -n project1 Check output from cURL that response is from frontend-v2 Set route back to v1 oc patch route frontend -p '{\"spec\":{\"to\":{\"name\":\"frontend-v1\"}}}' -n project1 Check output from cURL that response is from frontend-v1 Canary Deployment Apply route for Canary deployment to v1 and v2 with 80% and 20% ratio route-with-alternate-backend.yaml oc apply -f manifests/route-with-alternate-backend.yaml -n project1 Call frontend for 10 times. You will get 8 responses from v1 and 2 responses from v2 FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') COUNT=0 while [ $COUNT -lt 10 ]; do curl -k $FRONTEND_URL/version echo sleep .2 COUNT=$(expr $COUNT + 1) done Update weight to 60% and 40% oc patch route frontend -p '{\"spec\":{\"to\":{\"weight\":60}}}' -n project1 oc patch route frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/alternateBackends/0/weight\",\"value\":40}]' -n project1 Re-run previous bash script to loop frontend. This times you will get 6 responses from v1 and 4 responses from v2 Restrict TLS to v1.2 Check default ingresscontroller by run command or use OpenShift Web Admin Console oc edit ingresscontroller default -n openshift-ingress-operator Use Web Admin Console to search for ingressscontroller and select default Minimum TLS version can be specified by attribute minTLSVersion Also test with custom profile, edit tlsProfile: and click Save spec: replicas: 2 tlsSecurityProfile: type: Custom custom: ciphers: - ECDHE-ECDSA-AES128-GCM-SHA256 - ECDHE-RSA-AES128-GCM-SHA256 minTLSVersion: VersionTLS12 Test TLS/SSL To test TLS/SSL encryption enabled on OpenShift ingresscontroller, use https://testssl.sh/ testssl.ssh tool to run report for Ingress VIP support of TLS/SSL ciphers and protocols Run the test docker run --rm -ti drwetter/testssl.sh https://frontend-project1.apps.ocp01.example.com Sample results From OpenShift ingress (default tls profile) Start 2020-12-30 03:37:42 -->> 198.18.1.202:443 (frontend-project1.apps.ocp01.example.com) = 60 days (2020-12-28 05:07 --> 2022-12-28 05:07) > 398 days issued after 2020/09/01 is too long ETS/\"eTLS\", visibility info not present Certificate Revocation List -- OCSP URI -- NOT ok -- neither CRL nor OCSP URI provided OCSP stapling not offered OCSP must staple extension -- DNS CAA RR (experimental) not offered Certificate Transparency -- Certificates provided 2 Issuer ingress-operator@1609132063 Intermediate cert validity #1: ok > 40 days (2022-12-28 05:07). ingress-operator@1609132063 > 198.18.1.202:443 (frontend-project1.apps.ocp01.example.com) From OpenShift ingress (default tls profile) Start 2020-12-30 03:46:29 -->> 198.18.1.202:443 (frontend-project1.apps.ocp01.example.com) = 60 days (2020-12-28 05:07 --> 2022-12-28 05:07) > 398 days issued after 2020/09/01 is too long ETS/\"eTLS\", visibility info not present Certificate Revocation List -- OCSP URI -- NOT ok -- neither CRL nor OCSP URI provided OCSP stapling not offered OCSP must staple extension -- DNS CAA RR (experimental) not offered Certificate Transparency -- Certificates provided 2 Issuer ingress-operator@1609132063 Intermediate cert validity #1: ok > 40 days (2022-12-28 05:07). ingress-operator@1609132063 > 198.18.1.202:443 (frontend-project1.apps.ocp01.example.com) From https://www.google.com -------------------------------------------------------------------------- Start 2020-12-30 03:24:29 -->> 74.125.24.147:443 (www.google.com) 2021-02-02 14:28) ETS/\"eTLS\", visibility info not present Certificate Revocation List http://crl.pki.goog/GTS1O1core.crl OCSP URI http://ocsp.pki.goog/gts1o1core OCSP stapling not offered OCSP must staple extension -- DNS CAA RR (experimental) available - please check for match with \"Issuer\" below: issue=pki.goog Certificate Transparency yes (certificate extension) Certificates provided 2 Issuer GTS CA 1O1 (Google Trust Services from US) Intermediate cert validity #1: ok > 40 days (2021-12-15 00:00). GTS CA 1O1 2021-02-02 14:41) ETS/\"eTLS\", visibility info not present Certificate Revocation List http://crl.pki.goog/GTS1O1core.crl OCSP URI http://ocsp.pki.goog/gts1o1core OCSP stapling not offered OCSP must staple extension -- DNS CAA RR (experimental) available - please check for match with \"Issuer\" below: issue=pki.goog Certificate Transparency yes (certificate extension) Certificates provided 2 Issuer GTS CA 1O1 (Google Trust Services from US) Intermediate cert validity #1: ok > 40 days (2021-12-15 00:00). GTS CA 1O1 > 74.125.24.147:443 (www.google.com) "},"hpa.html":{"url":"hpa.html","title":"Horizontal Pod Autoscaler","keywords":"","body":"Horizontal Pod Autoscaler (HPA) Horizontal Pod Autoscaler (HPA) CPU Memory Custom Metrics CPU Deploy frontend app (if you still not deploy it yet)oc new-project project1 oc apply -f manifests/frontend.yaml -n project1 oc delete deployment frontend-v2 -n project1 FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') curl -k $FRONTEND_URL Review CPU HPA for deployment frontend v1 Scale out when average CPU utilization is greater than 80% of CPU limit Maximum pods is 3 Scale down to min replicas if utilization is lower than threshold for 60 sec apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: frontend-v1-cpu namespace: project1 spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: frontend-v1 minReplicas: 1 maxReplicas: 3 metrics: - type: Resource resource: name: cpu target: averageUtilization: 80 type: Utilization behavior: scaleDown: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 15 Create CPU HPA for deployment frontend v1 oc create -f manifests/frontend-v1-cpu-hpa.yaml -n project1 Check HPA status watch oc get horizontalpodautoscaler/frontend-v1-cpu -n project1 Generate load with load test tool (siege) FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') siege -c 40 $FRONTEND_URL If you don't have siege, run k6 as pod on OpenShift 40 threads Duration 3 minutes Ramp up 30 sec Ramp down 30 sec FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') oc run load-test -n project1 -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Remark: k6 will run as pod name load-test for 4 minutes if you want to force stop before 4 minutes just delete load-test pod oc delete pod load-test -n project1 Wait for HPA to trigger Check Developer Console Stop load test and wait 1 minutes to scale down to 1 replica. Memory Review Memory HPA for deployment frontend v1 Scale out when average memory utilization is greater than 60M of memory limit Maximum pods is 3 Scale down to min replicas if utilization is lower than threshold for 60 secapiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: frontend-v1-memory spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: frontend-v1 minReplicas: 1 maxReplicas: 3 metrics: - type: Resource resource: name: memory target: type: Utilization averageValue: 50Mi behavior: scaleDown: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 15 Create Memory HPA for deployment frontend v1oc delete -f manifests/frontend-v1-cpu-hpa.yaml -n project1 oc create -f manifests/frontend-v1-memory-hpa.yaml -n project1 Check HPA statuswatch oc get horizontalpodautoscaler/frontend-v1-memory -n project1 Generate load with load test tool (siege)FRONTEND_URL=https://$(oc get route frontend -n project1 -jsonpath='{.spec.host}') siege -c 40 $FRONTEND_URL Wait for HPA to trigger Check memory utilization from Developer console Stop load test and wait 1 minutes to scale down to 1 replica. Custom Metrics WIP "},"health.html":{"url":"health.html","title":"Health Check","keywords":"","body":"Pod Health Check Pod Health Check Prerequisite Readiness, Livenss and Startup Probe Configure and Test Probes Command Line Developer Console Prerequisite Deploy frontend app (if you still not deploy it yet) oc apply -f manifests/frontend.yaml -n project1 oc delete deployment frontend-v2 -n project1 Readiness, Livenss and Startup Probe Kubernetes provide 3 types of probe to check pod's health Readiness, check that pod is ready for process request or not. If success, service will allow traffic to this pod. Liveness, check that pod is dead or alive. Pod will be restarted if liveness probe is failed. Startup, for dealing with long startup time pod. Set probe with same as liveness with a duration to failureThreshold * periodSecond. Liveness probe will takeover Startup probe after startup is success. Frontend application provides health check with following URI URI Description /health/live Livenness probe URL /health/ready Readiness probe URL For demo purpose we can set readiness and liveness by following URI URI Description /stop Set liveness to false /start Set liveness to true /not_ready Set readiness to false /ready Set readiness to true Test frontend app readiness Connect to podPOD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc -n project1 rsh $POD Test frontend appcurl http://localhost:8080/ Test readiness probecurl http://localhost:8080/health/ready Set pod to not ready state curl http://localhost:8080/not_ready Test frontend readiness probe and test app again. You will get 503 Service Unavailable response code.curl -v http://localhost:8080/health/ready curl -v http://localhost:8080/ Set pod to ready statecurl http://localhost:8080/ready Configure and Test Probes Command Line Test frontend app liveness Connect to podPOD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc -n project1 rsh $POD Test frontend appcurl http://localhost:8080/ Test liveness probecurl http://localhost:8080/health/live Set pod to not ready state curl http://localhost:8080/stop Test frontend liveness probe and test app again. You will get 503 Service Unavailable response code.curl -v http://localhost:8080/health/live curl -v http://localhost:8080/ Set pod to ready statecurl http://localhost:8080/start Configure Readiness, Liveness and Startup probe oc rollout pause deployment/frontend-v1 -n project1 oc set probe deployment/frontend-v1 --readiness --get-url=http://:8080/health/ready --initial-delay-seconds=8 --failure-threshold=1 --period-seconds=3 --timeout-seconds=5 -n project1 oc set probe deployment/frontend-v1 --liveness --get-url=http://:8080/health/live --initial-delay-seconds=5 --failure-threshold=1 --period-seconds=10 --timeout-seconds=5 -n project1 oc set probe deployment/frontend-v1 --startup --get-url=http://:8080/health/live --initial-delay-seconds=5 --period-seconds=10 -n project1 oc rollout resume deployment/frontend-v1 -n project1 watch oc get pods -n project1 Check pod status. New pod is created and previous pod is terminated. NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-rhzwt 0/1 ContainerCreating 0 3s frontend-v1-c5d4648f9-fkc84 1/1 Running 0 49s Scale frontend-v1 to 3 pods oc scale deployment/frontend-v1 --replicas=3 Test Liveness Probe Test live probe by set one frontend pod to return 503 for liveness probe POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/stop printf \"\\n%s is dead\\n\" $POD Check pod's events oc describe pod $POD -n project1 Sample output Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m57s default-scheduler Successfully assigned project1/frontend-v1-5d8c4ccc8c-j9b6d to ip-10-0-148-247.ap-southeast-1.compute.internal Normal AddedInterface 3m55s multus Add eth0 [10.131.0.112/23] Normal Pulling 3m54s kubelet Pulling image \"quay.io/voravitl/frontend-js:v1\" Normal Pulled 3m51s kubelet Successfully pulled image \"quay.io/voravitl/frontend-js:v1\" in 2.988138314s Normal Created 3m51s kubelet Created container frontend Normal Started 3m51s kubelet Started container frontend Warning Unhealthy 24s kubelet Liveness probe failed: HTTP probe failed with statuscode: 503 Normal Killing 24s kubelet Container frontend failed liveness probe, will be restarted Check that pod is restarted oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-j9b6d 1/1 Running 1 5m28s frontend-v1-5d8c4ccc8c-lx4xb 1/1 Running 0 4m3s frontend-v1-5d8c4ccc8c-qvxwp 1/1 Running 0 4m3s Test Readiness Probe Test live probe by set one frontend pod to return 503 for liveness probe POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/not_ready printf \"\\n%s is not ready\\n\" $POD Check pod status oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-5d8c4ccc8c-j9b6d 0/1 Running 1 5m28s frontend-v1-5d8c4ccc8c-lx4xb 1/1 Running 0 4m3s frontend-v1-5d8c4ccc8c-qvxwp 1/1 Running 0 4m3s Check pod's events oc describe pod/$POD -n project1 Sample Output Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m57s default-scheduler Successfully assigned project1/frontend-v1-5d8c4ccc8c-j9b6d to ip-10-0-148-247.ap-southeast-1.compute.internal Normal AddedInterface 3m55s multus Add eth0 [10.131.0.112/23] Normal Pulling 3m54s kubelet Pulling image \"quay.io/voravitl/frontend-js:v1\" Normal Pulled 3m51s kubelet Successfully pulled image \"quay.io/voravitl/frontend-js:v1\" in 2.988138314s Normal Created 3m51s kubelet Created container frontend Normal Started 3m51s kubelet Started container frontend Warning Unhealthy 24s kubelet Liveness probe failed: HTTP probe failed with statuscode: 503 Normal Killing 24s kubelet Container frontend failed liveness probe, will be restarted Check that pod is removed from service oc describe svc/frontend-v1 -n project1 Test readiness probe. Notice that all responses will not come from not ready pod. while [ 1 ]; do curl -k https://$(oc get route/frontend -n project1 -o jsonpath='{.spec.host}') printf \"\\n\" sleep 10 done Use another terminal to set not ready pod back to ready. Notice that response now including all 3 pods. POD=$(oc get pods --no-headers -n project1 | grep frontend |head -n 1| awk '{print $1}') oc exec -n project1 $POD -- curl -s http://localhost:8080/ready printf \"\\n%s is ready\\n\" $POD Check developer console Developer Console Remove probes from previous stepsoc set probe deployment/frontend-v1 --remove --readiness --liveness --startup -n project1 watch oc get pods -n project1 Login to OpenShift Web Admin Console and change to Developer Console Select topology then select frontend-v1 deployment, then select \"Add Health Checks\" Add health checks "},"openshift-service-mesh.html":{"url":"openshift-service-mesh.html","title":"OpenShift Service Mesh","keywords":"","body":"OpenShift Service Mesh OpenShift Service Mesh Overview Setup Control Plane and sidecar Create Istio Gateway Weight-Routing with Istio Virtual Service Routing by condition based on URI A/B with Istio Virtual Service Traffic Analysis Distributed Tracing Traffic Mirroring (Dark Launch) Envoy Access Log Circuit Breaker Secure with mTLS JWT Token Red Hat Single Sign-On RequestAuthentication and Authorization Policy Service Level Objective (SLO) Control Plane with High Availability OpenShift Service Mesh 1.x OpenShift Service Mesh 2.x Overview Sample application Setup Control Plane and sidecar Install following Operators from OperatorHub ElasticSearch Jaeger Kiali OpenShift Service Mesh Create control plane by create ServiceMeshControlPlane CRD oc new-project istio-system oc create -f manifests/smcp.yaml -n istio-system Check for control plane(get-smcp-status.sh) bin/get-smcp-status.sh istio-system Join project1 into control plane Review ServiceMeshMemberRoll CRD apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - project1 Create data plane project oc new-project project1 Apply ServiceMeshMemberRoll oc create -f manifests/smmr.yaml -n istio-system Check for ServiceMeshMemberRoll status oc describe smmr/default -n istio-system | grep -A2 Spec: Deploy sidecar to frontend app in project1 oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc patch deployment/frontend-v2 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 Check for sidecar in frontend-v1 and frontend-v2 pods oc get pods -n project1 Sample output NAME READY STATUS RESTARTS AGE frontend-v1-577b98f48c-6j5zg 2/2 Running 0 15s frontend-v1-c5d4648f9-7jfk2 1/1 Terminating 0 13m frontend-v2-5cd968bc59-cwsd8 2/2 Running 0 14s frontend-v2-5d4dbdbc9-k6787 1/1 Terminating 0 13m Check developer console Check frontend service which set slector to both v1 and v2 selector: app: frontend Create frontend service oc create -f manifests/frontend-service.yaml -n project1 Create Istio Gateway Create Gateway for frontend app Check for cluster's sub-domainSUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') echo $SUBDOMAIN Review Gateway CRD, Replaced SUBDOMAIN with cluster's sub-domain apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: frontend-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http2 protocol: HTTP hosts: - 'frontend.apps.SUBDOMAIN' Replace SUBDOMAIN with your clsuter sub-domain and Create gateway oc apply -f manifests/frontend-gateway.yaml -n istio-system or use following bash command SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-gateway.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n istio-system -f - Create Destination Rule for frontend v1 and frontend v2 Review Destination Rule CRD apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: frontend spec: host: frontend subsets: - name: v1 labels: app: frontend version: v1 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v2 labels: app: frontend version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN Create destination rule oc apply -f manifests/frontend-destination-rule.yaml -n project1 Create Virtual Service for frontend app Review Virtual Service CRD, Replace SUBDOMAIN with cluster's sub-domain. apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.SUBDOMAIN gateways: - istio-system/frontend-gateway http: - route: - destination: port: number: 8080 host: frontend.project1.svc.cluster.local Replace SUBDOMAIN with cluster subdomain and create virtual service oc apply -f manifests/frontend-virtual-service.yaml -n project1 or use following bash command SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - Check that route is automatically created oc get route -n istio-system | grep istio-system-frontend-gateway Sample outout istio-system-frontend-gateway-fmlsp frontend.apps.cluster-ba08.ba08.example.opentlc.com istio-ingressgateway http2 istio-ingressgateway http2 None Review Route, Replace SUBDOMAIN with cluster's subdomain apiVersion: v1 kind: Route metadata: name: frontend spec: host: frontend.apps.SUBDOMAIN port: targetPort: http2 to: kind: Service name: istio-ingressgateway weight: 100 wildcardPolicy: None Replace SUBDOMAIN with cluster subdomain then create Route oc apply -f manifests/frontend-route-istio.yaml -n istio-system or use following bash command bash SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-route-istio.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - --> Test with cURL FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl http://$FRONTEND_ISTIO_ROUTE Weight-Routing with Istio Virtual Service Set weight routing between 2 services with virtual service Check for virtual service with weight routing, Replace SUBDOMAIN with cluster's subdomain.apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.SUBDOMAIN gateways: - istio-system/frontend-gateway http: - route: - destination: port: number: 8080 host: frontend.project1.svc.cluster.local subset: v1 weight: 100 - destination: port: number: 8080 host: frontend.project1.svc.cluster.local subset: v2 weight: 0 or use following bash command SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-weight-routing.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - Apply virtual service for Blue/Green deployment with route all traffic to v1 oc apply -f manifests/frontend-virtual-service-with-weight-routing.yaml -n project1 Test with cURL to verify that all requests are routed to v1 Blue/Green deployment by route all requests to v2 oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":0},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":100}]}}]' -n project1 Test with cURL to verify that all requests are routed to v2 Canary deployment by weight requests between v1 and v2 with 70% and 30% oc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":70},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":30}]}}]' -n project1 Test canary deployment Run 100 requests FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') COUNT=0 rm -f result.txt while [ $COUNT -lt 100 ]; do OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE/version) printf \"%s\\n\" $OUTPUT >> result.txt printf \"%s\\n\" $OUTPUT sleep .2 COUNT=$(expr $COUNT + 1) done Check result for comparing percentage of requests to v1 and v2 printf \"Version 1: %s\\n\" $(cat result.txt | grep \"1.0.0\" | wc -l) printf \"Version 2: %s\\n\" $(cat result.txt | grep \"2.0.0\" | wc -l) rm -f result.txt Routing by condition based on URI Set conditional routing between 2 services with virtual service Check for virtual service by URI, Replace SUBDOMAIN with cluster's subdomain. Condition with regular expression Route to v1 if request URI start with \"/ver\" and end with \"1\"apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.SUBDOMAIN gateways: - istio-system/frontend-gateway http: - match: - uri: regex: /ver(.*)1 rewrite: # Rewrite URI back to / because frontend app not have /ver(*)1 uri: \"/\" route: - destination: host: frontend port: number: 8080 subset: v1 - route: - destination: host: frontend port: number: 8080 subset: v2 Apply virtual service oc apply -f manifests/frontend-virtual-service-with-uri.yaml -n project1 or use following bash command SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-uri.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - Test with URI /version1 and /ver1 FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl $FRONTEND_ISTIO_ROUTE/version1 curl $FRONTEND_ISTIO_ROUTE/vers1 curl $FRONTEND_ISTIO_ROUTE/ver1 Test with URI / FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl $FRONTEND_ISTIO_ROUTE/ A/B with Istio Virtual Service A/B testing by investigating User-Agent header with Virtual Service, Replace SUBDOMAIN with cluster's sub-domain. If HTTP header User-Agent contains text Firewall, request will be routed to frontend v2 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - frontend.apps.SUBDOMAIN gateways: - istio-gateway/frontend-gateway http: - match: - headers: user-agent: regex: (.*)Firefox(.*) route: - destination: host: frontend port: number: 8080 subset: v2 - route: - destination: host: frontend port: number: 8080 subset: v1 Apply Virtual Service oc apply -f manifests/frontend-virtual-service-with-header.yaml -n project1 or use following bash command SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service-with-header.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - Test with cURL with HTTP header User-Agent contains Firefox FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl -H \"User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" $FRONTEND_ISTIO_ROUTE Traffic Analysis Deploy backend application oc apply -f manifests/backend.yaml -n project1 oc apply -f manifests/backend-destination-rule.yaml -n project1 oc apply -f manifests/backend-virtual-service-v1-v2-50-50.yaml -n project1 oc get pods -n project1 Optional: Draw connetion from frontend to backend in Developer Console oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc annotate deployment frontend-v2 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 Configure frontend to request to backend oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc set env deployment/frontend-v2 BACKEND_URL=http://backend:8080/ -n project1 Check Kiali Console login to OpenShift Developer Console, select project istio-system and open Kiali console Login to Kiali Console and select Graph Namespace: select checkbox \"project1\" Display: select checkbox \"Requests percentage\" and \"Traffic animation\" Run following commandoc patch virtualservice frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/http/0\",\"value\":{\"route\":[{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v1\"},\"weight\":70},{\"destination\":{\"host\":\"frontend.project1.svc.cluster.local\",\"port\":{\"number\":8080},\"subset\":\"v2\"},\"weight\":30}]}}]' -n project1 FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') while [ 1 ]; do OUTPUT=$(curl -s $FRONTEND_ISTIO_ROUTE) printf \"%s\\n\" $OUTPUT sleep .2 done Check Kiali Console Traffic analysis for frontend app. Select Application->frontend->inbound traffic and outbound traffic Distributed Tracing Distributed tracing with Jaeger. Select tab Tracing Overall tracing for frontend app Login to Jaeger by select \"View in Tracing\" Drill down to tracing information Show feature config on the fly in service --> frontend v2 --> action Traffic Mirroring (Dark Launch) Deploy audit app and mirror every requests that frontend call backend to audit app oc apply -f manifests/audit-app.yaml -n project1 oc get pods -n project1 Update backend virtual service to mirror requests to audit app. oc apply -f manifests/backend-virtual-service-mirror.yaml -n project1 Use cURL to call frontend and check audit's pod log by CLI (with another terminal) or Web Console cURL frontend FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl $FRONTEND_ISTIO_ROUTE View audit log oc logs -f $(oc get pods --no-headers | grep audit|head -n 1|awk '{print $1}') -c backend -n project1 Envoy Access Log Envoy access log already enabled with ServiceMeshControlPlane CRD proxy: accessLogging: envoyService: enabled: false file: encoding: TEXT name: /dev/stdout Check access log oc logs -f $(oc get pods -n project1 --no-headers|grep frontend|head -n 1|awk '{print $1}') -c istio-proxy -n project1 Sample output [2020-12-25T10:33:04.848Z] \"GET / HTTP/1.1\" 200 - \"-\" \"-\" 0 103 5750 5749 \"-\" \"-\" \"0c3ce34a-f5a0-9340-b84f-3631cd8eb444\" \"backend:8080\" \"10.128.2.133:8080\" outbound|8080|v2|backend.project1.svc.cluster.local 10.128.2.131:48300 172.30.116.252:8080 10.128.2.131:36992 - - [2020-12-25T10:33:04.846Z] \"GET / HTTP/1.1\" 200 - \"-\" \"-\" 0 184 5756 5755 \"184.22.250.124,10.131.0.4\" \"curl/7.64.1\" \"0c3ce34a-f5a0-9340-b84f-3631cd8eb444\" \"frontend.apps.cluster-1138.1138.example.opentlc.com\" \"127.0.0.1:8080\" inbound|8080|http|frontend-v1.project1.svc.cluster.local 127.0.0.1:56540 10.128.2.131:8080 10.131.0.4:0 outbound_.8080_.v1_.frontend.project1.svc.cluster.local default Circuit Breaker Configure our application to contains only frontend-v1 and backend-v1 and scale backend to 3 pods. oc apply -f manifests/frontend.yaml -n project1 oc patch deployment/frontend-v1 -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"true\"}}}}}' -n project1 oc apply -f manifests/backend.yaml -n project1 oc delete deployment frontend-v2 -n project1 oc delete deployment backend-v2 -n project1 oc delete svc frontend-v2 -n project1 oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc annotate deployment frontend-v1 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v1\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"backend-v2\"}]' -n project1 oc delete route frontend -n project1 oc scale deployment backend-v1 --replicas=3 -n project1 oc apply -f manifests/backend-destination-rule-v1-only.yaml -n project1 oc apply -f manifests/backend-virtual-service.yaml -n project1 oc apply -f manifests/frontend-destination-rule-v1-only.yaml -n project1 SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-virtual-service.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n project1 -f - SUBDOMAIN=$(oc whoami --show-console|awk -F'apps.' '{print $2}') cat manifests/frontend-gateway.yaml | sed 's/SUBDOMAIN/'$SUBDOMAIN'/'|oc apply -n istio-system -f - oc get pods -n project1 Test with cURL FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') curl http://$FRONTEND_ISTIO_ROUTE Sample output - Check for field Host that is backend pod that processed for this request Frontend version: 1.0.0 => [Backend: http://backend:8080/, Response: 200, Body: Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Hello, Quarkus] Loop 6 times. Result from backend will be round robin. Create bash function function loop_frontend(){ FRONTEND_ISTIO_ROUTE=$(oc get route -n istio-system|grep istio-system-frontend-gateway |awk '{print $2}') COUNT=0 MAX=$1 while [ $COUNT -lt $MAX ]; do curl -s http://$FRONTEND_ISTIO_ROUTE | awk -F',' '{print $5 \"=>\" $6}' COUNT=$(expr $COUNT + 1 ) done } Run function with input paramter 6 loop_frontend 6 Sample output Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-h7rwg=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 By default, Envoy will automatically retry if it get response with code 503 Force one backend pod to return 503 by command line. oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -s http://localhost:8080/not_ready Sample output Backend version:v1, Response:200, Host:backend-v1-f4dbf777f-h7rwg, Status:200, Message: Readiness: false by Web Console Verify response from that pod. oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -sv http://localhost:8080/ Sample Output * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8080 (#0) > GET / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.61.1 > Accept: */* > Test with cURL again. You will get only status 200 loop_frontend 10 Sample Output Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Set backend pod to return 200 oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -s http://localhost:8080/ready Test CB Update destination rule with circuit breaker oc apply -f manifests/backend-destination-rule-circuit-breaker.yaml -n project1 Review Circuit Breaker configuration in deatination rule If found error 1 times (consecutiveErrors) then eject that pod from pool for 15 mintues (baseEjectionTime) Maximum number of pod that can be ejected is 100% (maxEjectionPercent) Check this every 15 min (interval) outlierDetection: baseEjectionTime: 15m consecutiveErrors: 1 interval: 15m maxEjectionPercent: 100 Set one backend pod to return 504 and verify that pod return 504 oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -sv http://localhost:8080/stop Sample output * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8080 (#0) > GET /stop HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.61.1 > Accept: */* > Verify that backend pod return 504 oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -sv http://localhost:8080/ Sample output * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8080 (#0) > GET / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.61.1 > Accept: */* > Test again with cURL. You will get 504 just one times. loop_frontend 15 Sample output Host:backend-v1-f4dbf777f-h7rwg=> Status:504 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Host:backend-v1-f4dbf777f-vjhcl=> Status:200 Host:backend-v1-f4dbf777f-tgssd=> Status:200 Check Kiali Console. Remark that there is lightning icon at backend service. This is represent for circuit breaker. Set backend pod to normal status oc exec -n project1 -c backend $(oc get pod -n project1 | grep -m1 backend | cut -d \" \" -f1) -- curl -sv http://localhost:8080/start Secure with mTLS Check following Git for setup mTLS between service and ingress service Secure Application with mTLS by OpenShift Service Mesh JWT Token Red Hat Single Sign-On Setup Red Hat Single Sign-On (Keycloak) Create namespace oc new-project sso --description=\"Red Hat Single Sign-On\" --display-name=\"Red Hat Single Sign-On\" Install Red Hat Single Sign-On Operator Install to namespace sso Create Keycloak instance oc create -f manifests/keycloak.yaml -n sso watch oc get pods -n sso Cosmetic topology view oc label deployment/keycloak-postgresql app.kubernetes.io/name=postgresql -n sso oc annotate statefulset/keycloak 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"keycloak-postgresql\"}]' -n sso Extract admin password KEYCLOAK_ADMIN_PASSWORD=$(oc extract secret/credential-keycloak -n sso --to=- --keys=ADMIN_PASSWORD 2>/dev/null) Create Realm and Keycloak Client with Client Credential oc create -f manifests/keycloak-realm.yaml -n sso oc create -f manifests/keycloak-client.yaml -n sso Client secret Test login with client secret KEYCLOAK=$(oc get route/keycloak -n sso -o jsonpath='{.spec.host}') CLIENT_SECRET=e31fe61b-2cc1-41da-9644-d72bdb8339d5 TOKEN=$(curl -s --location --request \\ POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode client_id=frontend-app \\ --data-urlencode client_secret=$CLIENT_SECRET \\ --data-urlencode scope=email \\ --data-urlencode grant_type=client_credentials | jq .access_token | sed s/\\\"//g) RequestAuthentication and Authorization Policy Create RequestAuthentication and AuthorizationPolicy oc apply -f manitests/frontend-jwt -n project1 Test without JWT token. You will get HTTP resonse code 403 curl -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com Test with invalid JWT token. You will get HTTP resonse code 401 Test with valid JWT token TOKEN=$(curl -s --location --request \\ POST https://$KEYCLOAK/auth/realms/demo/protocol/openid-connect/token \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode client_id=frontend-app \\ --data-urlencode client_secret=$CLIENT_SECRET \\ --data-urlencode scope=email \\ --data-urlencode grant_type=client_credentials | jq .access_token | sed s/\\\"//g) curl --header 'Authorization: Bearer '$TOKEN -kv https://frontend-user1.apps.cluster-7bbc.7bbc.sandbox1708.opentlc.com Test with expired JWT token (token life is 5 minutes). You will get HTTP response code 401 with following message Jwt is expired* Closing connection 0 Service Level Objective (SLO) We can use Service Level Indicator (SLI) and Service Level Objective (SLO) to determine and measure availability of services. For RESTful Web Service we can use HTTP response code to measure for SLI Prometheus in Service Mesh's control plane contains information about HTTP responses then we can use following PromQL to check for the sucessfull request and total request of backend service Success Rate Successful request for last 5 minutessum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) Total requests for last 5 minutessum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m])) Sample data provided by Prometheusistio_requests_total{connection_security_policy=\"unknown\",destination_app=\"backend\",destination_canonical_revision=\"v1\",destination_canonical_service=\"backend\",destination_principal=\"spiffe://cluster.local/ns/user1/sa/default\",destination_service=\"backend.user1.svc.cluster.local\",destination_service_name=\"backend\",destination_service_namespace=\"user1\",destination_version=\"v1\",destination_workload=\"backend-v1\",destination_workload_namespace=\"user1\",instance=\"10.128.2.42:15090\",job=\"envoy-stats\",namespace=\"user1\",pod_name=\"frontend-v1-66fbd89459-8ksr8\",reporter=\"source\",request_protocol=\"http\",response_code=\"503\",response_flags=\"URX\",source_app=\"frontend\",source_canonical_revision=\"v1\",source_canonical_service=\"frontend\",source_principal=\"spiffe://cluster.local/ns/user1/sa/default\",source_version=\"v1\",source_workload=\"frontend-v1\",source_workload_namespace=\"user1\"} Latency 99th Percentile of response time in sec of frontend servicehistogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"frontend\",response_code!~\"5*\"}[5m])) by (le))/1000 SLO for success rate can be calculated by following PromQL and compare this to your desired service level e.g. 99.9%sum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) / sum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m]))*100 Configure Grafana Dashboard in OpenShift Service Mesh's control plane for measuring SLO Dashbaord Backend Application service %availabilitysum(increase(istio_requests_total{destination_service_name=\"backend\",response_code!~\"5.*\"}[5m])) / sum(increase(istio_requests_total{destination_service_name=\"backend\"}[5m])) *100 Frontend 99th percentile response time in secondhistogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"frontend\",response_code!~\"5*\"}[5m])) by (le))/1000 Backend 99th percentile response time in secondhistogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name=\"backend\",response_code!~\"5*\"}[5m])) by (le))/1000 --> Control Plane with High Availability OpenShift Service Mesh 1.x ServiceMeshControlPlane with high availability configuration Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway Set request and limit Set autoscaling to true Set number of min and max replicas with target CPU utilization to trigger HPA ingress: enabled: true runtime: container: resources: requests: cpu: 500m memory: 300Mi limits: cpu: 2 memory: 1Gi deployment: autoScaling: enabled: true maxReplicas: 4 minReplicas: 2 targetCPUUtilizationPercentage: 85 For others components Set number of replicas to 2 deployment: autoScaling: enabled: false replicas: 2 Set pod anti-affinity to prevent scheduler to place pods to the same node Remark: namespaces in podAntiAffinity is needed to support multiples control planes in the same OpenShift cluster. Change this to match name of control plane's namespace pod: tolerations: - key: node.kubernetes.io/unreachable operator: Exists effect: NoExecute tolerationSeconds: 60 affinity: podAntiAffinity: requiredDuringScheduling: - key: istio topologyKey: kubernetes.io/hostname operator: In values: - galley namespaces: istio-system Check that pods of each deployment run on different nodes oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase Output NAME NODE PHASE grafana-7bdb4fb848-847c8 ip-10-0-160-48.us-east-2.compute.internal Running istio-citadel-6668b5b947-njgbb ip-10-0-160-48.us-east-2.compute.internal Running istio-citadel-6668b5b947-nk9dz ip-10-0-137-21.us-east-2.compute.internal Running istio-galley-6dc7f9c496-hkm57 ip-10-0-137-21.us-east-2.compute.internal Running istio-galley-6dc7f9c496-qcw9q ip-10-0-160-48.us-east-2.compute.internal Running istio-ingressgateway-6bcd484457-25tq7 ip-10-0-137-21.us-east-2.compute.internal Running istio-ingressgateway-6bcd484457-nvfb9 ip-10-0-160-48.us-east-2.compute.internal Running istio-pilot-74d5db759c-m9jxm ip-10-0-137-21.us-east-2.compute.internal Running istio-pilot-74d5db759c-rcdxj ip-10-0-160-48.us-east-2.compute.internal Running istio-policy-58ff56d7dc-26wsq ip-10-0-137-21.us-east-2.compute.internal Running istio-policy-58ff56d7dc-62gwl ip-10-0-160-48.us-east-2.compute.internal Running istio-sidecar-injector-ffc58c87-4t5gc ip-10-0-137-21.us-east-2.compute.internal Running istio-sidecar-injector-ffc58c87-rjz7l ip-10-0-160-48.us-east-2.compute.internal Running istio-telemetry-646d7cf56c-fz72g ip-10-0-137-21.us-east-2.compute.internal Running istio-telemetry-646d7cf56c-lctxg ip-10-0-160-48.us-east-2.compute.internal Running jaeger-7b866d475f-nhrp5 ip-10-0-160-48.us-east-2.compute.internal Running kiali-75dc58b5f6-bwk7q ip-10-0-137-21.us-east-2.compute.internal Running prometheus-85db9d786b-vzskf ip-10-0-160-48.us-east-2.compute.internal Running prometheus-85db9d786b-wgrwz ip-10-0-137-21.us-east-2.compute.internal Running Verify HPA for ingress gateway oc get hpa -n istio-system Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-ingressgateway Deployment/istio-ingressgateway 0%/85% 2 4 2 10m OpenShift Service Mesh 2.x ServiceMeshControlPlane with high availability configuration Configure Horizontal Pod Autoscaler (HPA) for ingress-gateway Set request and limit Set autoscaling to true Set number of min and max replicas with target CPU utilization to trigger HPA ingress: enabled: true runtime: container: resources: requests: cpu: 500m memory: 300Mi limits: cpu: 2 memory: 1Gi deployment: autoScaling: enabled: true maxReplicas: 4 minReplicas: 2 targetCPUUtilizationPercentage: 85 For others components Set number of replicas to 2 pilot: deployment: replicas: 2 Check that pods of each deployment run on different nodes oc get pods -n istio-system Output grafana-78f656547-gkm92 2/2 Running 0 54s istio-ingressgateway-667749f4bd-pfl2l 1/1 Running 0 54s istio-ingressgateway-667749f4bd-sfwx4 1/1 Running 0 39s istiod-basic-install-6994d86579-4n8jf 1/1 Running 0 77s istiod-basic-install-6994d86579-b5bgv 1/1 Running 0 77s jaeger-85d4744d8b-krqfl 2/2 Running 0 54s kiali-784df775f8-xccsw 1/1 Running 0 28s prometheus-79ff59d59f-6j99k 3/3 Running 0 65s prometheus-79ff59d59f-msrpb 3/3 Running 0 65s Verify HPA for ingress gateway oc get hpa -n istio-system Output NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-ingressgateway Deployment/istio-ingressgateway 0%/85% 2 4 2 10m Check that pods of each deployment run on different nodes oc get pods -o wide -n istio-system -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,PHASE:.status.phase Output NAME NODE PHASE grafana-99f7c499f-jnd9k ip-10-0-166-202.us-east-2.compute.internal Running istio-ingressgateway-5fc94885b5-hjhqw ip-10-0-166-202.us-east-2.compute.internal Running istio-ingressgateway-5fc94885b5-hxn9r ip-10-0-151-28.us-east-2.compute.internal Running istiod-basic-install-58c9bc5bf8-4wbhq ip-10-0-151-28.us-east-2.compute.internal Running jaeger-596448d54d-gwn97 ip-10-0-166-202.us-east-2.compute.internal Running kiali-85c677967c-k7767 ip-10-0-166-202.us-east-2.compute.internal Running prometheus-565c997f45-plqqb ip-10-0-151-28.us-east-2.compute.internal Running prometheus-565c997f45-s7q2t ip-10-0-166-202.us-east-2.compute.internal Running "},"application-metrics.html":{"url":"application-metrics.html","title":"User Workload Monitoring","keywords":"","body":"User Workload Metrics User Workload Metrics Prerequisites Custom Alert Prerequisites Setup User Workload Monitoring Remark: You may need to change storageClassName based on your cluster configuration oc apply -f manifests/cluster-monitoring-config.yaml Verify monitoring stack oc get pod -n openshift-user-workload-monitoring Sample output NAME READY STATUS RESTARTS AGE prometheus-operator-5fc7d894dc-9nlhc 2/2 Running 0 9m3s prometheus-user-workload-0 4/4 Running 1 5m45s prometheus-user-workload-1 4/4 Running 1 6m1s thanos-ruler-user-workload-0 3/3 Running 5 8m55s thanos-ruler-user-workload-1 3/3 Running 0 11s Service Monitoring Deploy application with custom metrics Backend application provides metrics by /q/metrics and /q/metrics/applications oc apply -f manifests/frontend.yaml -n project1 oc apply -f manifests/backend.yaml -n project1 oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc set env deployment/frontend-v2 BACKEND_URL=http://backend:8080/ -n project1 Remark: If frontend and backend deployment configured with service mesh and mTLS for dataplane is enbled. User workload monitoring will not work properly because prometheus-user-workload doesn't have sidecar and not in the service mesh. Test backend's metrics oc exec -n project1 $(oc get pods -n project1 | grep backend | head -n 1 | awk '{print $1}') \\ -- curl -Ls http://localhost:8080/q/metrics Sample output # TYPE vendor_memory_committedNonHeap_bytes gauge vendor_memory_committedNonHeap_bytes 3.1780976E7 # HELP vendor_memory_maxNonHeap_bytes Displays the maximum amount of used non-heap memory in bytes. # TYPE vendor_memory_maxNonHeap_bytes gauge vendor_memory_maxNonHeap_bytes -1.0 # HELP vendor_memory_usedNonHeap_bytes Displays the amount of used non-heap memory in bytes. # TYPE vendor_memory_usedNonHeap_bytes gauge vendor_memory_usedNonHeap_bytes 3.1780976E7 Test backend application level's metrics oc exec -n project1 $(oc get pods -n project1 | grep backend | head -n 1 | awk '{print $1}') \\ -- curl -Ls http://localhost:8080/q/metrics/application Sample output # HELP application_com_example_quarkus_BackendResource_timeBackend_seconds Times how long it takes to invoke the backend method # TYPE application_com_example_quarkus_BackendResource_timeBackend_seconds summary application_com_example_quarkus_BackendResource_timeBackend_seconds_count 889.0 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.5\"} 0.213724005 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.75\"} 0.213724005 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.95\"} 0.213724005 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.98\"} 0.213724005 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.99\"} 0.213724005 application_com_example_quarkus_BackendResource_timeBackend_seconds{quantile=\"0.999\"} 0.213724005 Create Service Monitoring to monitor backend service oc apply -f manifests/backend-monitor.yaml -n project1 Remark: Role monitor-edit is required for create ServiceMonitor and PodMonitor resources. Following example is granting role montior-edit to user1 for project1 oc adm policy add-role-to-user monitoring-edit user1 -n project1 Developer Console monitoring metrics Select application metrics Application metrics PromQL for query qequest rate for last 1 minute rate(application_com_example_quarkus_BackendResource_countBackend_total[1m]) Sample stacked graph PromQL for query concurrent requests application_com_example_quarkus_BackendResource_concurrentBackend_current Sample stacked graph Custom Grafana Dashboard Use Grafana Operator (Community Edition) to deploy Grafana and configure datasource to Thanos Querier Create project oc new-project application-monitor --display-name=\"Custom Grafana\" --description=\"Custom Grafana\" Install Grafana Operator to project application-monitor Install Grafana Operator from OperatorHub Install to application-monitor project Create Grafana instance oc create -f manifests/grafana.yaml -n application-monitor watch -d oc get pods -n application-monitor Sample Output NAME READY STATUS RESTARTS AGE grafana-deployment-cd4764497-jcwtx 1/1 Running 0 52s grafana-operator-7d585d8fb4-nks8s 1/1 Running 0 4m55s Add cluster role cluster-monitoring-view to Grafana ServiceAccount oc adm policy add-cluster-role-to-user cluster-monitoring-view \\ -z grafana-serviceaccount -n application-monitor Create Grafana DataSource with serviceaccount grafana-serviceaccount's token and connect to thanos-querier TOKEN=$(oc serviceaccounts get-token grafana-serviceaccount -n application-monitor) cat manifests/grafana-datasource.yaml|sed 's/Bearer .*/Bearer '\"$TOKEN\"\"'\"'/'|oc apply -n application-monitor -f - Create Grafana Dashboard oc apply -f manifests/grafana-dashboard.yaml -n application-monitor Login to Grafana Dashboard with following URL echo \"Grafana URL => https://$(oc get route grafana-route -o jsonpath='{.spec.host}' -n application-monitor)\" or use link provided by Developer Console Login with user admin and password openshift Generate workload bash script to loop request to frontend application. FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') while [ 1 ]; do curl $FRONTEND_URL printf \"\\n\" sleep .2 done k6 load test tool with 10 threads for 5 minutes FRONTEND_SVC=$(oc get svc -n project1 | grep frontend | head -n 1 | awk '{print $1}') oc run load-test-frontend -n project1 \\ -i --image=loadimpact/k6 \\ --rm=true --restart=Never -- run - Sample output of k6 load test tool data_received..................: 714 kB 2.0 kB/s data_sent......................: 95 kB 264 B/s http_req_blocked...............: avg=17.62µs min=1.74µs med=2.84µs max=5.39ms p(90)=3.85µs p(95)=4.35µs http_req_connecting............: avg=12.92µs min=0s med=0s max=4ms p(90)=0s p(95)=0s http_req_duration..............: avg=2.87s min=215.57ms med=5.21s max=5.98s p(90)=5.64s p(95)=5.64s { expected_response:true }...: avg=2.87s min=215.57ms med=5.21s max=5.98s p(90)=5.64s p(95)=5.64s http_req_failed................: 0.00% ✓ 0 ✗ 1163 http_req_receiving.............: avg=65.78µs min=36.34µs med=64.54µs max=402.6µs p(90)=78.39µs p(95)=82.43µshttp_req_tls_handshaking.......: avg=0s min=0s med=0s max=0s p(90)=0s p(95)=0s http_req_waiting...............: avg=2.87s min=215.49ms med=5.21s max=5.98s p(90)=5.64s p(95)=5.64s http_reqs......................: 1163 3.222008/s iteration_duration.............: avg=2.87s min=215.71ms med=5.21s max=5.98s p(90)=5.64s p(95)=5.64s iterations.....................: 1163 3.222008/s vus............................: 2 min=1 max=10 vus_max........................: 10 min=10 max=10 Remark: You need to configure frontend app to connect to backend app oc set env deployment/frontend-v1 BACKEND_URL=http://backend:8080/ -n project1 oc set env deployment/frontend-v2 BACKEND_URL=http://backend:8080/ -n project1 Grafana Dashboard Custom Alert Check PrometheusRule backend-app-alert apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: backend-app-alert namespace: project1 labels: openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus spec: groups: - name: backend-app rules: - alert: ConcurrentBackend expr: sum(application_com_example_quarkus_BackendResource_concurrentBackend_current)>15 # wait just 1 minute for demo purpose for: 1m labels: severity: 'warning' annotations: message: 'Total concurrent request is request/sec' - alert: HighLatency expr: application_com_example_quarkus_BackendResource_timeBackend_max_seconds>5 labels: severity: 'critical' annotations: message: ' response time is sec' backend-app-alert is consists with 2 following alerts: ConcurrentBackend severity warning when total concurrent reqeusts is greater than 15 HighLatency servierity critical when response time is greateer than 5 sec Create backend-app-alert oc apply -f manifests/backend-custom-alert.yaml Remark: Role monitoring-rules-view is required for view PrometheusRule resource and role monitoring-rules-edit is required to create, modify, and deleting PrometheusRule Following example is granting role monitoring-rules-view and monitoring-rules-edit to user1 for project1 oc adm policy add-role-to-user monitoring-rules-view user1 -n project1 oc adm policy add-role-to-user monitoring-rules-edit user1 -n project1 For simplified our test, set backend app to 2 pod oc delete deployment backend-v2 -n project1 oc scale deployment backend-v1 --replicas=2 -n project1 Test ConcurrentBackend alert with 25 concurrent requests FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') siege -c 25 $FRONTEND_URL If you don't have siege, run k6 as pod on OpenShift 25 threads Duration 2 minutes Ramp up 30 sec Ramp down 30 sec FRONTEND_URL=https://$(oc get route frontend -n project1 -o jsonpath='{.spec.host}') oc run load-test -n project1 -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - Check for alert in Developer Console by select Menu Monitoring then select Alerts Test HighLatency alert Set backend with 6 sec response tim By CLIoc set env deployment/backend-v1 APP_BACKEND=https://httpbin.org/delay/6 -n project1 By Developer Console Select Topology then select backend-v1 and select dropdown menu Edit Deployment Select Environment and set APP_BACKEND to https://httpbin.org/delay/6 Request to frontend app curl $FRONTEND_URL Check for alert in Developer Console Console overview status "},"ci-cd-with-jenkins.html":{"url":"ci-cd-with-jenkins.html","title":"CI/CD with Jenkins","keywords":"","body":"CI/CD with Jenkins CI/CD with Jenkins Overall Solution Build and Deploy to Development Environment Deploy Staging Environment Deploy UAT Environment Deploy Production Environment Setup Projects Jenkins, SonarQube and Nexus Jenkins Slave Jenkins Pipelines Jenkins Remote API Checkpoints Possible improvement Overall Solution Jenkins pipelines to demonstrate CI/CD process to build Quarkus application from source code to container image with version control by tag name and deploy application to Development, Staging, UAT and blue/green deployment to Production environment. Remark: Source code of Quarkus and Jenkins Build and Deploy to Development Environment Build fast-jar Quarkus application Pull dependencies from Nexus Run unit test with JUnit and code quality with SonarQube Push container image to Nexus or internal registry Create service, route and deploymentconfig in dev project Deploy Staging Environment Select version to deploy to stage project Tag container image with version-DDMMYYYY-round Tear down and deploy application to stage project Deploy UAT Environment Select version to deploy to uat project. Only images with tag version-DDMMYYYY-round will be avaiable in list to deploy Tear down and deploy application to uat project Deploy Production Environment Select version to deploy to prod project. Only images with tag version-DDMMYYYY-round will be avaiable in list to deploy Create deploymentconfig and service for blue and green version Create route Select version to deploy and scale down previous version Setup Projects Create 4 projects ci-cd, dev, stage, uat and prod CI_CD=ci-cd DEV=dev STAGE=stage UAT=uat PROD=prod oc new-project $DEV --display-name=\"Development Environment\" oc new-project $STAGE --display-name=\"Staging Environment\" oc new-project $UAT --display-name=\"User Acceptance Test Environment\" oc new-project $PROD --display-name=\"Production Environment\" oc new-project $CI_CD --display-name=\"CI/CD Tools\" Allow jenkins service account to managed dev, stage, uat and prod oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${DEV} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${STAGE} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${UAT} oc policy add-role-to-user edit system:serviceaccount:${CI_CD}:jenkins -n ${PROD} Allow dev, stage, uat and prod to pull image from ci-cd project (in case use internal image registry instead of Nexus) oc policy add-role-to-group system:image-puller system:serviceaccounts:${DEV} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${STAGE} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${UAT} -n ${CI_CD} oc policy add-role-to-group system:image-puller system:serviceaccounts:${PROD} -n ${CI_CD} Remark: You can use bash script setup_projects.sh for all above steps. Jenkins, SonarQube and Nexus Setup Run setup_ci_cd_tools.sh to setup Jenkins, SonarQube and Nexus cd bin ./setup_ci_cd_tools.sh Sample output Check Developer Console Login to Nexus with user admin with password from the 1st line nexus_password.txt. You can change password to whatever you want Check for Nexus's repositories Jenkins will use user and password stored in secret nexus-credential Check for nexus-credential oc describe secret/nexus-credential -n ci-cd Sample output Name: nexus-credential Namespace: ci-cd Labels: Annotations: Type: Opaque Data ==== password: 48 bytes username: 8 bytes Jenkins Slave Create custom jenkins slave which contains maven 3.6, skopeo and nexus-cli by run setup_maven36_slave.sh oc project ci-cd ./setup_maven36_slave.sh Check imagestream in project ci-cd oc get is/maven36-with-tools Jenkins Pipelines Create pipelines oc apply -f manifests/backend-build-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-uat-pipeline.yaml -n ci-cd oc apply -f manifests/backend-release-prod-pipeline.yaml -n ci-cd Control pipeline to use internal registry or Nexus by pipeline's parameter USE_INTERNAL_REGISTRY env: - name: DEV_PROJECT value: dev - name: CICD_PROJECT value: ci-cd - name: BACKEND_URL value: https://httpbin.org/status/200 - name: NEXUS_SVC value: http://nexus.ci-cd.svc.cluster.local:8081 - name: NEXUS_REGISTRY_SVC value: nexus-registry.ci-cd.svc.cluster.local:5000 - name: NEXUS_REGISTRY value: nexus-registry-ci-cd.apps.cluster-a987.a987.example.opentlc.com - name: SONARQUBE_SVC value: http://sonarqube:9000 - name: NEXUS_SECRET value: nexus-credential - name: USE_INTERNAL_REGISTRY value: \"false\" Jenkins Remote API Create token Check for jenkins's user ID Configure pipeline Trigger builds Test USERID=opentlc-mgr-admin-edit-view TOKEN=117d9459d809be344f1823cbc1248fba09 JENKINS_URL=https://jenkins-ci-cd.apps.cluster-1516.1516.example.opentlc.com curl -X POST -L -v --user $USERID:$TOKEN \"$JENKINS_URL/job/ci-cd/job/ci-cd-backend-build-pipeline/buildWithParameters?token=jira&NEXUS_REGISTRY_SVC=nexus-registry.ci-cd.svc.cluster.local:5000&NEXUS_REGISTRY=nexus-registry-ci-cd.apps.cluster-a987.a987.example.opentlc.com\" Checkpoints Maven build in pipeline pull dependencies from nexus Code snippets environment { mvnCmd = \"mvn -s ./nexus_settings.xml \" ... ... } ... script { sh \"${mvnCmd} -Dquarkus.package.type=fast-jar -Dinternal.repo.username=${nexusUser} -Dinternal.repo.password=${nexusPassword} -DskipTests=true clean package\" } Nexus's repository SonarQube code quality checking Code snippets script { sh \"${mvnCmd} sonar:sonar -Dinternal.repo.username=${nexusUser} -Dinternal.repo.password=${nexusPassword} -Dsonar.host.url=${env.SONARQUBE_SVC} -Dsonar.projectName=${imageName}-${devTag} -Dsonar.projectVersion=${devTag}\" } Scan result Container images is built and pushed to Nexus Code snippets openshift.withCluster() { openshift.withProject(env.CICD_PROJECT) { openshift.newBuild( \"--name=${imageName}\", \"--to=${nexus_url}/${imageName}:latest\", \"--to-docker=true\", \"--push-secret=nexus-registry\", \"--strategy=docker\", \"--binary=true\" ) } } Nexus image registry Application is deployed with label version and tag Possible improvement Change from deploymentconfig to deployment Change from OpenShift's template to kustomize Validate SonarQube result "},"ci-cd.html":{"url":"ci-cd.html","title":"CI/CD with Azure DevOps","keywords":"","body":"CI/CD with Azure DevOps CI/CD Prerequisites Azure DevOps Deploy Back App Deploy Front App Prepare Harbor On Kubernetes/OpenShift Prepare Azure DevOps Service Connection Azure pipelines Prerequisites Openshift 4.6 Cluster Oepnshift User with Admin Permission Azure DevOps Project Harbor Container Registry Postman / K6 for Test Azure DevOps Azure Devops Project https://dev.azure.com/user/project Azure Repo user: chatapazar PAT: xxx demo.Front Repository: https://user@dev.azure.com/user/demo.Front/_git/demo.Front demo.Back Repository : https://user@dev.azure.com/user/demo.Front/_git/demo.Back Azure Artifact create new feed name: my-very-private-feed leave all default Deploy Back App deploy source code from azure repo with openshift s2i login, new project call 'test' oc login oc new-project test prepare secret for azure repo oc create secret generic azure-repo --from-literal=username=chatapazar --from-literal=password=xxx --type=kubernetes.io/basic-auth oc secrets link builder azure-repo deploy back app back.yaml oc create -f back.yaml oc expose svc/back Deploy Front App set current project to test oc project test create secret for harbor oc create secret docker-registry myharbor --docker-username=chatapazar --docker-server=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com --docker-password=xxx oc secrets link default myharbor --for=pull --namespace=test For private Container Registry and Self Sign Cert, if use CA go to create imagestream get ca.crt with openssl openssl s_client -connect ocr.apps.cluster-b3e9.b3e9.example.opentlc.com:443 -showcerts /dev/null|openssl x509 -outform PEM > ca.crt or get cert with firefox (ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/v2 --> select both PEM & PEM chain) create configmap and add trust ca to openshift (both PEM & PEM chain oc create configmap harbor-registry --from-file=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com=ca1.crt -n openshift-config oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"harbor-registry\"}}}' --type=merge oc create configmap registry-config --from-file=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com=ca.crt -n openshift-config oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-config\"}}}' --type=merge create imagestream oc import-image test/front-blue:latest --from=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 --confirm oc import-image test/front-green:latest --from=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 --confirm update imagestream, if you need change version of image in openshift oc tag ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 test/front-blue:latest oc tag ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/test/testdemoapp.front:20201230.32 test/front-green:latest deploy front-blue, front-green and expose route to front-blue front-blue.yaml, front-green.yaml oc create -f front-blue.yaml oc create -f front-green.yaml oc patch dc front-green -p \"{\\\"spec\\\":{\\\"replicas\\\":0}}\" -n test oc expose service front-blue -l name=front --name=front Environment of front app, can change in front-blue.yaml and front-green.yaml ITERATION_COUNT=1000 BACKEND_URL=http://back:8080/api/values/back ASPNETCORE_URLS=http://*:8080 Canary Deployment create imagestream for canary oc import-image test/front-main:latest --from=ocr.apps.cluster-852b.852b.example.opentlc.com/test/testdemoapp.front:20210105.5 --confirm oc import-image test/front-sub:latest --from=ocr.apps.cluster-852b.852b.example.opentlc.com/test/testdemoapp.front:20210105.5 --confirm create front-main dc oc project test oc create -f front-main.yaml create front-sub dc oc create -f front-sub.yaml create route canary oc create -f canary.yaml test canary manual run release canary in azure devops curl http://canary-test.apps.cluster-852b.852b.example.opentlc.com/api/values/information Prepare Harbor On Kubernetes/OpenShift create new project 'harbor' oc new-project harbor oc adm policy add-scc-to-group anyuid system:authenticated install harbor with helm: https://computingforgeeks.com/install-harbor-image-registry-on-kubernetes-openshift-with-helm-chart/ helm install harbor harbor/harbor \\ --set persistence.persistentVolumeClaim.registry.size=10Gi \\ --set persistence.persistentVolumeClaim.chartmuseum.size=5Gi \\ --set persistence.persistentVolumeClaim.database.size=5Gi \\ --set externalURL=https://ocr.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set expose.ingress.hosts.core=ocr.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set expose.ingress.hosts.notary=notary.apps.cluster-b3e9.b3e9.example.opentlc.com \\ --set harborAdminPassword=H@rb0rAdm \\ -n harbor change externalURL to https://ocr.{openshift-clustername} change expose.ingress.hosts.core to ocr.{openshift-clustername} change expose.ingress.hosts.notary to notary.{openshift-clustername} create project test create user for access harbor Prepare Azure DevOps Service Connection Service Connection: openshift select new service connection, select type Openshift Authentication method: Token Based Authentication Server URL: such as https://api.cluster-b3e9.b3e9.example.opentlc.com:6443 accept untrusted SSL: checked api token: such as sha256~fF0TCW0az6FMJ6232dJAxdhX4lqZo-bkYdbfFKwv_Zw service connection name: openshift grant access permission to all pipelines: checked Service Connection: harbor select new service connection, select type docker registry registry type: Others Docker Registry: such as https://ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/ Docker ID: harbor user Docker Password: harbor password service connection name: harbor grant access permission to all pipelines: checked Service Connection: fortify select new service connection, select type fortify authen mode: basic authen api url: https://api.trial.fortify.com portal url: https://trial.fortify.com username: chatapazar@gmail.com PAT: xxx Tenant ID: xxx connection name: fortify Azure pipelines Pipelines: sample-pipeline.yml, sample-pipeline-redhat-image.yml current step in ci or pipeline install .net sdk 2.2 for test project (app use 2.1, test use 2.2 ???) restore package/library with azure artifacts build unit test --> publish to Azure DevOps code coverage with cobertura --> publish to Azure DevOps publish Option: scan code with fortify (use fortify on demand, don't have fortify scs license file) Option: login registry.redhat.io for pull ubi8/dotnet-21-runtime --> sample-pipeline-redhat-image.yml build image install trivy, scan image with trivy, publish resutl to Azure DevOps (test) harbor login, with self sign of harbor, need copy ca.crt to docker (such as /etc/docker/certs.d/ocr.apps.cluster-b3e9.b3e9.example.opentlc.com/ca.crt ) in Azure DevOps agent and manual login, recommended use CA in Prod push image to harbor Releases: blue-green.json, canary.json trigger from ci/pipeline or manual stage 1: switch to new version setup oc command check current deployment and destination deployment switch from blue to green or green to blue switch route to new version stage 2: scale down previous version can add approval for confirm setup oc command scale down previous version Test with postman script of Test Canary Deployment change in front route with yaml spec: host: front-test.apps.cluster-852b.852b.example.opentlc.com to: kind: Service name: front-blue weight: 90 alternateBackends: - kind: Service name: front-green weight: 10 port: targetPort: 8080-tcp wildcardPolicy: None or oc pathc oc patch route frontend -p '{\"spec\":{\"to\":{\"weight\":60}}}' -n project1 oc patch route frontend --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/alternateBackends/0/weight\",\"value\":40}]' -n project1 Use Openshift Internal Registry oc create imagestream demofront -n test oc create imagestream demoapp -n test oc login with plugin docker login -u opentlc-mgr -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/test/demofront:xxx docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/test/demoapp:xxx oc tag test/demofront:xxx test/demofront:latest oc tag test/demoapp:xxx test/demoapp:latest create imagestream login openshift login docker to openshift internal registry with token push image tag image stream Step Demo preset architecture prod vs demo preset ocp console / usage / login / user management preset harbor / usage / project / user management / scan manual/auto / set cve block pull / set cve whitelist / set auto scan on push present pipeline / release scan code with fortify scan image in pipeline , change show image from red hat , run with red hat image blue green / deploy canary --> example --> see again in service mesh , network deploy section detail of openshift route deployment streategy openshift internal registry add permission to user for internal registryoc adm policy add-role-to-user system:registry -n oc adm policy add-role-to-user system:image-builder -n ## or for cluster-wide access... oc adm policy add-cluster-role-to-user system:registry oc adm policy add-cluster-role-to-user system:image-builder exampleoc login (with user1) oc new-project user1 oc login (with user2) oc new-project user2 oc login (with admin) oc adm policy add-role-to-user system:registry user1 -n user1 oc adm policy add-role-to-user system:image-builder user2 -n user2 test rbac internal registryoc login (with user1) docker login -u user1 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull hello-world docker tag hello-world:latest default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest docker push default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view imagestream in openshift project user1 docker rmi -f $(docker images -a -q) docker images docker logout default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com oc login (with user2) docker login -u user2 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view result error docker logout default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com oc login (with user1) docker login -u user1 -p $(oc whoami -t) default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com docker pull default-route-openshift-image-registry.apps.cluster-852b.852b.example.opentlc.com/user1/hello-world:latest #view result success prune image https://docs.openshift.com/container-platform/4.4/applications/pruning-objects.html#pruning-images_pruning-objects "},"eap-on-ocp.html":{"url":"eap-on-ocp.html","title":"EAP on OpenShift","keywords":"","body":"JBoss EAP on OpenShift JBoss EAP on OpenShift Binary Build and Deployment Configure Standalone Cluster Binary Build and Deployment Clone verify cluster app Pacakge application (WAR) and copy at your local directory mvn clean package mkdir -p deployments cp target/verify-cluster.war deployments Create build config with EAP image stream and use local WAR file Initial environment variables APP_NAME=verify-cluster PROJECT=$(oc project -q) IMAGE_STREAM=jboss-eap73-openshift:7.3 REPLICAS=2 VERSION=1.0.0 BINARY_PATH=deployments/verify-cluster.war Build EAP container image with deployed application Create build config oc new-build --binary=true \\ --name=${APP_NAME} -l app=${APP_NAME} --image-stream=${IMAGE_STREAM} Start build oc start-build ${APP_NAME} \\ --from-file=${BINARY_PATH} \\ --follow Tag image with version oc tag ${APP_NAME}:latest ${APP_NAME}:${VERSION} Check build config oc get buildconfig/verify-cluster output NAME TYPE FROM LATEST verify-cluster Source Binary 1 Create Deoloyment Start build and deploy with oc new-app oc new-app ${APP_NAME}:${VERSION} \\ --labels=app=${APP_NAME},deploymentconfig=${APP_NAME},version=${VERSION},app.kubernetes.io/name=jboss Check pods oc get pods -l app=$APP_NAME Output NAME READY STATUS RESTARTS AGE verify-cluster-565fbb6f86-jt5xl 1/1 Running 0 4m42s Check on Development Console Pods Resources Create route oc expose svc/${APP_NAME} echo \"URL: http://$(oc get route ${APP_NAME} -n ${PROJECT} -o jsonpath='{.spec.host}')/verify-cluster\" Test access verify-cluster and press Increment. Notice pod name and number of hits will be increased Configure Standalone Cluster Service Account default needs view role to run KUBE_PING oc adm policy add-cluster-role-to-user \\ view system:serviceaccount:$PROJECT:default -n $PROJECT Pause rollout oc rollout pause deployment ${APP_NAME} Configure JGROUP oc set env deployment ${APP_NAME} \\ JGROUPS_PING_PROTOCOL=kubernetes.KUBE_PING \\ KUBERNETES_NAMESPACE=${PROJECT} \\ KUBERNETES_LABELS=app=${APP_NAME},version=${VERSION} oc set env deployment ${APP_NAME} DISABLE_EMBEDDED_JMS_BROKER=true Scale replicas oc scale deployment ${APP_NAME} --replicas=${REPLICAS} Resume rollout and check pods oc rollout resume deployment ${APP_NAME} oc get pods -l app=$APP_NAME,version=$VERSION Output NAME READY STATUS RESTARTS AGE verify-cluster-75584cb799-rcf5t 1/1 Terminating 0 52s verify-cluster-789f5dd9d-qmg9s 1/1 Running 0 34s verify-cluster-789f5dd9d-s4q8z 1/1 Running 0 38s Check log oc logs $(oc get pods -l app=$APP_NAME,version=$VERSION | tail -n 1 | awk '{print $1}') Test that sessions is replicated Access verify-cluster app Increment counter Delete pod that response your request oc delete pods Refresh page again and check that counter still continue. binary_build.sh can be used to automated all steps. --list oc set env dc/datagrid-app MYCACHE_CACHE_START=LAZY ``` ## EAP 7.3 - Update Imagestreams - boss-eap73-openshift - jboss-eap73-runtime-openshift - jboss-eap73-openjdk11-openshift - jboss-eap73-openjdk11-runtime-openshift ```bash for resource in \\ eap73-amq-persistent-s2i.json \\ eap73-amq-s2i.json \\ eap73-basic-s2i.json \\ eap73-https-s2i.json \\ eap73-image-stream.json \\ eap73-sso-s2i.json \\ eap73-starter-s2i.json \\ eap73-third-party-db-s2i.json \\ eap73-tx-recovery-s2i.json \\ eap73-openjdk11-amq-persistent-s2i.json \\ eap73-openjdk11-amq-s2i.json \\ eap73-openjdk11-basic-s2i.json \\ eap73-openjdk11-https-s2i.json \\ eap73-openjdk11-image-stream.json \\ eap73-openjdk11-sso-s2i.json \\ eap73-openjdk11-starter-s2i.json \\ eap73-openjdk11-third-party-db-s2i.json \\ eap73-openjdk11-tx-recovery-s2i.json do oc replace -n openshift --force -f \\ https://raw.githubusercontent.com/jboss-container-images/jboss-eap-7-openshift-image/eap73/templates/${resource} done ``` ## EAP 7.2 ```bash for resource in \\ eap72-amq-persistent-s2i.json \\ eap72-amq-s2i.json \\ eap72-basic-s2i.json \\ eap72-https-s2i.json \\ eap72-image-stream.json \\ eap72-sso-s2i.json \\ eap72-starter-s2i.json \\ eap72-third-party-db-s2i.json \\ eap72-tx-recovery-s2i.json do oc replace --force -f \\ https://raw.githubusercontent.com/jboss-container-images/jboss-eap-7-openshift-image/eap72/templates/${resource} -n openshift ``` ## Setup JDG on EAP You need to start EAP with **stanalone-ha.xml** - JBOSS CLI ```bash ./jboss-cli.sh --user=admin --password=admin \\ --controller=http-remoting://127.0.0.1:9990 --connect ``` - Add Socket Binding to RHDG server ```bash /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=remote-rhdg-server1:add(host=127.0.0.1,port=11222) ``` - Add remote cache ```bash batch /subsystem=infinispan/remote-cache-container=rhdg:add(default-remote-cluster=data-grid-cluster) /subsystem=infinispan/remote-cache-container=rhdg/remote-cluster=data-grid-cluster:add(socket-bindings=[remote-rhdg-server1]) run-batch ``` - Enable remote cache statistics ```bash /subsystem=infinispan/remote-cache-container=rhdg:write-attribute(name=statistics-enabled, value=true) /subsystem=infinispan/remote-cache-container=rhdg:read-attribute(name=active-connections) ``` - Check for statistics ```bash /subsystem=infinispan/remote-cache-container=rhdg/remote-cache=verify-cluster.war:read-resource(include-runtime=true, recursive=true) /subsystem=infinispan/remote-cache-container=rhdg/remote-cache=verify-cluster.war:reset-statistics() ``` - Create cache container name **demo** for externalized HTTP to for web ```bash batch /subsystem=infinispan/cache-container=web/invalidation-cache=demo:add() /subsystem=infinispan/cache-container=web/invalidation-cache=demo/store=hotrod:add(remote-cache-container=rhdg,fetch-state=false,purge=false,passivation=false,shared=true) /subsystem=infinispan/cache-container=web/invalidation-cache=demo/component=transaction:add(mode=BATCH) /subsystem=infinispan/cache-container=web/invalidation-cache=demo/component=locking:add(isolation=REPEATABLE_READ) /subsystem=infinispan/cache-container=web:write-attribute(name=default-cache,value=demo) run-batch ``` - edit label ```properties app.openshift.io/runtime=jboss ``` -->"},"gitops.html":{"url":"gitops.html","title":"OpenShift GitOps","keywords":"","body":"OpenShift GitOps Install OpenShift GitOps Operator Check ArgoCD in openshift-gitops namespaceoc get pods -n openshift-gitops OutputNAME READY STATUS RESTARTS AGE cluster-5b574cff45-szpsl 1/1 Running 0 36s kam-7f65f49f56-2bd7g 1/1 Running 0 35s openshift-gitops-application-controller-0 1/1 Running 0 35s openshift-gitops-applicationset-controller-769bc45f-qbhs6 1/1 Running 0 35s openshift-gitops-redis-7765dd9fc9-gbc42 1/1 Running 0 35s openshift-gitops-repo-server-7c46884cf6-jjrn8 1/1 Running 0 35s openshift-gitops-server-7975f7b985-56tn7 1/1 Running 0 35s ArgoCD URL. Remark that ArgoCD route is passtrough.ARGOCD=$(oc get route/openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}') echo https://$ARGOCD PasswordPASSWORD=$(oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-) 2>/dev/null echo $PASSWORD Install argocd cli. For OSX use brewbrew install argocd login to argocdargocd login $ARGOCD --insecure \\ --username admin \\ --password $PASSWORD Output'admin:login' logged in successfully Context 'openshift-gitops-server-openshift-gitops.apps.cluster-0e2b.0e2b.sandbox563.opentlc.com' updated Use oc or kubectl CLI to login to target cluster and rename contextoc config rename-context $(oc config current-context) dev-cluster OutputContext \"default/api-cluster-0e2b-0e2b-sandbox563-opentlc-com:6443/opentlc-mgr\" renamed to \"dev-cluster\". Use argocd CLI to add current cluster to be managed by ArgoCDargocd add cluster dev-cluster OutputINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\" INFO[0001] ClusterRole \"argocd-manager-role\" updated INFO[0002] ClusterRoleBinding \"argocd-manager-role-binding\" updated Cluster 'https://api.cluster-0e2b.0e2b.sandbox563.opentlc.com:6443' added Create application demo-dev-clusteroc apply -f manifests/gitops/applications/demo-dev-cluster.yaml Outputapplication.argoproj.io/demo-dev-cluster created Check application demo-dev-cluster statusoc get application -n openshift-gitops OutputNAME SYNC STATUS HEALTH STATUS demo-dev-cluster Synced Healthy demo-dev-cluster use kustomize and configured to manifests/apps-kustomize/overlyas/dev manifests/apps-kustomize ├── base │ ├── backend-service.yaml │ ├── backend.yaml │ ├── demo-rolebinding.yaml │ ├── frontend-service.yaml │ ├── frontend.yaml │ ├── kustomization.yaml │ ├── namespace.yaml │ └── route.yaml └── overlays ├── dev │ ├── backend.yaml │ ├── frontend.yaml │ └── kustomization.yaml └── prod ├── backend.yaml ├── frontend.yaml └── kustomization.yaml Walkthrough ArgoCD console Open ArgoCD URL Application status Overall Reference to git commit Application topology Node topology Pod's log "}}